
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="An open, intuition-first textbook covering mathematics, computer science, and artificial intelligence from the ground up.">
      
      
        <meta name="author" content="Henry Ndubuaku">
      
      
        <link rel="canonical" href="https://henryndubuaku.github.io/maths-cs-ai-compendium/chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/">
      
      
        <link rel="prev" href="../01.%20multimodal%20representations/">
      
      
        <link rel="next" href="../03.%20image%20and%20video%20tokenisation/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>Vision Language Models - Maths, CS & AI Compendium</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  
<meta property="og:type" content="website" />
<meta property="og:title" content="Vision Language Models - Maths, CS & AI Compendium" />
<meta property="og:description" content="An open, intuition-first textbook covering mathematics, computer science, and artificial intelligence from the ground up." />
<meta property="og:image" content="https://henryndubuaku.github.io/maths-cs-ai-compendium/assets/images/social/chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models.png" />
<meta property="og:image:type" content="image/png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="630" />
<meta property="og:url" content="https://henryndubuaku.github.io/maths-cs-ai-compendium/chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/" />
<meta property="twitter:card" content="summary_large_image" />
<meta property="twitter:title" content="Vision Language Models - Maths, CS & AI Compendium" />
<meta property="twitter:description" content="An open, intuition-first textbook covering mathematics, computer science, and artificial intelligence from the ground up." />
<meta property="twitter:image" content="https://henryndubuaku.github.io/maths-cs-ai-compendium/assets/images/social/chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models.png" />
</head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="slate" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#vision-language-models" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Maths, CS &amp; AI Compendium" class="md-header__button md-logo" aria-label="Maths, CS & AI Compendium" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Maths, CS & AI Compendium
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Vision Language Models
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="slate" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="slate" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/HenryNdubuaku/maths-cs-ai-compendium" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    HenryNdubuaku/maths-cs-ai-compendium
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2001%3A%20vectors/01.%20vector%20spaces/" class="md-tabs__link">
          
  
  
  Vectors

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2002%3A%20matrices/01.%20matrix%20properties/" class="md-tabs__link">
          
  
  
  Matrices

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2003%3A%20calculus/01.%20differential%20calculus/" class="md-tabs__link">
          
  
  
  Calculus

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2004%3A%20statistics/01.%20fundamentals/" class="md-tabs__link">
          
  
  
  Statistics

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2005%3A%20probability/01.%20counting/" class="md-tabs__link">
          
  
  
  Probability

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2006%3A%20machine%20learning/01.%20classical%20machine%20learning/" class="md-tabs__link">
          
  
  
  Machine Learning

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2007%3A%20computational%20linguistics/01.%20linguistic%20foundations/" class="md-tabs__link">
          
  
  
  Computational Linguistics

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2008%3A%20computer%20vision/01.%20image%20fundamentals/" class="md-tabs__link">
          
  
  
  Computer Vision

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2009%3A%20audio%20and%20speech/01.%20digital%20signal%20processing/" class="md-tabs__link">
          
  
  
  Audio and Speech

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../01.%20multimodal%20representations/" class="md-tabs__link">
          
  
  
  Multimodal Learning

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2011%3A%20autonomous%20systems/01.%20perception/" class="md-tabs__link">
          
  
  
  Autonomous Systems

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2012%3A%20computing%20and%20OS/01.%20discrete%20maths/" class="md-tabs__link">
          
  
  
  Computing and OS

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2013%3A%20data%20structures%20and%20algorithms/01.%20arrays%20and%20hashing/" class="md-tabs__link">
          
  
  
  Data Structures and Algorithms

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2014%3A%20SIMD%20and%20GPU%20programming/01.%20hardware%20fundamentals/" class="md-tabs__link">
          
  
  
  SIMD and GPU Programming

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2015%3A%20systems%20design/01.%20systems%20design%20fundamentals/" class="md-tabs__link">
          
  
  
  Systems Design

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2016%3A%20inference/01.%20quantisation/" class="md-tabs__link">
          
  
  
  Inference

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../chapter%2017%3A%20intersecting%20fields/01.%20quantum%20machine%20learning/" class="md-tabs__link">
          
  
  
  Intersecting Fields

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Maths, CS &amp; AI Compendium" class="md-nav__button md-logo" aria-label="Maths, CS & AI Compendium" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Maths, CS & AI Compendium
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/HenryNdubuaku/maths-cs-ai-compendium" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    HenryNdubuaku/maths-cs-ai-compendium
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Vectors
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Vectors
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2001%3A%20vectors/01.%20vector%20spaces/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vector Spaces
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2001%3A%20vectors/02.%20vector%20properties/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vector Properties
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2001%3A%20vectors/03.%20norms%20and%20metrics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Norms and Metrics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2001%3A%20vectors/04.%20products/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Products
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2001%3A%20vectors/05.%20basis%20and%20duality/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Basis and Duality
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Matrices
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Matrices
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2002%3A%20matrices/01.%20matrix%20properties/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Matrix Properties
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2002%3A%20matrices/02.%20matrix%20types/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Matrix Types
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2002%3A%20matrices/03.%20operations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Operations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2002%3A%20matrices/04.%20linear%20transformations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Linear Transformations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2002%3A%20matrices/05.%20decompositions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Decompositions
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Calculus
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Calculus
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2003%3A%20calculus/01.%20differential%20calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Differential Calculus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2003%3A%20calculus/02.%20integral%20calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Integral Calculus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2003%3A%20calculus/03.%20multivariate%20calculus/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Multivariate Calculus
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2003%3A%20calculus/04.%20function%20approximation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Function Approximation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2003%3A%20calculus/05.%20optimisation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Optimisation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Statistics
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Statistics
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2004%3A%20statistics/01.%20fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Fundamentals
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2004%3A%20statistics/02.%20measures/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Measures
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2004%3A%20statistics/03.%20sampling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Sampling
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2004%3A%20statistics/04.%20hypothesis%20testing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hypothesis Testing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2004%3A%20statistics/05.%20inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Inference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Probability
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Probability
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2005%3A%20probability/01.%20counting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Counting
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2005%3A%20probability/02.%20probability%20concepts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Probability Concepts
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2005%3A%20probability/03.%20distributions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Distributions
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2005%3A%20probability/04.%20bayesian/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Bayesian
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2005%3A%20probability/05.%20information%20theory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Information Theory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Machine Learning
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Machine Learning
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2006%3A%20machine%20learning/01.%20classical%20machine%20learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Classical Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2006%3A%20machine%20learning/02.%20gradient%20machine%20learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Gradient Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2006%3A%20machine%20learning/03.%20deep%20learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Deep Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2006%3A%20machine%20learning/04.%20reinforcement%20learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Reinforcement Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2006%3A%20machine%20learning/05.%20distributed%20deep%20learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Distributed Deep Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Computational Linguistics
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            
  
    Computational Linguistics
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2007%3A%20computational%20linguistics/01.%20linguistic%20foundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Linguistic Foundations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2007%3A%20computational%20linguistics/02.%20text%20processing%20and%20classic%20NLP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Text Processing and Classic NLP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2007%3A%20computational%20linguistics/03.%20embeddings%20and%20sequence%20models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Embeddings and Sequence Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2007%3A%20computational%20linguistics/04.%20transformers%20and%20language%20models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Transformers and Language Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2007%3A%20computational%20linguistics/05.%20advanced%20text%20generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Advanced Text Generation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Computer Vision
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            
  
    Computer Vision
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2008%3A%20computer%20vision/01.%20image%20fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Image Fundamentals
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2008%3A%20computer%20vision/02.%20convolutional%20networks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Convolutional Networks
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2008%3A%20computer%20vision/03.%20object%20detection%20and%20segmentation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Object Detection and Segmentation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2008%3A%20computer%20vision/04.%20vision%20transformers%20and%20generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vision Transformers and Generation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2008%3A%20computer%20vision/05.%20video%20and%203D%20vision/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Video and 3D Vision
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_10" >
        
          
          <label class="md-nav__link" for="__nav_10" id="__nav_10_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Audio and Speech
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_10">
            <span class="md-nav__icon md-icon"></span>
            
  
    Audio and Speech
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2009%3A%20audio%20and%20speech/01.%20digital%20signal%20processing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Digital Signal Processing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2009%3A%20audio%20and%20speech/02.%20automatic%20speech%20recognition/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Automatic Speech Recognition
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2009%3A%20audio%20and%20speech/03.%20text%20to%20speech%20and%20voice/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Text to Speech and Voice
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2009%3A%20audio%20and%20speech/04.%20speaker%20and%20audio%20analysis/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Speaker and Audio Analysis
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2009%3A%20audio%20and%20speech/05.%20source%20separation%20and%20noise/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Source Separation and Noise
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" checked>
        
          
          <label class="md-nav__link" for="__nav_11" id="__nav_11_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Multimodal Learning
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            
  
    Multimodal Learning
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01.%20multimodal%20representations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Multimodal Representations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Vision Language Models
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Vision Language Models
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#visual-question-answering" class="md-nav__link">
    <span class="md-ellipsis">
      
        Visual Question Answering
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-captioning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Image Captioning
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecture-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Architecture Patterns
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Architecture Patterns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dual-encoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dual Encoder
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fusion-encoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fusion Encoder
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#encoder-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        Encoder-Decoder
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#flamingo-few-shot-multimodal-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Flamingo: Few-Shot Multimodal Learning
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llava-and-visual-instruction-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA and Visual Instruction Tuning
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#scaling-vision-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scaling Vision-Language Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Scaling Vision-Language Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pali" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaLI
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen-vl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen-VL
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#internvl" class="md-nav__link">
    <span class="md-ellipsis">
      
        InternVL
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#grounding-and-referring" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grounding and Referring
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ocr-free-document-understanding" class="md-nav__link">
    <span class="md-ellipsis">
      
        OCR-Free Document Understanding
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-visual-token-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Visual Token Pipeline
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Objectives
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coding-tasks-use-colab-or-notebook" class="md-nav__link">
    <span class="md-ellipsis">
      
        Coding Tasks (use CoLab or notebook)
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03.%20image%20and%20video%20tokenisation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Image and Video Tokenisation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04.%20cross-modal%20generation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Cross-Modal Generation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05.%20unified%20multimodal%20architectures/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Unified Multimodal Architectures
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_12" >
        
          
          <label class="md-nav__link" for="__nav_12" id="__nav_12_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Autonomous Systems
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_12_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_12">
            <span class="md-nav__icon md-icon"></span>
            
  
    Autonomous Systems
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2011%3A%20autonomous%20systems/01.%20perception/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Perception
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2011%3A%20autonomous%20systems/02.%20robot%20learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Robot Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2011%3A%20autonomous%20systems/03.%20vision-language-action%20models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Vision-Language-Action Models
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2011%3A%20autonomous%20systems/04.%20self-driving/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Self-Driving
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2011%3A%20autonomous%20systems/05.%20space%20and%20extreme%20robotics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Space and Extreme Robotics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_13" >
        
          
          <label class="md-nav__link" for="__nav_13" id="__nav_13_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Computing and OS
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_13_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_13">
            <span class="md-nav__icon md-icon"></span>
            
  
    Computing and OS
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2012%3A%20computing%20and%20OS/01.%20discrete%20maths/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Discrete Maths
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2012%3A%20computing%20and%20OS/02.%20computer%20architecture/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Computer Architecture
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2012%3A%20computing%20and%20OS/03.%20operating%20systems/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Operating Systems
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2012%3A%20computing%20and%20OS/04.%20concurrency%20and%20parallelism/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Concurrency and Parallelism
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2012%3A%20computing%20and%20OS/05.%20programming%20languages/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Programming Languages
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_14" >
        
          
          <label class="md-nav__link" for="__nav_14" id="__nav_14_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Data Structures and Algorithms
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_14_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_14">
            <span class="md-nav__icon md-icon"></span>
            
  
    Data Structures and Algorithms
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2013%3A%20data%20structures%20and%20algorithms/01.%20arrays%20and%20hashing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Arrays and Hashing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2013%3A%20data%20structures%20and%20algorithms/02.%20linked%20lists%2C%20stacks%2C%20and%20queues/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Linked Lists, Stacks, and Queues
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2013%3A%20data%20structures%20and%20algorithms/03.%20trees/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Trees
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2013%3A%20data%20structures%20and%20algorithms/04.%20graphs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Graphs
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2013%3A%20data%20structures%20and%20algorithms/05.%20sorting%20and%20search/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Sorting and Search
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_15" >
        
          
          <label class="md-nav__link" for="__nav_15" id="__nav_15_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    SIMD and GPU Programming
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_15_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_15">
            <span class="md-nav__icon md-icon"></span>
            
  
    SIMD and GPU Programming
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2014%3A%20SIMD%20and%20GPU%20programming/01.%20hardware%20fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Hardware Fundamentals
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2014%3A%20SIMD%20and%20GPU%20programming/02.%20ARM%20and%20NEON/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ARM and NEON
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2014%3A%20SIMD%20and%20GPU%20programming/03.%20x86%20and%20AVX/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    x86 and AVX
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2014%3A%20SIMD%20and%20GPU%20programming/04.%20GPU%20architecture%20and%20CUDA/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    GPU Architecture and CUDA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2014%3A%20SIMD%20and%20GPU%20programming/05.%20triton%2C%20TPUs%2C%20and%20Vulkan/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Triton, TPUs, and Vulkan
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_16" >
        
          
          <label class="md-nav__link" for="__nav_16" id="__nav_16_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Systems Design
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_16_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_16">
            <span class="md-nav__icon md-icon"></span>
            
  
    Systems Design
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2015%3A%20systems%20design/01.%20systems%20design%20fundamentals/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Systems Design Fundamentals
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2015%3A%20systems%20design/02.%20cloud%20computing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Cloud Computing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2015%3A%20systems%20design/03.%20large%20scale%20infrastructure/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Large Scale Infrastructure
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2015%3A%20systems%20design/04.%20ML%20systems%20design/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ML Systems Design
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2015%3A%20systems%20design/05.%20ML%20design%20examples/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ML Design Examples
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_17" >
        
          
          <label class="md-nav__link" for="__nav_17" id="__nav_17_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Inference
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_17_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_17">
            <span class="md-nav__icon md-icon"></span>
            
  
    Inference
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2016%3A%20inference/01.%20quantisation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quantisation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2016%3A%20inference/02.%20efficient%20architectures/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Efficient Architectures
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2016%3A%20inference/03.%20serving%20and%20batching/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Serving and Batching
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2016%3A%20inference/04.%20edge%20inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Edge Inference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2016%3A%20inference/05.%20scaling%20and%20deployment/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Scaling and Deployment
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_18" >
        
          
          <label class="md-nav__link" for="__nav_18" id="__nav_18_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Intersecting Fields
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_18_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_18">
            <span class="md-nav__icon md-icon"></span>
            
  
    Intersecting Fields
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2017%3A%20intersecting%20fields/01.%20quantum%20machine%20learning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Quantum Machine Learning
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2017%3A%20intersecting%20fields/02.%20neuromorphic%20computing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Neuromorphic Computing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2017%3A%20intersecting%20fields/03.%20AI%20for%20finance/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AI for Finance
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2017%3A%20intersecting%20fields/04.%20AI%20for%20biology/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    AI for Biology
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../chapter%2017%3A%20intersecting%20fields/05.%20emerging%20intersections/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Emerging Intersections
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#visual-question-answering" class="md-nav__link">
    <span class="md-ellipsis">
      
        Visual Question Answering
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#image-captioning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Image Captioning
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#architecture-patterns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Architecture Patterns
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Architecture Patterns">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dual-encoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        Dual Encoder
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fusion-encoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        Fusion Encoder
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#encoder-decoder" class="md-nav__link">
    <span class="md-ellipsis">
      
        Encoder-Decoder
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#flamingo-few-shot-multimodal-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Flamingo: Few-Shot Multimodal Learning
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llava-and-visual-instruction-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      
        LLaVA and Visual Instruction Tuning
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#scaling-vision-language-models" class="md-nav__link">
    <span class="md-ellipsis">
      
        Scaling Vision-Language Models
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Scaling Vision-Language Models">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pali" class="md-nav__link">
    <span class="md-ellipsis">
      
        PaLI
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#qwen-vl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Qwen-VL
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#internvl" class="md-nav__link">
    <span class="md-ellipsis">
      
        InternVL
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#grounding-and-referring" class="md-nav__link">
    <span class="md-ellipsis">
      
        Grounding and Referring
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ocr-free-document-understanding" class="md-nav__link">
    <span class="md-ellipsis">
      
        OCR-Free Document Understanding
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-visual-token-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        The Visual Token Pipeline
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-objectives" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Objectives
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#coding-tasks-use-colab-or-notebook" class="md-nav__link">
    <span class="md-ellipsis">
      
        Coding Tasks (use CoLab or notebook)
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="vision-language-models">Vision Language Models<a class="headerlink" href="#vision-language-models" title="Permanent link">&para;</a></h1>
<p><em>Vision language models jointly understand images and text, enabling visual question answering, image captioning, and visual reasoning. This file covers VQA, image captioning, visual grounding, and architectures like VisualBERT, BLIP, LLaVA, Flamingo, PaLI, and Qwen-VL that fuse vision encoders with large language models.</em></p>
<ul>
<li>
<p>Think of a museum guide who can look at a painting and articulate everything about it: what objects are present, what story it tells, what emotions it conveys, and answer any question a visitor might pose. A <strong>vision language model (VLM)</strong> is the computational equivalent  a system that jointly understands images and text, enabling it to describe visual scenes, answer questions about them, follow visual instructions, and even locate specific objects within an image given a natural language query.</p>
</li>
<li>
<p>VLMs sit at the intersection of the vision encoders you met in Chapter 8 and the language models from Chapter 7. The central engineering challenge is bridging two very different representational worlds: the spatial, continuous feature maps of a vision backbone and the sequential, discrete token embeddings of a language model. Every architecture in this file is, at its core, a different answer to the question: how do you fuse vision and language?</p>
</li>
</ul>
<p><img alt="High-level VLM taxonomy showing dual encoder, fusion encoder, and encoder-decoder families with their inputs and outputs" src="../../images/vlm_taxonomy.svg" /></p>
<h2 id="visual-question-answering">Visual Question Answering<a class="headerlink" href="#visual-question-answering" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Imagine someone shows you a photograph and asks "How many dogs are in the park?" You effortlessly parse the image, locate the dogs, count them, and produce an answer. <strong>Visual question answering (VQA)</strong> formalises this: given an image <span class="arithmatex">\(I\)</span> and a natural language question <span class="arithmatex">\(q\)</span>, predict the answer <span class="arithmatex">\(a\)</span>.</p>
</li>
<li>
<p>The task can be framed in several ways. The most common treats VQA as <strong>open-ended classification</strong>: the model selects from a fixed vocabulary of the most frequent answers (e.g., the top 3,129 answers in VQA v2). Alternatively, it can be treated as <strong>generative answering</strong>, where the model produces a free-form text string  this is the approach modern VLMs use.</p>
</li>
<li>
<p>Formally, you want to learn a function <span class="arithmatex">\(f(I, q) \to a\)</span> that maximises the likelihood of the correct answer. In the classification setup, this becomes:</p>
</li>
</ul>
<div class="arithmatex">\[p(a \mid I, q) = \text{softmax}(W \cdot g(v, h))\]</div>
<ul>
<li>
<p>where <span class="arithmatex">\(v\)</span> is a visual feature vector (from a CNN or ViT), <span class="arithmatex">\(h\)</span> is a question encoding (from an LSTM or Transformer), and <span class="arithmatex">\(g\)</span> is a fusion function that combines them. The design of <span class="arithmatex">\(g\)</span> is where the real architectural creativity lies.</p>
</li>
<li>
<p><strong>VQA v1</strong> (Antol et al., 2015) introduced the benchmark with 614,000 questions on 204,000 images from MS COCO. Researchers quickly discovered that models could achieve surprisingly high accuracy by exploiting <strong>language priors</strong>  answering "2" for "how many" questions or "yes" for "is there" questions without even looking at the image.</p>
</li>
<li>
<p><strong>VQA v2</strong> (Goyal et al., 2017) addressed this by pairing each question with two similar images that yield different answers. This forced models to actually ground their reasoning in visual content. The balanced pair setup roughly doubles the dataset and makes language-only shortcuts much less effective.</p>
</li>
<li>
<p>Other important VQA datasets include <strong>GQA</strong> (Hudson &amp; Manning, 2019) with compositional questions requiring multi-step reasoning, <strong>OK-VQA</strong> (Marino et al., 2019) requiring outside knowledge beyond the image, and <strong>TextVQA</strong> (Singh et al., 2019) where the answer depends on reading text within the image.</p>
</li>
</ul>
<p><img alt="VQA pipeline: an image passes through a vision encoder and a question passes through a text encoder, their representations are fused, and the fused vector is classified into an answer" src="../../images/vqa_pipeline.svg" /></p>
<ul>
<li>
<p>Early VQA models used a simple strategy: extract image features from a pre-trained CNN (typically the penultimate layer of ResNet or VGGNet from Chapter 8), encode the question with an LSTM (Chapter 6), and combine them. The combination function <span class="arithmatex">\(g\)</span> evolved rapidly: from simple element-wise multiplication, to bilinear pooling, to multi-modal Tucker decomposition. <strong>Bilinear attention</strong> computes <span class="arithmatex">\(v^T W h\)</span>, where <span class="arithmatex">\(W\)</span> is a learnable interaction matrix, but the full bilinear form has <span class="arithmatex">\(O(d_v \times d_h)\)</span> parameters, which is prohibitively large. <strong>MLB</strong> (multimodal low-rank bilinear pooling) factorises this into two low-rank projections, making it tractable.</p>
</li>
<li>
<p>The breakthrough for VQA was attention. <strong>Stacked Attention Networks</strong> (Yang et al., 2016) used the question encoding to attend over spatial image regions, iteratively refining which parts of the image to focus on. This idea  letting the question "look at" relevant image regions  became standard.</p>
</li>
</ul>
<h2 id="image-captioning">Image Captioning<a class="headerlink" href="#image-captioning" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Picture a friend looking at your holiday photos and narrating what they see: "A golden retriever is catching a frisbee on a sunny beach." <strong>Image captioning</strong> is the task of generating a natural language description of an image. Unlike VQA, there is no question  the model must decide what is worth describing on its own.</p>
</li>
<li>
<p><strong>Show and Tell</strong> (Vinyals et al., 2015) established the canonical encoder-decoder architecture for captioning. A CNN encoder (e.g., Inception or ResNet) produces a single image feature vector <span class="arithmatex">\(v\)</span>. This vector is used as the initial hidden state of an LSTM decoder, which then generates a caption word by word, autoregressively:</p>
</li>
</ul>
<div class="arithmatex">\[p(w_t \mid w_{1:t-1}, I) = \text{LSTM}(w_{t-1}, h_{t-1})\]</div>
<ul>
<li>
<p>The entire model is trained end-to-end by maximising the log-likelihood of ground-truth captions. At inference time, beam search (Chapter 7) is used to find high-probability captions.</p>
</li>
<li>
<p>The problem with Show and Tell is that the entire image is compressed into a single vector. For complex scenes, a single vector cannot capture all the relevant details. You lose spatial information  the model cannot "look back" at specific parts of the image while generating different words.</p>
</li>
<li>
<p><strong>Show, Attend and Tell</strong> (Xu et al., 2015) solved this by introducing <strong>attention over image regions</strong>. Instead of encoding the image as one vector, the CNN produces a spatial feature grid (e.g., <span class="arithmatex">\(14 \times 14 \times 512\)</span> from the last convolutional layer of VGGNet). At each decoding step, the model computes attention weights over these spatial locations, producing a context vector that highlights the most relevant region for the current word.</p>
</li>
<li>
<p>Recall the attention mechanism from Chapter 6: the decoder hidden state acts as the query, the spatial features act as keys and values, and the attention weights tell the model where to look. The authors proposed two variants: <strong>soft attention</strong> (differentiable, weighted average of all regions) and <strong>hard attention</strong> (stochastic sampling of a single region, trained with REINFORCE).</p>
</li>
</ul>
<p><img alt="Attention-based captioning: at each decoding step, the model attends to different spatial regions of the image, focusing on relevant areas like the dog when generating the word dog" src="../../images/attention_captioning.svg" /></p>
<ul>
<li>
<p>The attention maps produced by these models are remarkably interpretable: when generating "dog," the attention peaks over the dog region; when generating "beach," it shifts to the sand and water. This was one of the first compelling demonstrations that attention provides built-in interpretability.</p>
</li>
<li>
<p><strong>CIDEr</strong> (Vedantam et al., 2015), <strong>METEOR</strong>, <strong>BLEU</strong>, and <strong>SPICE</strong> are the standard captioning evaluation metrics. CIDEr computes TF-IDF weighted n-gram similarity between generated and reference captions, specifically designed for captioning evaluation. Modern VLMs are typically evaluated on CIDEr for captioning benchmarks like MS COCO Captions and NoCaps.</p>
</li>
<li>
<p>Later captioning models incorporated <strong>bottom-up attention</strong> (Anderson et al., 2018), where an object detector (Faster R-CNN, Chapter 8) first proposes salient image regions, and the captioning model attends over these region features rather than a uniform grid. This was the dominant approach before ViT-based encoders took over.</p>
</li>
</ul>
<h2 id="architecture-patterns">Architecture Patterns<a class="headerlink" href="#architecture-patterns" title="Permanent link">&para;</a></h2>
<ul>
<li>Every VLM must answer a fundamental design question: at what point do vision and language interact? The answer defines the model's architecture family. There are three primary patterns, each with distinct trade-offs.</li>
</ul>
<h3 id="dual-encoder">Dual Encoder<a class="headerlink" href="#dual-encoder" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Imagine two translators working independently  one reads a French document, the other reads an English document  and they each produce a summary in a shared "universal language." They never communicate during translation, but their summaries are directly comparable. This is the <strong>dual encoder</strong> pattern.</p>
</li>
<li>
<p>A vision encoder <span class="arithmatex">\(f_v\)</span> and a text encoder <span class="arithmatex">\(f_t\)</span> independently map their respective inputs to a shared embedding space of dimension <span class="arithmatex">\(d\)</span>. The image embedding is <span class="arithmatex">\(v = f_v(I) \in \mathbb{R}^d\)</span> and the text embedding is <span class="arithmatex">\(t = f_t(q) \in \mathbb{R}^d\)</span>. Similarity is computed via a dot product or cosine similarity: <span class="arithmatex">\(\text{sim}(I, q) = v^T t / (\|v\| \|t\|)\)</span>.</p>
</li>
<li>
<p>CLIP (Radford et al., 2021), covered in the previous file on multimodal representations, is the prototypical dual encoder. It is trained with a contrastive objective (InfoNCE) on 400 million image-text pairs scraped from the internet. Because the encoders are independent, you can pre-compute and cache all image embeddings, making retrieval extremely efficient  you only need to encode the query text at search time.</p>
</li>
<li>
<p>The dual encoder's weakness is that vision and language never interact at the feature level. The model cannot perform fine-grained cross-modal reasoning: it cannot, for example, determine whether a specific word in the caption corresponds to a specific region in the image. This limits its usefulness for tasks like VQA or grounded captioning.</p>
</li>
</ul>
<h3 id="fusion-encoder">Fusion Encoder<a class="headerlink" href="#fusion-encoder" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>Now imagine the two translators are in the same room, actively discussing both documents. They can point at specific passages, ask each other questions, and build a joint understanding. This is the <strong>fusion encoder</strong> pattern.</p>
</li>
<li>
<p>Both modalities are encoded and then fused through <strong>cross-attention layers</strong> where tokens from one modality attend to tokens from the other. The image is first processed by a vision encoder into a sequence of patch or region tokens <span class="arithmatex">\(V = [v_1, \ldots, v_N]\)</span>. The text is tokenised into <span class="arithmatex">\(T = [t_1, \ldots, t_M]\)</span>. In the fusion layers, text tokens attend to image tokens via cross-attention:</p>
</li>
</ul>
<div class="arithmatex">\[\text{CrossAttn}(T, V) = \text{softmax}\!\left(\frac{(TW_Q)(VW_K)^T}{\sqrt{d_k}}\right)(VW_V)\]</div>
<ul>
<li>This enables fine-grained interaction: each text token can attend to the specific image regions it needs. Models like <strong>VisualBERT</strong>, <strong>VilBERT</strong>, and <strong>UNITER</strong> use this pattern. The cost is that you cannot pre-compute separate embeddings for retrieval  every image-text pair requires a full forward pass through the fusion layers.</li>
</ul>
<p><img alt="Dual encoder versus fusion encoder: the dual encoder computes separate embeddings and a similarity score, while the fusion encoder merges modalities through cross-attention layers" src="../../images/dual_vs_fusion_encoder.svg" /></p>
<h3 id="encoder-decoder">Encoder-Decoder<a class="headerlink" href="#encoder-decoder" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p>The <strong>encoder-decoder</strong> pattern combines the vision encoder with a text decoder that generates output tokens autoregressively, similar to the seq2seq models from Chapter 7. The vision encoder produces contextual image representations, and the text decoder cross-attends to them while generating output text.</p>
</li>
<li>
<p>This pattern naturally supports generative tasks: captioning, VQA with free-form answers, and visual dialogue. Models like <strong>GIT</strong> (Generative Image-to-text Transformer, Wang et al., 2022), <strong>CoCa</strong> (Contrastive Captioner, Yu et al., 2022), and <strong>PaLI</strong> use this architecture. CoCa cleverly combines the dual encoder and encoder-decoder patterns: the first half of the text decoder layers operate as a unimodal text encoder (for contrastive learning), while the second half cross-attend to image features (for generative captioning), getting the best of both worlds.</p>
</li>
<li>
<p>The choice among these three patterns depends on the target task. Dual encoders are optimal for retrieval at scale. Fusion encoders are best for fine-grained understanding tasks. Encoder-decoders are most versatile for generative tasks. Modern state-of-the-art VLMs increasingly adopt the encoder-decoder or decoder-only paradigm, treating every vision-language task as text generation.</p>
</li>
</ul>
<h2 id="flamingo-few-shot-multimodal-learning">Flamingo: Few-Shot Multimodal Learning<a class="headerlink" href="#flamingo-few-shot-multimodal-learning" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Think of a seasoned expert who, after years of studying both art and literature, can look at a completely new painting style and describe it eloquently after seeing just one or two examples. <strong>Flamingo</strong> (Alonso et al., 2022, DeepMind) is built on the same principle: it leverages a powerful pre-trained language model and a pre-trained vision encoder, connecting them with lightweight architectural components that enable few-shot learning on multimodal tasks.</p>
</li>
<li>
<p>Flamingo's design philosophy is conservative and effective: keep the pre-trained vision encoder (NFNet) and language model (Chinchilla) frozen, and learn only the "glue" that connects them. This glue consists of two components: a <strong>Perceiver Resampler</strong> and <strong>gated cross-attention layers</strong>.</p>
</li>
<li>
<p>The <strong>Perceiver Resampler</strong> takes the variable-length output of the vision encoder (which depends on image resolution) and compresses it into a fixed set of <span class="arithmatex">\(N\)</span> visual tokens (typically <span class="arithmatex">\(N = 64\)</span>). It works by initialising a set of <span class="arithmatex">\(N\)</span> learnable query vectors and using cross-attention to let these queries attend to the full set of vision encoder outputs. This is essentially the Perceiver architecture (Jaegle et al., 2021) applied as a bottleneck  it produces a compact, fixed-size visual representation regardless of the input image size.</p>
</li>
</ul>
<div class="arithmatex">\[z = \text{CrossAttn}(Q_{\text{learned}}, V_{\text{image}}) \in \mathbb{R}^{N \times d}\]</div>
<ul>
<li>The <strong>gated cross-attention layers</strong> are interleaved between the frozen language model layers. At each such layer, the language model's text tokens cross-attend to the visual tokens produced by the Perceiver Resampler. Critically, each gated cross-attention layer includes a learnable scalar gate <span class="arithmatex">\(\alpha\)</span>, initialised to zero, that multiplies the cross-attention output before adding it to the residual stream:</li>
</ul>
<div class="arithmatex">\[\hat{x} = x + \alpha \cdot \text{CrossAttn}(x, z)\]</div>
<ul>
<li>Initialising <span class="arithmatex">\(\alpha = 0\)</span> means that at the start of training, the cross-attention contributes nothing, and the model behaves exactly like the original frozen language model. The gates gradually open during training, smoothly integrating visual information without disrupting the language model's pre-trained representations.</li>
</ul>
<p><img alt="Flamingo architecture: frozen vision encoder feeds into a Perceiver Resampler that produces fixed-length visual tokens, which are injected into a frozen LM via gated cross-attention layers interleaved between LM blocks" src="../../images/flamingo_architecture.svg" /></p>
<ul>
<li>
<p>Flamingo natively handles <strong>interleaved image-text sequences</strong>. You can feed it a prompt containing multiple images interspersed with text, such as: "[Image 1] This is a cat. [Image 2] This is a dog. [Image 3] This is a ___." The model processes each image through the vision encoder and Perceiver Resampler, and the resulting visual tokens are inserted at the corresponding positions in the text sequence. The language model's causal attention mask ensures that each text token can only attend to visual tokens from the current and preceding images.</p>
</li>
<li>
<p>This interleaving enables powerful <strong>few-shot multimodal learning</strong>. By providing a few image-text examples in context, Flamingo can perform new tasks without any gradient updates. On benchmarks like VQAv2, OK-VQA, and captioning, Flamingo with 80B parameters achieved state-of-the-art few-shot performance, often matching or exceeding fine-tuned specialist models with just 4 or 32 examples.</p>
</li>
</ul>
<h2 id="llava-and-visual-instruction-tuning">LLaVA and Visual Instruction Tuning<a class="headerlink" href="#llava-and-visual-instruction-tuning" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Imagine you have a brilliant language expert (an LLM) and a brilliant art critic (a vision encoder). If you could teach the art critic to "speak the language expert's language," they could collaborate seamlessly. <strong>LLaVA</strong> (Large Language and Vision Assistant, Liu et al., 2023) does exactly this: it projects vision features into the LLM's token embedding space using a simple linear layer, then fine-tunes the whole system on instruction-following data.</p>
</li>
<li>
<p>LLaVA's architecture is strikingly simple. An image is encoded by a pre-trained CLIP ViT-L/14 vision encoder into a grid of patch features <span class="arithmatex">\(V \in \mathbb{R}^{N \times d_v}\)</span>, where <span class="arithmatex">\(N = 256\)</span> patches (for 336px images with 14px patches). A <strong>projection layer</strong> <span class="arithmatex">\(W\)</span> maps these vision features into the LLM's embedding dimension:</p>
</li>
</ul>
<div class="arithmatex">\[H_v = VW, \quad W \in \mathbb{R}^{d_v \times d_{\text{LLM}}}\]</div>
<ul>
<li>The projected visual tokens <span class="arithmatex">\(H_v\)</span> are simply concatenated with the text token embeddings and fed into the LLM (Vicuna, a fine-tuned LLaMA) as a single sequence. The LLM processes them with its standard causal self-attention  no special cross-attention layers, no perceiver, just concatenation. The visual tokens are treated as if they were text tokens that happen to encode visual information.</li>
</ul>
<p><img alt="LLaVA architecture: CLIP ViT encodes the image into patch features, a linear projection maps them to the LLM embedding space, projected visual tokens are prepended to text tokens and fed into the LLM" src="../../images/llava_architecture.svg" /></p>
<ul>
<li>
<p><strong>Visual instruction tuning</strong> is LLaVA's key training innovation. The authors used GPT-4 to generate 158,000 multimodal instruction-following examples from COCO images. Each example consists of an image paired with a conversational instruction (e.g., "Describe this image in detail," "What is unusual about this image?," "If I were a tourist visiting this place, what should I know?"). The model is trained to generate the GPT-4-authored response given the image and instruction.</p>
</li>
<li>
<p>Training proceeds in two stages. <strong>Stage 1 (pre-training)</strong>: only the projection layer <span class="arithmatex">\(W\)</span> is trained on image-caption pairs (595K from CC3M), while both the vision encoder and LLM are frozen. This teaches <span class="arithmatex">\(W\)</span> to align visual features with the LLM's embedding space. <strong>Stage 2 (fine-tuning)</strong>: the projection layer and the LLM are jointly fine-tuned on the instruction-following data, while the vision encoder stays frozen. This teaches the model to follow complex visual instructions.</p>
</li>
<li>
<p><strong>LLaVA-1.5</strong> improved the original with three key changes: replacing the single linear projection with a two-layer MLP (more expressive mapping), using higher-resolution images (336px instead of 224px, producing more patch tokens), and adding academic VQA datasets to the training mix. These seemingly minor modifications produced a large jump in benchmark performance.</p>
</li>
<li>
<p>The LLaVA approach demonstrated that you do not need complex architectural innovations like Flamingo's Perceiver Resampler or gated cross-attention. A simple linear projection, combined with high-quality instruction-tuning data, is enough to connect a vision encoder to an LLM effectively. This simplicity made LLaVA extremely influential  most subsequent open-source VLMs follow a similar recipe.</p>
</li>
</ul>
<h2 id="scaling-vision-language-models">Scaling Vision-Language Models<a class="headerlink" href="#scaling-vision-language-models" title="Permanent link">&para;</a></h2>
<ul>
<li>The field moved rapidly from proof-of-concept VLMs to industrial-scale systems trained on billions of image-text pairs. Three model families illustrate different approaches to scaling.</li>
</ul>
<h3 id="pali">PaLI<a class="headerlink" href="#pali" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>PaLI</strong> (Pathways Language and Image model, Chen et al., 2022, Google) scales both the vision encoder and the language model simultaneously. PaLI uses a ViT-e (4B parameters) as the vision encoder and mT5 (13B parameters) as the language model, for a total of 17B parameters. The image is encoded into a sequence of patch tokens, which are prepended to the text tokens and fed into the encoder-decoder mT5.</p>
</li>
<li>
<p>PaLI's key insight is that <strong>scaling the vision encoder matters as much as scaling the language model</strong>. Previous work typically used a fixed, moderate-sized vision backbone (e.g., ViT-B or ViT-L) and poured all the parameter budget into the LLM. PaLI showed that a 4B-parameter ViT-e, pre-trained on JFT-4B (4 billion labelled images), dramatically improves performance on fine-grained visual tasks like OCR and spatial reasoning.</p>
</li>
<li>
<p>PaLI is trained on WebLI, a dataset of 10 billion image-text pairs in 109 languages, making it inherently multilingual. The model is pre-trained with a mixture of tasks: image captioning, VQA, and image-text matching, all cast as text-to-text generation (following the T5 paradigm from Chapter 7). <strong>PaLI-X</strong> (55B parameters) and <strong>PaLI-3</strong> (5B, using SigLIP as the vision encoder) are subsequent iterations.</p>
</li>
</ul>
<h3 id="qwen-vl">Qwen-VL<a class="headerlink" href="#qwen-vl" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>Qwen-VL</strong> (Bai et al., 2023, Alibaba) builds on the Qwen LLM by adding a ViT vision encoder and a single-layer cross-attention module (similar to Flamingo's Perceiver Resampler) that compresses the vision encoder's output into a fixed set of 256 visual tokens. The visual tokens are concatenated with text tokens and processed by the Qwen LLM.</p>
</li>
<li>
<p>Qwen-VL's training uses a three-stage recipe. Stage 1: pre-train on 1.4 billion weakly-supervised image-text pairs with only the vision encoder unfrozen. Stage 2: multi-task pre-training on higher-quality data including VQA, captioning, grounding, and OCR datasets, with the full model unfrozen. Stage 3: supervised fine-tuning on instruction-following and dialogue data. This progressive refinement, from noisy web data to curated instruction data, is a pattern shared across most modern VLMs.</p>
</li>
<li>
<p><strong>Qwen2-VL</strong> (2024) introduced <strong>dynamic resolution</strong> support: instead of resizing all images to a fixed size, it processes images at their native resolution by dynamically adjusting the number of visual tokens. Higher-resolution images produce more tokens, and lower-resolution images produce fewer. This improves performance on detail-sensitive tasks like document understanding and fine-grained recognition without wasting computation on low-resolution inputs.</p>
</li>
</ul>
<h3 id="internvl">InternVL<a class="headerlink" href="#internvl" title="Permanent link">&para;</a></h3>
<ul>
<li>
<p><strong>InternVL</strong> (Chen et al., 2024, Shanghai AI Lab) scales the vision encoder aggressively, using InternViT-6B  a 6-billion-parameter vision transformer  paired with a language model. The key architectural contribution is <strong>dynamic high-resolution processing</strong>: images are divided into tiles of 448x448 pixels, each processed independently by the vision encoder, and the resulting tile features are concatenated with a thumbnail feature of the full image. This allows the model to handle images of arbitrary aspect ratios and resolutions.</p>
</li>
<li>
<p>InternVL-2 further introduced <strong>progressive alignment training</strong>: first aligning the vision encoder with a contrastive objective (like CLIP), then connecting it to the LLM through a lightweight MLP connector, and finally fine-tuning end-to-end on instruction data. The progressive strategy prevents catastrophic forgetting of the vision encoder's pre-trained representations.</p>
</li>
</ul>
<p><img alt="Scaling VLMs: comparison of PaLI, Qwen-VL, and InternVL showing different approaches to connecting vision encoders and language models, including their training stages" src="../../images/scaling_vlms_comparison.svg" /></p>
<ul>
<li>A common theme across all three families is the importance of <strong>training data curation</strong>. Raw web-scraped image-text pairs are noisy and often misaligned. Successive training stages progressively filter and refine the data, moving from billions of noisy pairs to millions of high-quality instruction examples. The quality of the final fine-tuning data often matters more than the model's raw parameter count.</li>
</ul>
<h2 id="grounding-and-referring">Grounding and Referring<a class="headerlink" href="#grounding-and-referring" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Imagine pointing at a person in a crowd and saying "the woman in the red hat." You are using language to refer to a specific spatial region. <strong>Visual grounding</strong> is the reverse: given an image and a natural language expression, the model must identify (localise) the referred object. <strong>Referring expression comprehension</strong> produces a bounding box; <strong>referring expression segmentation</strong> produces a pixel mask.</p>
</li>
<li>
<p>Formally, given an image <span class="arithmatex">\(I\)</span> and a referring expression <span class="arithmatex">\(r\)</span> (e.g., "the large brown dog on the left"), the model predicts a bounding box <span class="arithmatex">\(b = (x, y, w, h)\)</span> or a set of coordinates that localise the referent. The datasets include <strong>RefCOCO</strong>, <strong>RefCOCO+</strong>, and <strong>RefCOCOg</strong>, each containing images with multiple objects and unambiguous referring expressions for each.</p>
</li>
<li>
<p>Early grounding models used a two-stage approach: first generate region proposals (from Faster R-CNN or similar), then score each proposal against the language query using a fusion model. The highest-scoring region is the prediction. This is computationally expensive and limited by the quality of the proposals.</p>
</li>
<li>
<p>Modern VLMs integrate grounding directly into the generative framework. The key idea is to represent bounding box coordinates as <strong>text tokens</strong>. You discretise the continuous coordinate space into bins (e.g., 1000 bins for each of <span class="arithmatex">\(x, y, w, h\)</span>) and add special location tokens like <code>&lt;loc_342&gt;</code> to the vocabulary. The model then generates a bounding box by outputting a sequence of location tokens:</p>
</li>
</ul>
<div class="arithmatex">\[\text{Output: } \texttt{&lt;loc\_102&gt;&lt;loc\_215&gt;&lt;loc\_487&gt;&lt;loc\_398&gt;}\]</div>
<ul>
<li>
<p>This tokenisation trick allows any autoregressive language model to perform grounding without any architectural changes  it simply learns to "speak coordinates." <strong>Pix2Seq</strong> (Chen et al., 2022) pioneered this approach for object detection, and models like Qwen-VL, Ferret, and Kosmos-2 extend it to referring expression comprehension and phrase grounding.</p>
</li>
<li>
<p><strong>Kosmos-2</strong> (Peng et al., 2023, Microsoft) adds grounding capability to a multimodal LLM by representing spatial locations as special tokens embedded within the generated text. For example, it can generate: "A <code>&lt;phrase&gt;</code> golden retriever <code>&lt;/phrase&gt;</code> <code>&lt;box&gt;</code> <code>&lt;loc_102&gt;</code> <code>&lt;loc_215&gt;</code> <code>&lt;loc_487&gt;</code> <code>&lt;loc_398&gt;</code> <code>&lt;/box&gt;</code> is catching a frisbee." This interleaving of text and spatial tokens enables simultaneous captioning and grounding.</p>
</li>
</ul>
<p><img alt="Grounding via coordinate tokenisation: the model generates text tokens interspersed with discretised bounding box coordinate tokens, localising objects mentioned in the caption" src="../../images/grounding_coordinate_tokens.svg" /></p>
<ul>
<li><strong>Pointing</strong> takes grounding further: instead of bounding boxes, the model predicts a single point (typically the centre of the referred object). This is useful for interactive applications where a user asks "Where is the nearest exit?" and the model responds with a coordinate overlaid on the image. Models like <strong>Shikra</strong> and <strong>Ferret</strong> support point-based referring in addition to box-based grounding.</li>
</ul>
<h2 id="ocr-free-document-understanding">OCR-Free Document Understanding<a class="headerlink" href="#ocr-free-document-understanding" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Traditional document understanding pipelines are complex: first run an OCR engine to extract text and layout, then feed the extracted text into a language model. This multi-stage approach is fragile  OCR errors propagate downstream, and the spatial layout information is often lost or poorly represented. What if the model could read directly from pixels, the way you do?</p>
</li>
<li>
<p><strong>Donut</strong> (Document Understanding Transformer, Kim et al., 2022) eliminates OCR entirely. It uses a Swin Transformer (Chapter 8) as the vision encoder to process the document image, and a BART-style Transformer decoder to generate structured text output directly from the visual features. The decoder can produce JSON, key-value pairs, or plain text, depending on the task.</p>
</li>
<li>
<p>Donut's training is two-stage. <strong>Pre-training</strong>: the model learns to read by performing synthetic OCR  given a document image, it generates the full text content. This is trained on millions of synthetic document images rendered from text corpora, teaching the vision encoder to recognise characters, fonts, and layouts. <strong>Fine-tuning</strong>: the model is adapted to specific downstream tasks like receipt parsing, form understanding, or document classification, by training it to generate task-specific structured output.</p>
</li>
<li>
<p>The Donut decoder uses a special prompting scheme: the task is specified by a prompt token (e.g., <code>&lt;doc_class&gt;</code> for classification or <code>&lt;parse_receipt&gt;</code> for receipt parsing), and the model generates the output conditioned on this prompt. This unified interface allows a single model to handle multiple document understanding tasks.</p>
</li>
<li>
<p><strong>Pix2Struct</strong> (Lee et al., 2023, Google) takes the OCR-free idea and applies it to web page understanding and chart/figure comprehension. The key pre-training objective is <strong>screenshot parsing</strong>: given a masked screenshot of a web page, the model generates the underlying HTML that produced the visible region. This teaches the model to understand the relationship between visual rendering and structured markup.</p>
</li>
<li>
<p>Pix2Struct introduces <strong>variable-resolution input processing</strong>: instead of resizing all images to a fixed size (which distorts aspect ratios and destroys fine text), it packs the image into a fixed number of patches while preserving the original aspect ratio. A tall, narrow document produces a tall, narrow patch grid. This is critical for document understanding, where aspect ratio carries semantic information (a receipt is narrow and tall; a spreadsheet is wide and short).</p>
</li>
</ul>
<p><img alt="OCR-free document understanding: Donut and Pix2Struct process document images directly through a vision encoder and generate structured text output without any OCR preprocessing" src="../../images/ocr_free_document_understanding.svg" /></p>
<ul>
<li>
<p><strong>Nougat</strong> (Blecher et al., 2023, Meta) applies the Donut architecture specifically to academic papers, generating full LaTeX markup directly from PDF page images. It can handle complex mathematical equations, tables, and figures  tasks where traditional OCR pipelines struggle badly. The model is trained on pairs of PDF page images and their corresponding LaTeX source code.</p>
</li>
<li>
<p>The success of OCR-free models demonstrates a broader principle in deep learning: end-to-end models that learn directly from raw inputs (pixels) often outperform complex multi-stage pipelines, because they can jointly optimise all components and learn representations that are specifically tailored to the final task. The intermediate OCR step is a bottleneck that constrains what the model can learn.</p>
</li>
</ul>
<h2 id="the-visual-token-pipeline">The Visual Token Pipeline<a class="headerlink" href="#the-visual-token-pipeline" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>Regardless of architecture family, every VLM must convert an image into a sequence of tokens that a language model can process. Understanding this pipeline is essential. The process varies by model, but the general flow is:</p>
</li>
<li>
<p><strong>Step 1: Patch extraction.</strong> The image (height <span class="arithmatex">\(H\)</span>, width <span class="arithmatex">\(W\)</span>) is divided into non-overlapping patches of size <span class="arithmatex">\(P \times P\)</span>, producing <span class="arithmatex">\(N = HW / P^2\)</span> patches. For a 336x336 image with 14x14 patches, <span class="arithmatex">\(N = 576\)</span>.</p>
</li>
<li>
<p><strong>Step 2: Vision encoding.</strong> Each patch is linearly projected and passed through the vision encoder (typically a ViT). The output is a sequence of contextual patch embeddings <span class="arithmatex">\(V = [v_1, \ldots, v_N] \in \mathbb{R}^{N \times d_v}\)</span>. These embeddings carry both local appearance information and global context (from self-attention).</p>
</li>
<li>
<p><strong>Step 3: Token compression (optional).</strong> Some models compress the <span class="arithmatex">\(N\)</span> visual tokens into a smaller set of <span class="arithmatex">\(M \ll N\)</span> tokens to reduce the computational burden on the language model. Flamingo uses a Perceiver Resampler (<span class="arithmatex">\(M = 64\)</span>); Qwen-VL uses cross-attention (<span class="arithmatex">\(M = 256\)</span>); <strong>Q-Former</strong> (used in BLIP-2, Li et al., 2023) uses a set of <span class="arithmatex">\(M = 32\)</span> learnable query tokens that cross-attend to the vision encoder's output.</p>
</li>
<li>
<p><strong>Step 4: Projection.</strong> The visual tokens (either the full set or the compressed set) are projected into the language model's embedding space via a linear layer or MLP. After projection, visual tokens have the same dimensionality as text token embeddings and can be concatenated with them.</p>
</li>
<li>
<p><strong>Step 5: Injection into the LLM.</strong> The projected visual tokens are inserted into the token sequence at the position of a special <code>&lt;image&gt;</code> placeholder token, and the combined sequence is processed by the language model. The LLM's self-attention allows text tokens to attend to visual tokens and vice versa.</p>
</li>
</ul>
<p><img alt="The visual token pipeline: image patches are extracted, encoded by a ViT, optionally compressed by a Perceiver or Q-Former, projected to the LLM dimension, and concatenated with text tokens" src="../../images/visual_token_pipeline.svg" /></p>
<ul>
<li>
<p>The number of visual tokens directly affects computational cost. Each visual token participates in the LLM's self-attention, which is quadratic in sequence length. A high-resolution image with many patches can produce hundreds or thousands of visual tokens, dominating the LLM's context window. This is why token compression is important: reducing 576 visual tokens to 64 cuts the visual contribution to attention by roughly 9x.</p>
</li>
<li>
<p><strong>BLIP-2</strong> (Li et al., 2023) is notable for its efficient bridging strategy. It introduces a lightweight <strong>Q-Former</strong> (a small Transformer with learnable queries) that sits between the frozen vision encoder and the frozen LLM. The Q-Former is the only trainable component  both the vision encoder and LLM remain frozen. It is pre-trained in two stages: first with image-text contrastive learning, matching, and captioning objectives (connecting it to the vision encoder), then with language generation objectives (connecting it to the LLM). This modular design allows BLIP-2 to plug any vision encoder into any LLM.</p>
</li>
</ul>
<h2 id="training-objectives">Training Objectives<a class="headerlink" href="#training-objectives" title="Permanent link">&para;</a></h2>
<ul>
<li>
<p>VLMs are trained with a combination of objectives, depending on the architecture pattern:</p>
</li>
<li>
<p><strong>Image-text contrastive loss (ITC):</strong> aligns image and text representations in a shared embedding space, as in CLIP. This is the primary objective for dual encoders and is often used as a pre-training objective for fusion models. The loss is the InfoNCE loss from the previous file.</p>
</li>
<li>
<p><strong>Image-text matching (ITM):</strong> a binary classification objective  given an image and a text, predict whether they match. Hard negatives (text that is similar but paired with a different image) make this task challenging and force the model to learn fine-grained alignment.</p>
</li>
<li>
<p><strong>Language modelling (LM):</strong> the standard autoregressive language modelling objective  predict the next token given all previous tokens. For VLMs, the "previous tokens" include the visual tokens, so the model learns to generate text conditioned on visual input. This is the primary objective for encoder-decoder and decoder-only VLMs.</p>
</li>
</ul>
<div class="arithmatex">\[\mathcal{L}_{\text{LM}} = -\sum_{t=1}^{T} \log p(w_t \mid w_{&lt;t}, V)\]</div>
<ul>
<li>
<p><strong>Prefix language modelling:</strong> a variant where the image and a text prefix are provided as context (not trained on), and the model is trained to generate only the continuation. This is used in models like PaLI and SimVLM.</p>
</li>
<li>
<p>Most modern VLMs combine multiple objectives during pre-training (e.g., ITC + ITM + LM in BLIP, ITC + LM in CoCa) and then fine-tune with a pure LM objective on instruction data.</p>
</li>
</ul>
<h2 id="coding-tasks-use-colab-or-notebook">Coding Tasks (use CoLab or notebook)<a class="headerlink" href="#coding-tasks-use-colab-or-notebook" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p>Implement a simple attention-based image captioning decoder. Use random "image features" as the encoder output and train the decoder to generate a fixed caption, observing how the attention weights shift across spatial positions at each decoding step.
<div class="highlight"><pre><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="c1"># Simulate a 4x4 spatial grid of image features (16 regions, dim=32)</span>
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">k3</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="n">img_features</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span>  <span class="c1"># 16 spatial regions, 32-dim</span>
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a><span class="c1"># Vocabulary: 0=&lt;start&gt;, 1=&quot;a&quot;, 2=&quot;red&quot;, 3=&quot;car&quot;, 4=&lt;end&gt;</span>
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="n">W_embed</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k2</span><span class="p">,</span> <span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span class="n">W_attn_q</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k3</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="mi">32</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>  <span class="c1"># query projection</span>
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a><span class="k">def</span><span class="w"> </span><span class="nf">attend</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">img_feats</span><span class="p">,</span> <span class="n">W_q</span><span class="p">):</span>
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute soft attention over image features given decoder state h.&quot;&quot;&quot;</span>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    <span class="n">query</span> <span class="o">=</span> <span class="n">h</span> <span class="o">@</span> <span class="n">W_q</span>  <span class="c1"># (32,)</span>
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>    <span class="n">scores</span> <span class="o">=</span> <span class="n">img_feats</span> <span class="o">@</span> <span class="n">query</span>  <span class="c1"># (16,)</span>
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>    <span class="n">weights</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>  <span class="c1"># (16,)</span>
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>    <span class="n">context</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">@</span> <span class="n">img_feats</span>  <span class="c1"># (32,)</span>
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>    <span class="k">return</span> <span class="n">context</span><span class="p">,</span> <span class="n">weights</span>
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a><span class="c1"># Simple GRU-like step (for illustration, just a linear + tanh)</span>
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a><span class="n">W_h</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="n">embed_dim</span> <span class="o">+</span> <span class="mi">32</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a><span class="k">def</span><span class="w"> </span><span class="nf">decode_step</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">word_idx</span><span class="p">,</span> <span class="n">img_feats</span><span class="p">):</span>
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>    <span class="n">context</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">attend</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">img_feats</span><span class="p">,</span> <span class="n">W_attn_q</span><span class="p">)</span>
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>    <span class="n">word_emb</span> <span class="o">=</span> <span class="n">W_embed</span><span class="p">[</span><span class="n">word_idx</span><span class="p">]</span>  <span class="c1"># (16,)</span>
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>    <span class="n">inp</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">word_emb</span><span class="p">,</span> <span class="n">context</span><span class="p">])</span>  <span class="c1"># (48,)</span>
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>    <span class="n">h_new</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">inp</span> <span class="o">@</span> <span class="n">W_h</span><span class="p">)</span>  <span class="c1"># (32,)</span>
<a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a>    <span class="k">return</span> <span class="n">h_new</span><span class="p">,</span> <span class="n">attn_weights</span>
<a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>
<a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a><span class="c1"># Run decoding for the sequence: &lt;start&gt; -&gt; &quot;a&quot; -&gt; &quot;red&quot; -&gt; &quot;car&quot; -&gt; &lt;end&gt;</span>
<a id="__codelineno-0-34" name="__codelineno-0-34" href="#__codelineno-0-34"></a><span class="n">target_seq</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<a id="__codelineno-0-35" name="__codelineno-0-35" href="#__codelineno-0-35"></a><span class="n">h</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">)</span>
<a id="__codelineno-0-36" name="__codelineno-0-36" href="#__codelineno-0-36"></a><span class="n">all_attn</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-0-37" name="__codelineno-0-37" href="#__codelineno-0-37"></a><span class="k">for</span> <span class="n">word_idx</span> <span class="ow">in</span> <span class="n">target_seq</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
<a id="__codelineno-0-38" name="__codelineno-0-38" href="#__codelineno-0-38"></a>    <span class="n">h</span><span class="p">,</span> <span class="n">attn_w</span> <span class="o">=</span> <span class="n">decode_step</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">word_idx</span><span class="p">,</span> <span class="n">img_features</span><span class="p">)</span>
<a id="__codelineno-0-39" name="__codelineno-0-39" href="#__codelineno-0-39"></a>    <span class="n">all_attn</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn_w</span><span class="p">)</span>
<a id="__codelineno-0-40" name="__codelineno-0-40" href="#__codelineno-0-40"></a>
<a id="__codelineno-0-41" name="__codelineno-0-41" href="#__codelineno-0-41"></a><span class="c1"># Visualise attention maps (reshaped to 4x4 grid) at each step</span>
<a id="__codelineno-0-42" name="__codelineno-0-42" href="#__codelineno-0-42"></a><span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;&lt;start&gt;&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="s2">&quot;car&quot;</span><span class="p">]</span>
<a id="__codelineno-0-43" name="__codelineno-0-43" href="#__codelineno-0-43"></a><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<a id="__codelineno-0-44" name="__codelineno-0-44" href="#__codelineno-0-44"></a><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">words</span><span class="p">)):</span>
<a id="__codelineno-0-45" name="__codelineno-0-45" href="#__codelineno-0-45"></a>    <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">all_attn</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<a id="__codelineno-0-46" name="__codelineno-0-46" href="#__codelineno-0-46"></a>    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Attending when</span><span class="se">\n</span><span class="s1">generating after &quot;</span><span class="si">{</span><span class="n">w</span><span class="si">}</span><span class="s1">&quot;&#39;</span><span class="p">)</span>
<a id="__codelineno-0-47" name="__codelineno-0-47" href="#__codelineno-0-47"></a>    <span class="n">ax</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
<a id="__codelineno-0-48" name="__codelineno-0-48" href="#__codelineno-0-48"></a><span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Attention Over Image Regions at Each Decoding Step&#39;</span><span class="p">)</span>
<a id="__codelineno-0-49" name="__codelineno-0-49" href="#__codelineno-0-49"></a><span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<a id="__codelineno-0-50" name="__codelineno-0-50" href="#__codelineno-0-50"></a><span class="c1"># Try changing img_features to see how attention patterns shift!</span>
</code></pre></div></p>
</li>
<li>
<p>Simulate the visual token pipeline: patchify an image, project patches to an embedding space, concatenate with text token embeddings, and run a single self-attention layer over the combined sequence.
<div class="highlight"><pre><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="n">key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">7</span><span class="p">)</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="c1"># Create a synthetic 8x8 &quot;image&quot; with 3 channels</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">,</span> <span class="n">k3</span><span class="p">,</span> <span class="n">k4</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span class="n">image</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span class="c1"># Step 1: Patchify into 4x4 patches -&gt; 4 patches</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a><span class="n">patch_size</span> <span class="o">=</span> <span class="mi">4</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a><span class="n">patches</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a><span class="n">patches</span> <span class="o">=</span> <span class="n">patches</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">patch_size</span> <span class="o">*</span> <span class="n">patch_size</span> <span class="o">*</span> <span class="mi">3</span><span class="p">)</span>  <span class="c1"># (4, 48)</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Number of patches: </span><span class="si">{</span><span class="n">patches</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">, patch dim: </span><span class="si">{</span><span class="n">patches</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a><span class="c1"># Step 2: Project patches to embedding dim (d=16)</span>
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a><span class="n">d_model</span> <span class="o">=</span> <span class="mi">16</span>
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a><span class="n">W_patch</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k2</span><span class="p">,</span> <span class="p">(</span><span class="n">patches</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">d_model</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a><span class="n">visual_tokens</span> <span class="o">=</span> <span class="n">patches</span> <span class="o">@</span> <span class="n">W_patch</span>  <span class="c1"># (4, 16)</span>
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a>
<a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a><span class="c1"># Step 3: Create text token embeddings (simulate 3 text tokens)</span>
<a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a><span class="n">text_tokens</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k3</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>
<a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a>
<a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a><span class="c1"># Step 4: Concatenate visual + text tokens</span>
<a id="__codelineno-1-26" name="__codelineno-1-26" href="#__codelineno-1-26"></a><span class="n">combined</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">visual_tokens</span><span class="p">,</span> <span class="n">text_tokens</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (7, 16)</span>
<a id="__codelineno-1-27" name="__codelineno-1-27" href="#__codelineno-1-27"></a><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Combined sequence length: </span><span class="si">{</span><span class="n">combined</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2"> (4 visual + 3 text)&quot;</span><span class="p">)</span>
<a id="__codelineno-1-28" name="__codelineno-1-28" href="#__codelineno-1-28"></a>
<a id="__codelineno-1-29" name="__codelineno-1-29" href="#__codelineno-1-29"></a><span class="c1"># Step 5: Single-head self-attention over the combined sequence</span>
<a id="__codelineno-1-30" name="__codelineno-1-30" href="#__codelineno-1-30"></a><span class="n">W_Q</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k4</span><span class="p">,</span> <span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>
<a id="__codelineno-1-31" name="__codelineno-1-31" href="#__codelineno-1-31"></a><span class="n">k5</span><span class="p">,</span> <span class="n">k6</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">k4</span><span class="p">)</span>
<a id="__codelineno-1-32" name="__codelineno-1-32" href="#__codelineno-1-32"></a><span class="n">W_K</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k5</span><span class="p">,</span> <span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>
<a id="__codelineno-1-33" name="__codelineno-1-33" href="#__codelineno-1-33"></a><span class="n">W_V</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k6</span><span class="p">,</span> <span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.1</span>
<a id="__codelineno-1-34" name="__codelineno-1-34" href="#__codelineno-1-34"></a>
<a id="__codelineno-1-35" name="__codelineno-1-35" href="#__codelineno-1-35"></a><span class="n">Q</span> <span class="o">=</span> <span class="n">combined</span> <span class="o">@</span> <span class="n">W_Q</span>
<a id="__codelineno-1-36" name="__codelineno-1-36" href="#__codelineno-1-36"></a><span class="n">K</span> <span class="o">=</span> <span class="n">combined</span> <span class="o">@</span> <span class="n">W_K</span>
<a id="__codelineno-1-37" name="__codelineno-1-37" href="#__codelineno-1-37"></a><span class="n">V</span> <span class="o">=</span> <span class="n">combined</span> <span class="o">@</span> <span class="n">W_V</span>
<a id="__codelineno-1-38" name="__codelineno-1-38" href="#__codelineno-1-38"></a><span class="n">attn_scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span>
<a id="__codelineno-1-39" name="__codelineno-1-39" href="#__codelineno-1-39"></a><span class="n">attn_weights</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">attn_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (7, 7)</span>
<a id="__codelineno-1-40" name="__codelineno-1-40" href="#__codelineno-1-40"></a>
<a id="__codelineno-1-41" name="__codelineno-1-41" href="#__codelineno-1-41"></a><span class="n">output</span> <span class="o">=</span> <span class="n">attn_weights</span> <span class="o">@</span> <span class="n">V</span>  <span class="c1"># (7, 16)</span>
<a id="__codelineno-1-42" name="__codelineno-1-42" href="#__codelineno-1-42"></a>
<a id="__codelineno-1-43" name="__codelineno-1-43" href="#__codelineno-1-43"></a><span class="c1"># Visualise the cross-modal attention pattern</span>
<a id="__codelineno-1-44" name="__codelineno-1-44" href="#__codelineno-1-44"></a><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;V1&#39;</span><span class="p">,</span> <span class="s1">&#39;V2&#39;</span><span class="p">,</span> <span class="s1">&#39;V3&#39;</span><span class="p">,</span> <span class="s1">&#39;V4&#39;</span><span class="p">,</span> <span class="s1">&#39;T1&#39;</span><span class="p">,</span> <span class="s1">&#39;T2&#39;</span><span class="p">,</span> <span class="s1">&#39;T3&#39;</span><span class="p">]</span>
<a id="__codelineno-1-45" name="__codelineno-1-45" href="#__codelineno-1-45"></a><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<a id="__codelineno-1-46" name="__codelineno-1-46" href="#__codelineno-1-46"></a><span class="n">im</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Blues&#39;</span><span class="p">)</span>
<a id="__codelineno-1-47" name="__codelineno-1-47" href="#__codelineno-1-47"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">));</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<a id="__codelineno-1-48" name="__codelineno-1-48" href="#__codelineno-1-48"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">));</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_yticklabels</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<a id="__codelineno-1-49" name="__codelineno-1-49" href="#__codelineno-1-49"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Key&#39;</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Query&#39;</span><span class="p">)</span>
<a id="__codelineno-1-50" name="__codelineno-1-50" href="#__codelineno-1-50"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Self-Attention: Visual (V) and Text (T) Tokens&#39;</span><span class="p">)</span>
<a id="__codelineno-1-51" name="__codelineno-1-51" href="#__codelineno-1-51"></a><span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<a id="__codelineno-1-52" name="__codelineno-1-52" href="#__codelineno-1-52"></a><span class="c1"># Observe: text tokens attend to visual tokens (cross-modal attention)!</span>
</code></pre></div></p>
</li>
<li>
<p>Implement coordinate tokenisation for visual grounding. Given a bounding box, convert it to discrete tokens; given discrete tokens, reconstruct the bounding box. Visualise the quantisation error at different bin resolutions.
<div class="highlight"><pre><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="k">def</span><span class="w"> </span><span class="nf">encode_bbox</span><span class="p">(</span><span class="n">bbox</span><span class="p">,</span> <span class="n">num_bins</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert continuous bbox (x, y, w, h) in [0,1] to discrete tokens.&quot;&quot;&quot;</span>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>    <span class="n">tokens</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">bbox</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">num_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>    <span class="k">return</span> <span class="n">tokens</span>
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a><span class="k">def</span><span class="w"> </span><span class="nf">decode_bbox</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">num_bins</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a><span class="w">    </span><span class="sd">&quot;&quot;&quot;Convert discrete tokens back to continuous bbox.&quot;&quot;&quot;</span>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>    <span class="k">return</span> <span class="n">tokens</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_bins</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a><span class="c1"># Ground-truth bounding box (normalised to [0, 1])</span>
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a><span class="n">gt_bbox</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.123</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.333</span><span class="p">,</span> <span class="mf">0.222</span><span class="p">])</span>
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a><span class="c1"># Test quantisation at different bin resolutions</span>
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a><span class="n">bin_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">1000</span><span class="p">]</span>
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a><span class="n">errors</span> <span class="o">=</span> <span class="p">[]</span>
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a><span class="k">for</span> <span class="n">n_bins</span> <span class="ow">in</span> <span class="n">bin_sizes</span><span class="p">:</span>
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#__codelineno-2-20"></a>    <span class="n">tokens</span> <span class="o">=</span> <span class="n">encode_bbox</span><span class="p">(</span><span class="n">gt_bbox</span><span class="p">,</span> <span class="n">n_bins</span><span class="p">)</span>
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#__codelineno-2-21"></a>    <span class="n">reconstructed</span> <span class="o">=</span> <span class="n">decode_bbox</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">n_bins</span><span class="p">)</span>
<a id="__codelineno-2-22" name="__codelineno-2-22" href="#__codelineno-2-22"></a>    <span class="n">error</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">gt_bbox</span> <span class="o">-</span> <span class="n">reconstructed</span><span class="p">))</span>
<a id="__codelineno-2-23" name="__codelineno-2-23" href="#__codelineno-2-23"></a>    <span class="n">errors</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">error</span><span class="p">))</span>
<a id="__codelineno-2-24" name="__codelineno-2-24" href="#__codelineno-2-24"></a>    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Bins=</span><span class="si">{</span><span class="n">n_bins</span><span class="si">:</span><span class="s2">&gt;5d</span><span class="si">}</span><span class="s2"> | Tokens=</span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="s2"> | &quot;</span>
<a id="__codelineno-2-25" name="__codelineno-2-25" href="#__codelineno-2-25"></a>          <span class="sa">f</span><span class="s2">&quot;Reconstructed=</span><span class="si">{</span><span class="n">reconstructed</span><span class="si">}</span><span class="s2"> | Max error=</span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<a id="__codelineno-2-26" name="__codelineno-2-26" href="#__codelineno-2-26"></a>
<a id="__codelineno-2-27" name="__codelineno-2-27" href="#__codelineno-2-27"></a><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<a id="__codelineno-2-28" name="__codelineno-2-28" href="#__codelineno-2-28"></a><span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">bin_sizes</span><span class="p">,</span> <span class="n">errors</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;#e74c3c&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<a id="__codelineno-2-29" name="__codelineno-2-29" href="#__codelineno-2-29"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Bins&#39;</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Max Quantisation Error&#39;</span><span class="p">)</span>
<a id="__codelineno-2-30" name="__codelineno-2-30" href="#__codelineno-2-30"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Bounding Box Quantisation Error vs Bin Resolution&#39;</span><span class="p">)</span>
<a id="__codelineno-2-31" name="__codelineno-2-31" href="#__codelineno-2-31"></a><span class="n">ax</span><span class="o">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">);</span> <span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">)</span>
<a id="__codelineno-2-32" name="__codelineno-2-32" href="#__codelineno-2-32"></a><span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span> <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<a id="__codelineno-2-33" name="__codelineno-2-33" href="#__codelineno-2-33"></a><span class="c1"># Try: what happens with very few bins (e.g., 5)? When is the error acceptable?</span>
</code></pre></div></p>
</li>
</ol>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="Footer" >
        
          
          <a href="../01.%20multimodal%20representations/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Multimodal Representations">
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Previous
              </span>
              <div class="md-ellipsis">
                Multimodal Representations
              </div>
            </div>
          </a>
        
        
          
          <a href="../03.%20image%20and%20video%20tokenisation/" class="md-footer__link md-footer__link--next" aria-label="Next: Image and Video Tokenisation">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                Next
              </span>
              <div class="md-ellipsis">
                Image and Video Tokenisation
              </div>
            </div>
            <div class="md-footer__button md-icon">
              
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/HenryNdubuaku/maths-cs-ai-compendium" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.expand", "navigation.top", "navigation.footer", "search.suggest", "search.highlight", "content.code.copy", "toc.follow"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>