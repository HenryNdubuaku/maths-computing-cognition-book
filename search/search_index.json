{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Maths, CS &amp; AI Compendium","text":"<p>Read online: henryndubuaku.github.io/maths-cs-ai-compendium</p>"},{"location":"#overview","title":"Overview","text":"<p>Most textbooks bury good ideas under dense notation, skip the intuition, assume you already know half the material, and quickly get outdated in fast-moving fields like AI. This is an open, unconventional textbook covering maths, computing, and artificial intelligence from the ground up. Written for curious practitioners looking to deeply understand the stuff, not just survive an exam/interview. </p>"},{"location":"#background","title":"Background","text":"<p>Over the past years working in AI/ML, I filled notebooks with intuition first, real-world context, no hand-waving explanations of maths, computing and AI concepts. In 2025, a few friends used these notes to prep for interviews at DeepMind, OpenAI, Nvidia etc. They all got in and currently perform well in their roles. So I'm sharing to everyone. </p>"},{"location":"#outline","title":"Outline","text":"# Chapter Summary Status 01 Vectors Spaces, magnitude, direction, norms, metrics, dot/cross/outer products, basis, duality Available 02 Matrices Properties, special types, operations, linear transformations, decompositions (LU, QR, SVD) Available 03 Calculus Derivatives, integrals, multivariate calculus, Taylor approximation, optimisation and gradient descent Available 04 Statistics Descriptive measures, sampling, central limit theorem, hypothesis testing, confidence intervals Available 05 Probability Counting, conditional probability, distributions, Bayesian methods, information theory Available 06 Machine Learning Classical ML, gradient methods, deep learning, reinforcement learning, distributed training Available 07 Computational Linguistics syntax, semantics, pragmatics, NLP, language models, RNNs, CNNs, attention, transformers, text diffusion, text OCR, MoE, SSMs, modern LLM architectures, NLP evaluation Available 08 Computer Vision image processing, object detection, segmentation, video processing, SLAM, CNNs, vision transformers, diffusion, flow matching, VR/AR Available 09 Audio &amp; Speech DSP, ASR, TTS, voice &amp; acoustic activity detection, diarisation, source separation, active noise cancellation, wavenet, conformer Available 10 Multimodal Learning fusion strategies, contrastive learning, CLIP, VLMs, image/video tokenisation, cross-modal generation, unified architectures, world models Available 11 Autonomous Systems perception, robot learning, VLAs, self-driving cars, space robots Coming 12 Computing &amp; OS discreet maths, computer architecture, operating systems, RAM, concurrency, parallelism, programming languages Coming 13 Data Structures &amp; Algorithms arrays, trees, graph, search, sorting, hashmaps Coming 14 SIMD &amp; GPU Programming ARM &amp; NEON, X86 chips, RISC ships, GPUs, TPUs, triton, CUDA, Vulkan Coming 15 Systems Design systems design fundamentals, cloud computing, large scale infra, ML systems design examples Coming 16 Inference quantisation, streamingLLMs, continuous batching, edge inference, Coming 17 Intersecting Fields quantum ML, neuromorphic ML, AI for finace, AI for bio Coming"},{"location":"#citation","title":"Citation","text":"<pre><code>@book{ndubuaku2025compendium,\n  title     = {Maths, CS &amp; AI Compendium},\n  author    = {Henry Ndubuaku},\n  year      = {2026},\n  publisher = {GitHub},\n  url       = {https://github.com/HenryNdubuaku/maths-cs-ai-compendium}\n}\n</code></pre>"},{"location":"chapter%2001%3A%20vectors/01.%20vector%20spaces/","title":"Vector Spaces","text":"<p>Vector spaces form the mathematical playground where ML lives. This file covers vector addition, scalar multiplication, closure axioms, subspaces, and why nearly everything in AI is represented as vectors.</p> <ul> <li> <p>Think of a Vector Space as a specific kind of playground where mathematical objects live, and each object is called a vector. </p> </li> <li> <p>For geometric intuition in machine learning (ML), we will always think of vectors as a point in Euclidean space, represented by it's coordinates. </p> </li> <li> <p>The vector \\(\\mathbf{a}\\) (denoted mathematically as lowercase letters in bold) has \\(n\\) coordinates, each representing a position along an axis.</p> </li> </ul> \\[\\mathbf{a} = [a_1, a_2, a_3]\\] <p></p> <ul> <li> <p>The vectors in the vector space live under a very specific, unbreakable set of rules:</p> <ul> <li> <p>Vector Addition (Combining): You can take any two vectors and combine them to create a new one. Think of vectors as instructions for movement. If vector A means \"walk 3 steps forward\" and vector B means \"walk 2 steps right,\" adding them (A + B) creates a new, single instruction: \"walk 3 steps forward and 2 steps right.\"</p> </li> <li> <p>Scalar Multiplication (Scaling): You can take any vector and scale it using a regular number (a \"scalar\"). You can stretch it, shrink it, or reverse it. If vector A is \"walk 3 steps forward,\" scaling it by 2 makes it \"walk 6 steps forward.\" Scaling it by -1 flips it entirely to \"walk 3 steps backward.\"</p> </li> </ul> </li> <li> <p>The dimension of a vector space is the number of independent directions it contains. \\(\\mathbb{R}^2\\) is 2-dimensional (needs 2 coordinates), while \\(\\mathbf{a}\\) above lives in \\(\\mathbb{R}^3\\).</p> </li> <li> <p>We can for instance represent any object, say, a human, as a vector, where \\(h_1\\) = height in cm, \\(h_2\\) = weight in kg, \\(h_3\\) = age.</p> </li> </ul> \\[\\mathbf{h} = [185, 75, 30]\\] <ul> <li> <p>We have now created a vector space with a vector representing a human.</p> </li> <li> <p>We can represent multiple humans, and see how close or apart they are!</p> </li> </ul> <p></p> <ul> <li> <p>We can add more features, creating a rich representation of a human, often called feature vectors in ML.</p> </li> <li> <p>The more unique and meaningful features you have, the more descriptive the feature vector is, an important factor to remember. </p> </li> <li> <p>Beyond 3 dimensions, vectors become very difficult to visually inspect, inspiring a field of mathematics called Linear Algebra.</p> </li> <li> <p>Now, Linear algebra is the study of vectors, vector spaces and mappings between vectors.</p> </li> <li> <p>We represent almost every thing in AI/ML as vectors, making linear algebra the bedrock of the field.</p> </li> <li> <p>Vector addition can be performed by placing one vector on the tail of the other visually, and drawing from the origin to the endpoint.</p> </li> </ul> <p></p> <ul> <li> <p>For two vectors \\(\\mathbf{a} = (a_1, a_2)\\) and \\(\\mathbf{b} = (b_1, b_2)\\): \\(\\mathbf{a} + \\mathbf{b} = (a_1 + b_1, a_2 + b_2)\\)</p> </li> <li> <p>Vectors can also be subtracted, with all addition rules applying too.</p> </li> <li> <p>Multiplying a vector by a scalar scales the vector by that factor in the same direction.</p> </li> </ul> <p></p> <ul> <li> <p>For a scalar \\(c\\) and vector \\(\\mathbf{v} = (v_1, v_2)\\): \\(c\\mathbf{v} = (cv_1, cv_2)\\)</p> </li> <li> <p>Closure under Addition: If you add any two vectors from the vector space, the result is also a vector within the same space: If \\(\\mathbf{u} \\in V\\) and \\(\\mathbf{v} \\in V\\), then \\(\\mathbf{u} + \\mathbf{v} \\in V\\)</p> </li> <li> <p>Closure under Scalar Multiplication: If you multiply any vector from the vector space by a scalar, the result is a vector within the same space: If \\(\\mathbf{v} \\in V\\) and \\(c \\in F\\), then \\(c\\mathbf{v} \\in V\\)</p> </li> <li> <p>Associativity of Addition: For any three vectors \\(\\mathbf{u}\\), \\(\\mathbf{v}\\), and \\(\\mathbf{w}\\): \\((\\mathbf{u} + \\mathbf{v}) + \\mathbf{w} = \\mathbf{u} + (\\mathbf{v} + \\mathbf{w})\\)</p> </li> <li> <p>Commutativity of Addition: For any two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\): \\(\\mathbf{u} + \\mathbf{v} = \\mathbf{v} + \\mathbf{u}\\)</p> </li> </ul> <p></p> <ul> <li> <p>Both paths through the parallelogram arrive at the same point.</p> </li> <li> <p>(Zero Vector): There exists a vector \\(\\mathbf{0}\\) such that for any vector \\(\\mathbf{v}\\): \\(\\mathbf{v} + \\mathbf{0} = \\mathbf{v}\\)</p> </li> </ul> <p></p> <ul> <li>Additive Inverse: For every vector \\(\\mathbf{v}\\), there exists a vector \\(-\\mathbf{v}\\) such that: \\(\\mathbf{v} + (-\\mathbf{v}) = \\mathbf{0}\\)</li> </ul> <p></p> <ul> <li>Distributivity 1: For any scalar \\(c\\) and vectors \\(\\mathbf{u}\\), \\(\\mathbf{v}\\): \\(c(\\mathbf{u} + \\mathbf{v}) = c\\mathbf{u} + c\\mathbf{v}\\)</li> </ul> <p></p> <ul> <li> <p>Scaling the sum (gold) gives the same result as summing the scaled vectors.</p> </li> <li> <p>Distributivity 2: For any scalars \\(c\\), \\(d\\) and vector \\(\\mathbf{v}\\): \\((c + d)\\mathbf{v} = c\\mathbf{v} + d\\mathbf{v}\\)</p> </li> <li> <p>Associativity: For any scalars \\(c\\), \\(d\\) and vector \\(\\mathbf{v}\\): \\((cd)\\mathbf{v} = c(d\\mathbf{v})\\)</p> </li> <li> <p>Identity Element: For any vector \\(\\mathbf{v}\\): \\(1\\mathbf{v} = \\mathbf{v}\\), where \\(1\\) is the multiplicative identity in the field of scalars.</p> </li> <li> <p>A subspace is just a smaller playground inside the bigger one. Imagine 3D space as a room. A flat sheet of paper passing through the centre of the room is a subspace, and so is a single straight wire through the centre.</p> </li> <li> <p>The key requirement is that the subspace must pass through the origin. If you shift that sheet of paper off-centre, it stops being a subspace because the zero vector is no longer on it.</p> </li> </ul> <p></p> <ul> <li> <p>All the same rules from the vector space (addition, scaling, closure) still work inside a subspace. You can add or scale vectors within it and never \"fall off\" into the larger space.</p> </li> <li> <p>A line through the origin is a 1-dimensional subspace, a plane through the origin is a 2-dimensional subspace, and the full space is a subspace of itself.</p> </li> <li> <p>In ML, subspaces appear naturally. High-dimensional data often has structure that lives on a lower-dimensional subspace. Techniques like PCA find that subspace so we can work with the data more efficiently.</p> </li> </ul>"},{"location":"chapter%2001%3A%20vectors/01.%20vector%20spaces/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Run code to verify the distributivity property, then Modify and play around to test other rules! <pre><code>import jax.numpy as jnp\n\nu = jnp.array([1, 2])\nv = jnp.array([3, 0])\nc = 2\n\nlhs = c * (u + v)\nrhs = c*u + c*v\n\nprint(f\"LHS: {lhs}\")\nprint(f\"RHS: {rhs}\")\n</code></pre></p> </li> <li> <p>Run code to visualise different vectors, then modify values for different coordinates to understand how each axis affects position. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Try changing these vectors!\na = jnp.array([3, 2, 4])\nb = jnp.array([1, 4, 2])\nc = jnp.array([4, 1, 3])\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n\nfor vec, name, color in [(a, \"a\", \"red\"), (b, \"b\", \"blue\"), (c, \"c\", \"green\")]:\n    ax.quiver(0, 0, 0, *vec, color=color, arrow_length_ratio=0.1, linewidth=2, label=name)\n\nlim = int(jnp.abs(jnp.stack([a, b, c])).max()) + 1\nax.set_xlim([0, lim]); ax.set_ylim([0, lim]); ax.set_zlim([0, lim])\nax.set_xlabel(\"X\"); ax.set_ylabel(\"Y\"); ax.set_zlabel(\"Z\")\nax.legend()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2001%3A%20vectors/02.%20vector%20properties/","title":"Vector Properties","text":"<p>Vector properties describe the geometric and algebraic characteristics that define how vectors behave. This file covers magnitude, direction, unit vectors, equality, parallelism, orthogonality, and linear independence -- the building blocks of every ML feature space.</p> <ul> <li>The magnitude (or length) of a vector tells you how far it reaches. Think of it as the length of the arrow. For a vector \\(\\mathbf{a} = (a_1, a_2, a_3)\\), its magnitude is:</li> </ul> \\[\\|\\mathbf{a}\\| = \\sqrt{a_1^2 + a_2^2 + a_3^2}\\] <ul> <li> <p>This is just the Pythagorean theorem extended to higher dimensions and measuring the straight-line distance from the origin to the point.</p> </li> <li> <p>The direction of a vector tells you where it points; simply visualise a straight line from the origin to the coordinate's point. </p> </li> <li> <p>When origin is not explicitly specifies, we often imply (0,0,...0), the centerpoint, at least for visualisation purposes. </p> </li> <li> <p>Position doesn't matter, its always about displacement: a vector \\((3, 2)\\) drawn from the origin and the same \\((3, 2)\\) drawn from another point are still equal.</p> </li> </ul> <p></p> <ul> <li>Two vectors can have the same length but point in completely different directions, or point the same way but differ in length.</li> </ul> <p></p> <ul> <li>Two vectors are equal if and only if all their corresponding components match; same length, same direction, the exact same arrow.</li> </ul> \\[\\mathbf{a} = \\mathbf{b} \\iff a_i = b_i \\text{ for all } i\\] <ul> <li>Two vectors are parallel if one is a scalar multiple of the other. They point along the same line, either in the same direction or exactly opposite.</li> </ul> \\[\\mathbf{a} \\parallel \\mathbf{b} \\iff \\mathbf{a} = k\\mathbf{b} \\text{ for some scalar } k \\neq 0\\] <p></p> <ul> <li> <p>If \\(k &gt; 0\\), they point the same way. If \\(k &lt; 0\\), they point in opposite directions. Either way, they lie on the same line through the origin.</p> </li> <li> <p>Intuitively, parallel vectors carry no \"new\" directional information. One is just a stretched or flipped version of the other.</p> </li> <li> <p>Two vectors are orthogonal (perpendicular) if they point in completely independent directions. Moving along one gives you zero progress along the other.</p> </li> </ul> <p></p> <ul> <li> <p>Think of walking north and then walking east, these are orthogonal directions, no amount of walking north will ever move you east. We will encounter orthogonality very often.</p> </li> <li> <p>Orthogonality is central to ML: features that are orthogonal carry completely independent information, which is ideal for representation.</p> </li> <li> <p>More generally, any two vectors have an angle \\(\\theta\\) between them, ranging from \\(0\u00b0\\) to \\(180\u00b0\\).</p> </li> <li> <p>This angle captures the entire relationship between two directions: \\(0\u00b0\\) means parallel (same direction), \\(180\u00b0\\) means parallel (opposite direction), and \\(90\u00b0\\) means orthogonal. Everything in between is a blend.</p> </li> <li> <p>Most vector relationships in ML live somewhere in this spectrum. Later, we will see exact tools (dot product, cosine similarity) to compute this angle.</p> </li> <li> <p>A set of vectors is linearly dependent if at least one of them can be built from the others by scaling and adding. It brings no new information to the set.</p> </li> <li> <p>For example, if \\(\\mathbf{c} = 2\\mathbf{a} + 3\\mathbf{b}\\), then \\(\\mathbf{c}\\) is redundant, you already have everything \\(\\mathbf{c}\\) offers through \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\).</p> </li> <li> <p>Parallel vectors are always linearly dependent, since one is just a scaled copy of the other. Any set containing the zero vector is also linearly dependent.</p> </li> <li> <p>Vectors are linearly independent if none of them can be built from the others. Each one contributes a genuinely new direction. Orthogonal vectors are always linearly independent.</p> </li> <li> <p>In 2D, two linearly independent vectors can reach any point in the plane. In 3D, you need three. This idea of \"how many independent vectors you need\" connects directly to dimension.</p> </li> <li> <p>A vector is sparse when most of its components are zero. The opposite, most components being nonzero, is called dense.</p> </li> </ul> \\[\\mathbf{s} = [0, 0, 3, 0, 0, 0, 1, 0, 0, 0]\\] <ul> <li> <p>Sparsity matters because it affects both storage and computation. Sparse vectors can be stored and processed much more efficiently by only tracking the nonzero entries.</p> </li> <li> <p>A unit vector is a vector with magnitude exactly 1. It purely represents a direction with no length information. You can turn any vector into a unit vector by dividing by its magnitude:</p> </li> </ul> \\[\\hat{\\mathbf{a}} = \\frac{\\mathbf{a}}{\\|\\mathbf{a}\\|}\\] <ul> <li> <p>This process is called normalisation. It strips away \"how far\" and keeps only \"which way.\"</p> </li> <li> <p>The standard unit vectors point along each axis: \\(\\hat{\\mathbf{i}} = (1, 0, 0)\\), \\(\\hat{\\mathbf{j}} = (0, 1, 0)\\), \\(\\hat{\\mathbf{k}} = (0, 0, 1)\\). Any vector can be written as a combination of these, e.g. \\((3, 2, 4) = 3\\hat{\\mathbf{i}} + 2\\hat{\\mathbf{j}} + 4\\hat{\\mathbf{k}}\\).</p> </li> </ul>"},{"location":"chapter%2001%3A%20vectors/02.%20vector%20properties/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Compute the magnitude of a vector and verify it matches the Pythagorean theorem, then modify to compute the unit vector.  <pre><code>import jax.numpy as jnp\n\na = jnp.array([3.0, 4.0])\n\nmagnitude = jnp.sqrt(jnp.sum(a ** 2))\nprint(f\"Magnitude of a: {magnitude}\") \n</code></pre></p> </li> <li> <p>Check whether two vectors are parallel by testing if one is a scalar multiple of the other. <pre><code>import jax.numpy as jnp\n\na = jnp.array([2, 4, 6])\nb = jnp.array([1, 2, 3])\n\nratios = a / b\nprint(f\"Ratios: {ratios}\")\nprint(f\"Parallel: {jnp.allclose(ratios, ratios[0])}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2001%3A%20vectors/03.%20norms%20and%20metrics/","title":"Metrics and Norms","text":"<p>Norms measure the size of a vector; metrics measure the distance between two vectors. This file covers L1, L2, and L-infinity norms, Euclidean and cosine distance, and why choosing the right distance function is critical for k-NN, clustering, and retrieval in ML.</p> <ul> <li> <p>We know vectors have magnitude and direction. But how do we actually measure \"how big\" a single vector is, or \"how far apart\" two vectors are? This is where norms and metrics come in.</p> </li> <li> <p>In scalars, we know that 10 &gt; 5, because their values quantify them, but how can we quantify a vector? It's norm, it measures the size of a single vector. </p> </li> <li> <p>The most familiar norm is the Euclidean norm (L2), which is just the magnitude formula we already know:</p> </li> </ul> \\[\\|\\mathbf{v}\\|_2 = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}\\] <ul> <li>But there are other ways to measure size. Imagine you are in a city with a grid of streets. You cannot walk diagonally through buildings, so the \"length\" of your journey is the total blocks walked along each street. This is the Manhattan norm (L1):</li> </ul> \\[\\|\\mathbf{v}\\|_1 = |v_1| + |v_2| + \\cdots + |v_n|\\] <ul> <li>Or you might only care about the single largest component, ignoring the rest. This is the Max norm (L-infinity):</li> </ul> \\[\\|\\mathbf{v}\\|_\\infty = \\max(|v_1|, |v_2|, \\ldots, |v_n|)\\] <ul> <li>All three are special cases of the general Lp norm:</li> </ul> \\[\\|\\mathbf{v}\\|_p = (|v_1|^p + |v_2|^p + \\cdots + |v_n|^p)^{1/p}\\] <ul> <li> <p>Setting \\(p = 2\\) gives Euclidean, \\(p = 1\\) gives Manhattan, and as \\(p \\to \\infty\\) you get the Max norm. As \\(p\\) grows, the largest component contributes more and more, until eventually only it matters.</p> </li> <li> <p>Every norm must obey three rules:</p> <ul> <li> <p>Non-negativity: \\(\\|\\mathbf{v}\\| \\geq 0\\), and \\(\\|\\mathbf{v}\\| = 0\\) only if \\(\\mathbf{v} = \\mathbf{0}\\). Size is never negative, and only the zero vector has zero size.</p> </li> <li> <p>Scaling: \\(\\|c\\mathbf{v}\\| = |c| \\cdot \\|\\mathbf{v}\\|\\). Doubling a vector doubles its size.</p> </li> <li> <p>Triangle inequality: \\(\\|\\mathbf{u} + \\mathbf{v}\\| \\leq \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|\\). The shortcut is never longer than going the long way round.</p> </li> </ul> </li> <li> <p>Now, a metric measures the distance between two vectors. Think of it as asking: \"how far apart are these two points?\"</p> </li> <li> <p>The simplest way to get a metric is to use a norm on the difference: \\(d(\\mathbf{u}, \\mathbf{v}) = \\|\\mathbf{u} - \\mathbf{v}\\|\\). Subtract the two vectors, then measure the size of what remains.</p> </li> <li> <p>Using the Euclidean norm this gives us the familiar Euclidean distance:</p> </li> </ul> \\[d(\\mathbf{u}, \\mathbf{v}) = \\sqrt{(u_1 - v_1)^2 + (u_2 - v_2)^2 + \\cdots + (u_n - v_n)^2}\\] <ul> <li> <p>Using the Manhattan norm gives Manhattan distance, the total difference along each axis, like counting city blocks between two locations.</p> </li> <li> <p>Every metric must obey four rules:</p> <ul> <li> <p>Non-negativity: \\(d(\\mathbf{u}, \\mathbf{v}) \\geq 0\\). Distance is never negative.</p> </li> <li> <p>Identity: \\(d(\\mathbf{u}, \\mathbf{v}) = 0\\) if and only if \\(\\mathbf{u} = \\mathbf{v}\\). Zero distance means the same point.</p> </li> <li> <p>Symmetry: \\(d(\\mathbf{u}, \\mathbf{v}) = d(\\mathbf{v}, \\mathbf{u})\\). The distance from A to B is the same as from B to A.</p> </li> <li> <p>Triangle inequality: \\(d(\\mathbf{u}, \\mathbf{w}) \\leq d(\\mathbf{u}, \\mathbf{v}) + d(\\mathbf{v}, \\mathbf{w})\\). Going directly is never longer than taking a detour.</p> </li> </ul> </li> <li> <p>So what is the relationship between the two? A norm measures one vector, a metric measures the gap between two. Every norm naturally creates a metric (by measuring the difference), but not every metric comes from a norm.</p> </li> <li> <p>For example, Hamming distance counts the number of positions where two vectors differ. It is a valid metric, but it does not come from any norm.</p> </li> <li> <p>In ML, choosing the right norm or metric matters. </p> </li> <li> <p>L2 distance squares each difference before summing, so a single large difference dominates the result.</p> </li> <li> <p>L1 distance sums the absolute differences, treating each one equally. A single large difference has less influence compared to L2.</p> </li> </ul>"},{"location":"chapter%2001%3A%20vectors/03.%20norms%20and%20metrics/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Compute L1, and L2 norms of the same vector. Try changing the values and notice which norm is most sensitive to large components vs many small ones. Then try computing the Lp norm for increasing values of p (e.g. 1, 2, 5, 10, 50, 100) and watch it converge towards the L-infinity value. <pre><code>import jax.numpy as jnp\n\nv = jnp.array([3.0, -4.0, 1.0])\n\nl1 = jnp.sum(jnp.abs(v))\nl2 = jnp.sqrt(jnp.sum(v ** 2))\n\nprint(f\"L1: {l1}, L2: {l2:.2f}\")\n</code></pre></p> </li> <li> <p>Compute the Euclidean and Manhattan distance between two vectors. Try moving the vectors closer or further apart and observe how each distance responds differently. <pre><code>import jax.numpy as jnp\n\nu = jnp.array([1.0, 2.0, 3.0])\nv = jnp.array([4.0, 0.0, 1.0])\n\neuclidean = jnp.sqrt(jnp.sum((u - v) ** 2))\nmanhattan = jnp.sum(jnp.abs(u - v))\n\nprint(f\"Euclidean: {euclidean:.2f}, Manhattan: {manhattan}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2001%3A%20vectors/04.%20products/","title":"Vector Products","text":"<p>Vector products are the fundamental operations for measuring similarity and computing projections. This file covers inner products, the dot product, cosine similarity, the cross product, and outer products -- operations that power attention mechanisms, embeddings, and geometric reasoning in AI.</p> <ul> <li> <p>We have seen how to add and scale vectors. But can we multiply two vectors together? It turns out there is more than one way to do it, and each answers a different question.</p> </li> <li> <p>An inner product is the general idea: a function that takes two vectors and produces a single number (a scalar). It is the abstract blueprint for \"multiplying\" vectors.</p> </li> <li> <p>Any inner product must satisfy three rules:</p> <ul> <li> <p>Positive definiteness: \\(\\langle \\mathbf{v}, \\mathbf{v} \\rangle \\geq 0\\), and equals zero only for the zero vector. Multiplying a vector with itself always gives a non-negative result.</p> </li> <li> <p>Symmetry: \\(\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\langle \\mathbf{v}, \\mathbf{u} \\rangle\\). The order does not matter.</p> </li> <li> <p>Linearity: \\(\\langle a\\mathbf{u} + b\\mathbf{v}, \\mathbf{w} \\rangle = a\\langle \\mathbf{u}, \\mathbf{w} \\rangle + b\\langle \\mathbf{v}, \\mathbf{w} \\rangle\\). It distributes over addition and scaling.</p> </li> </ul> </li> <li> <p>The dot product is the most common inner product. It is the concrete version you will use almost everywhere. For two vectors \\(\\mathbf{a} = (a_1, a_2, \\ldots, a_n)\\) and \\(\\mathbf{b} = (b_1, b_2, \\ldots, b_n)\\):</p> </li> </ul> \\[\\mathbf{a} \\cdot \\mathbf{b} = a_1 b_1 + a_2 b_2 + \\cdots + a_n b_n\\] <ul> <li> <p>Multiply matching components, then add everything up. That is all it is.</p> </li> <li> <p>But what does this number mean? The dot product has a beautiful geometric interpretation:</p> </li> </ul> \\[\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\, \\|\\mathbf{b}\\| \\cos(\\theta)\\] <p></p> <ul> <li> <p>This connects the dot product directly to the angle \\(\\theta\\) between the two vectors. The result tells you how much the two vectors \"agree\" in direction.</p> </li> <li> <p>If they point the same way (\\(\\theta = 0\u00b0\\)), \\(\\cos(\\theta) = 1\\) and the dot product is maximised.</p> </li> <li> <p>If they are orthogonal (\\(\\theta = 90\u00b0\\)), \\(\\cos(\\theta) = 0\\) and the dot product is exactly zero. This gives us a precise test for orthogonality.</p> </li> <li> <p>If they point in opposite directions (\\(\\theta = 180\u00b0\\)), \\(\\cos(\\theta) = -1\\) and the dot product is negative.</p> </li> <li> <p>A vector dotted with itself gives its magnitude squared: \\(\\mathbf{a} \\cdot \\mathbf{a} = \\|\\mathbf{a}\\|^2\\).</p> </li> <li> <p>The dot product also gives us projection, the shadow one vector casts onto another. The projection of \\(\\mathbf{a}\\) onto \\(\\mathbf{b}\\) is:</p> </li> </ul> \\[\\text{proj}_{\\mathbf{b}}(\\mathbf{a}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{b}\\|^2} \\, \\mathbf{b}\\] <ul> <li> <p>Think of shining a light straight down onto \\(\\mathbf{b}\\). The shadow of \\(\\mathbf{a}\\) on that line is the projection. It tells you how much of \\(\\mathbf{a}\\) lies in the direction of \\(\\mathbf{b}\\).</p> </li> <li> <p>Cosine similarity normalises the dot product by dividing out both magnitudes:</p> </li> </ul> \\[\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\, \\|\\mathbf{b}\\|}\\] <ul> <li> <p>This gives a value between \\(-1\\) and \\(1\\) that measures direction alignment, ignoring how long the vectors are. It is widely used in ML to compare things like documents, embeddings, and user preferences.</p> </li> <li> <p>Now, the dot product takes two vectors and returns a scalar. The cross product does the opposite, it takes two vectors and returns a new vector.</p> </li> <li> <p>The cross product \\(\\mathbf{a} \\times \\mathbf{b}\\) produces a vector that is perpendicular to both \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\):</p> </li> </ul> \\[\\mathbf{a} \\times \\mathbf{b} = (a_2 b_3 - a_3 b_2, \\; a_3 b_1 - a_1 b_3, \\; a_1 b_2 - a_2 b_1)\\] <ul> <li> <p>The cross product only works in 3D. While the dot product works in any number of dimensions, the cross product is specific to three-dimensional space.</p> </li> <li> <p>Its magnitude equals the area of the parallelogram formed by the two vectors:</p> </li> </ul> \\[\\|\\mathbf{a} \\times \\mathbf{b}\\| = \\|\\mathbf{a}\\| \\, \\|\\mathbf{b}\\| \\sin(\\theta)\\] <ul> <li> <p>Notice the pattern: the dot product uses \\(\\cos(\\theta)\\) and the cross product uses \\(\\sin(\\theta)\\). The dot product measures how much two vectors align, the cross product measures how much they differ in direction.</p> </li> <li> <p>The direction of the result follows the right-hand rule: curl the fingers of your right hand from \\(\\mathbf{a}\\) towards \\(\\mathbf{b}\\), and your thumb points in the direction of \\(\\mathbf{a} \\times \\mathbf{b}\\).</p> </li> <li> <p>Unlike the dot product, the cross product is not commutative: \\(\\mathbf{a} \\times \\mathbf{b} = -(\\mathbf{b} \\times \\mathbf{a})\\). Swapping the order flips the direction.</p> </li> <li> <p>If two vectors are parallel, their cross product is the zero vector (since \\(\\sin(0\u00b0) = 0\\)). No area, no perpendicular direction.</p> </li> <li> <p>What happens when you combine three vectors using both products? This gives us triple products.</p> </li> <li> <p>The scalar triple product \\(\\mathbf{a} \\cdot (\\mathbf{b} \\times \\mathbf{c})\\) first takes the cross product of two vectors, then dots the result with the third. The output is a single number that equals the volume of the parallelepiped (a slanted 3D box) formed by the three vectors.</p> </li> <li> <p>If the scalar triple product is zero, the three vectors are coplanar, they all lie in the same flat plane and form no volume.</p> </li> <li> <p>The order can be cycled without changing the result: \\(\\mathbf{a} \\cdot (\\mathbf{b} \\times \\mathbf{c}) = \\mathbf{b} \\cdot (\\mathbf{c} \\times \\mathbf{a}) = \\mathbf{c} \\cdot (\\mathbf{a} \\times \\mathbf{b})\\).</p> </li> <li> <p>The vector triple product \\(\\mathbf{a} \\times (\\mathbf{b} \\times \\mathbf{c})\\) applies the cross product twice and returns a vector. It expands neatly using the identity:</p> </li> </ul> \\[\\mathbf{a} \\times (\\mathbf{b} \\times \\mathbf{c}) = (\\mathbf{a} \\cdot \\mathbf{c})\\mathbf{b} - (\\mathbf{a} \\cdot \\mathbf{b})\\mathbf{c}\\] <ul> <li>The result always lies in the plane spanned by \\(\\mathbf{b}\\) and \\(\\mathbf{c}\\). Note that the cross product is not associative: \\(\\mathbf{a} \\times (\\mathbf{b} \\times \\mathbf{c}) \\neq (\\mathbf{a} \\times \\mathbf{b}) \\times \\mathbf{c}\\).</li> </ul>"},{"location":"chapter%2001%3A%20vectors/04.%20products/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Compute the dot product of two vectors and use it to find the angle between them. Try making them orthogonal, parallel, or opposite and see how the angle changes. <pre><code>import jax.numpy as jnp\n\na = jnp.array([1.0, 2.0, 3.0])\nb = jnp.array([4.0, -1.0, 2.0])\n\ndot = jnp.dot(a, b)\nangle = jnp.arccos(dot / (jnp.linalg.norm(a) * jnp.linalg.norm(b)))\n\nprint(f\"Dot product: {dot}\")\nprint(f\"Angle: {jnp.degrees(angle):.1f}\u00b0\")\n</code></pre></p> </li> <li> <p>Compute the cross product of two 3D vectors and verify the result is perpendicular to both by checking that its dot product with each original vector is zero. <pre><code>import jax.numpy as jnp\n\na = jnp.array([1.0, 0.0, 0.0])\nb = jnp.array([0.0, 1.0, 0.0])\n\ncross = jnp.cross(a, b)\n\nprint(f\"a x b = {cross}\")\nprint(f\"Perpendicular to a: {jnp.dot(cross, a) == 0}\")\nprint(f\"Perpendicular to b: {jnp.dot(cross, b) == 0}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2001%3A%20vectors/05.%20basis%20and%20duality/","title":"Basis and Duality","text":"<p>Bases define the coordinate systems of vector spaces, and duality reveals how linear functions act on vectors. This file covers linear independence, spanning sets, change of basis, dual spaces, and covectors -- concepts behind PCA, feature transforms, and attention queries in ML.</p> <ul> <li> <p>We have seen that vectors live in spaces with a certain number of dimensions. But what defines those dimensions? This is where basis vectors come in.</p> </li> <li> <p>A basis is a set of vectors that can build every other vector in the space through scaling and adding (linear combination), with no redundancy. They are the building blocks of the space.</p> </li> <li> <p>A basis must satisfy two conditions:</p> <ul> <li> <p>Linearly independent: No basis vector can be built from the others. Each one contributes a genuinely new direction.</p> </li> <li> <p>Spanning: Every vector in the space can be expressed as a combination of the basis vectors. Nothing is left out.</p> </li> </ul> </li> <li> <p>The number of vectors in a basis equals the dimension of the space. In \\(\\mathbb{R}^2\\) you need 2, in \\(\\mathbb{R}^3\\) you need 3, and so on.</p> </li> <li> <p>The most natural basis is the standard basis, the unit vectors along each axis:</p> <ul> <li>In \\(\\mathbb{R}^2\\): \\(\\hat{\\mathbf{i}} = (1, 0)\\) and \\(\\hat{\\mathbf{j}} = (0, 1)\\)</li> <li>In \\(\\mathbb{R}^3\\): \\(\\hat{\\mathbf{i}} = (1, 0, 0)\\), \\(\\hat{\\mathbf{j}} = (0, 1, 0)\\), \\(\\hat{\\mathbf{k}} = (0, 0, 1)\\)</li> </ul> </li> <li> <p>Any vector is just a weighted sum of these basis vectors. The vector \\((3, 2)\\) is really \\(3\\hat{\\mathbf{i}} + 2\\hat{\\mathbf{j}}\\). The weights (3 and 2) are the coordinates of the vector in that basis.</p> </li> <li> <p>But the standard basis is not the only valid basis. In \\(\\mathbb{R}^2\\), the vectors \\((1, 1)\\) and \\((-1, 1)\\) also form a basis. They are linearly independent and can reach any point in the plane. The same vector will just have different coordinates in this new basis.</p> </li> <li> <p>A change of basis re-expresses the same vector using different building blocks. The vector has not moved, we are just describing it from a different perspective.</p> </li> <li> <p>This is done by multiplying by a change of basis matrix \\(P\\), whose columns are the new basis vectors written in the old coordinates. To go back, multiply by \\(P^{-1}\\).</p> </li> <li> <p>In ML, change of basis appears frequently. PCA, for example, finds a new basis (the principal components) where the data is easier to understand, the axes align with the directions of greatest variation.</p> </li> <li> <p>Now, there is a deeper idea hiding here. When we write \\(\\mathbf{v} = (3, 2)\\), the coordinates 3 and 2 are really the result of \"measuring\" \\(\\mathbf{v}\\) along each basis direction. The first coordinate asks \"how much of \\(\\hat{\\mathbf{i}}\\) is in \\(\\mathbf{v}\\)?\", the second asks \"how much of \\(\\hat{\\mathbf{j}}\\)?\"</p> </li> <li> <p>Each of these measurements is a linear functional, a function that takes a vector and returns a single number. The collection of all such linear functionals forms the dual space \\(V^\\ast\\).</p> </li> <li> <p>Think of it this way: vectors are the objects, and linear functionals are the rulers that measure them. The dual space is the set of all possible rulers.</p> </li> <li> <p>For every basis \\(\\{\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_n\\}\\), there is a corresponding dual basis \\(\\{\\mathbf{e}_1^\\ast, \\mathbf{e}_2^\\ast, \\ldots, \\mathbf{e}_n^\\ast\\}\\). Each dual basis vector extracts exactly one coordinate:</p> </li> </ul> \\[ \\mathbf{e}_i^\\ast(\\mathbf{e}_j) = \\delta_{ij} = \\begin{cases} 1 &amp; \\text{if } i = j \\\\ 0 &amp; \\text{if } i \\neq j \\end{cases} \\] <ul> <li> <p>\\(\\mathbf{e}_1^\\ast\\) returns 1 when applied to \\(\\mathbf{e}_1\\) and 0 for everything else. It perfectly isolates the first coordinate.</p> </li> <li> <p>The dot product connects these two worlds. When you compute \\(\\mathbf{u} \\cdot \\mathbf{v}\\), you can think of one vector acting as a \"ruler\" measuring the other. The dot product \\(\\mathbf{u} \\cdot \\mathbf{v}\\) is the same as applying the linear functional defined by \\(\\mathbf{u}\\) to the vector \\(\\mathbf{v}\\).</p> </li> <li> <p>This means every vector secretly defines a linear functional, and every linear functional can be represented by a vector. In finite dimensions, the dual space is essentially a mirror image of the original space.</p> </li> <li> <p>Duality may seem abstract now, but it underlies many practical ideas: coordinates are dual basis evaluations, the dot product is a duality pairing, and transformations like attention in neural networks operate by having one set of vectors \"query\" another, which is duality in action.</p> </li> </ul>"},{"location":"chapter%2001%3A%20vectors/05.%20basis%20and%20duality/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Express a vector in two different bases and verify they represent the same point. Try creating your own basis and see what coordinates the vector gets. <pre><code>import jax.numpy as jnp\n\nv = jnp.array([3.0, 2.0])\n\n# Standard basis: coordinates are just the components\nprint(f\"Standard basis coords: {v}\")\n\n# New basis: (1,1) and (-1,1)\nP = jnp.array([[1.0, -1.0],\n               [1.0,  1.0]])\nnew_coords = jnp.linalg.solve(P, v)\nprint(f\"New basis coords: {new_coords}\")\n\n# Verify: reconstruct from new coords\nreconstructed = new_coords[0] * P[:, 0] + new_coords[1] * P[:, 1]\nprint(f\"Reconstructed: {reconstructed}\")\n</code></pre></p> </li> <li> <p>Verify the dual basis property: each dual basis vector extracts exactly one coordinate and returns zero for the others. <pre><code>import jax.numpy as jnp\n\n# Standard basis in R3\ne1 = jnp.array([1.0, 0.0, 0.0])\ne2 = jnp.array([0.0, 1.0, 0.0])\ne3 = jnp.array([0.0, 0.0, 1.0])\n\nv = jnp.array([5.0, 3.0, 7.0])\n\n# Each dot product extracts one coordinate\nprint(f\"e1 \u00b7 v = {jnp.dot(e1, v)}\")\nprint(f\"e2 \u00b7 v = {jnp.dot(e2, v)}\")\nprint(f\"e3 \u00b7 v = {jnp.dot(e3, v)}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2002%3A%20matrices/01.%20matrix%20properties/","title":"Matrix Properties","text":"<p>Matrices are the data structures that store datasets, encode transformations, and define every neural network layer. This file covers matrix dimensions, elements, transpose, trace, determinant, inverse, rank, and null space -- the foundational properties used throughout linear algebra and ML.</p> <ul> <li>At its core, a matrix is a rectangular grid of numbers arranged in rows and columns. If a vector is a single list of numbers, a matrix is a table of them.</li> </ul> \\[ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\] <ul> <li> <p>You can also think of a matrix as a stack of vectors.</p> </li> <li> <p>If a single person is described by the vector \\([\\text{age}, \\text{height}, \\text{weight}]\\), then three people form a matrix where each row is one person:</p> </li> </ul> \\[ \\begin{bmatrix} 25 &amp; 170 &amp; 65 \\\\ 30 &amp; 180 &amp; 80 \\\\ 22 &amp; 160 &amp; 55 \\end{bmatrix} \\] <ul> <li> <p>This matrix has 3 rows and 3 columns, so we call it a \\(3 \\times 3\\) matrix.</p> </li> <li> <p>Each number in the grid is called an element or entry, identified by its row and column: \\(A_{ij}\\) is the element in row \\(i\\), column \\(j\\).</p> </li> <li> <p>The transpose of a matrix flips it along its diagonal, turning rows into columns and columns into rows. If \\(A\\) is \\(m \\times n\\), then \\(A^T\\) is \\(n \\times m\\).</p> </li> </ul> \\[ A = \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\quad \\Rightarrow \\quad A^T = \\begin{bmatrix} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{bmatrix} \\] <ul> <li> <p>Multiplying a matrix by its transpose always gives a square matrix: \\(AA^T\\) is \\(m \\times m\\) and \\(A^TA\\) is \\(n \\times n\\).</p> </li> <li> <p>The trace of a square matrix is the sum of its diagonal elements: \\(\\text{tr}(A) = A_{11} + A_{22} + \\cdots + A_{nn}\\). The trace equals the sum of the eigenvalues (which we will see later).</p> </li> </ul> <p></p> <ul> <li> <p>For the matrix above, \\(\\text{tr}(A) = 1 + 4 + 9 = 14\\). Only the highlighted diagonal matters.</p> </li> <li> <p>If two matrices represent the same linear transformation under different bases, their traces will be the same. The trace is \"basis-independent.\"</p> </li> <li> <p>The rank of a matrix is the number of linearly independent rows (or equivalently, columns). It tells you how much \"useful information\" the matrix carries.</p> </li> <li> <p>For example, the following matrix has rank 2 because neither row is a multiple of the other:</p> </li> </ul> \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix} \\] <p>But this matrix has rank 1 because the second row is just twice the first, so it adds no new information:</p> \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; 4 \\end{bmatrix} \\] <ul> <li>A \\(5 \\times 3\\) matrix can have rank at most 3. If some rows are just scaled or combined versions of others, the rank drops. A matrix with maximum possible rank is called full rank.</li> </ul> <p></p> <ul> <li> <p>A square matrix is invertible (has an inverse) if and only if it is full rank.</p> </li> <li> <p>The rank is connected to the null space (the set of vectors that the matrix maps to zero) through the rank-nullity theorem: \\(\\text{rank}(A) + \\text{nullity}(A) = \\text{number of columns of } A\\). What the matrix keeps (rank) plus what it destroys (nullity) equals the total dimension.</p> </li> <li> <p>The column space of a matrix is the set of all possible outputs when you multiply the matrix by any vector. It is spanned by the columns of the matrix. If a matrix has 3 columns but only 2 are independent, the column space is a 2D plane, not all of 3D space.</p> </li> </ul> <p></p> <ul> <li> <p>The row space is the same idea but from the perspective of rows. The rank equals the dimension of both the column space and the row space, so they always agree.</p> </li> <li> <p>Together, the column space tells you \"what outputs can this matrix produce?\" and the null space tells you \"what inputs get mapped to zero?\" These two spaces completely describe what the matrix does.</p> </li> <li> <p>The determinant of a square matrix is a single number that captures how the matrix scales space. Think of a \\(2 \\times 2\\) matrix as transforming a unit square into a parallelogram. The determinant is the area of that parallelogram (with a sign).</p> </li> </ul> \\[ \\det\\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} = ad - bc \\] <p></p> <ul> <li>For example:</li> </ul> \\[ \\det\\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 3 \\end{bmatrix} = 2 \\cdot 3 - 1 \\cdot 0 = 6 \\] <p>The transformation stretches the unit square into a parallelogram with area 6.</p> <ul> <li> <p>If the determinant is positive, the transformation preserves orientation (things don't get \"flipped\"). If negative, it flips orientation (like a mirror reflection). If zero, the matrix squashes space into a lower dimension, collapsing the parallelogram to a line or point.</p> </li> <li> <p>A matrix with determinant zero is called singular. It has no inverse and has lost information permanently.</p> </li> <li> <p>For matrices larger than \\(2 \\times 2\\), the determinant is computed using minors and cofactors. The minor \\(M_{ij}\\) is the determinant of the smaller matrix you get by deleting row \\(i\\) and column \\(j\\).</p> </li> </ul> <p></p> <ul> <li> <p>The cofactor \\(C_{ij} = (-1)^{i+j} M_{ij}\\) attaches a sign to each minor (alternating like a checkerboard: \\(+, -, +, \\ldots\\)). The determinant of the full matrix is then the sum along any row or column: \\(\\det(A) = \\sum_j A_{1j} \\cdot C_{1j}\\). This is called cofactor expansion.</p> </li> <li> <p>The inverse of a square matrix \\(A\\), written \\(A^{-1}\\), is the matrix that undoes what \\(A\\) does: \\(AA^{-1} = A^{-1}A = I\\) (the identity matrix). Only non-singular matrices have inverses.</p> </li> <li> <p>For a \\(2 \\times 2\\) matrix, the inverse has a direct formula:</p> </li> </ul> \\[ \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix}^{-1} = \\frac{1}{ad - bc}\\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix} \\] <p>Notice the determinant in the denominator, which is why singular matrices (determinant zero) have no inverse.</p> <ul> <li> <p>The condition number measures how sensitive a matrix is to small changes in its input. It is defined as \\(\\kappa(A) = \\|A\\| \\cdot \\|A^{-1}\\|\\).</p> </li> <li> <p>A condition number close to 1 means the matrix is well-conditioned: small input changes produce small output changes. A large condition number means it is ill-conditioned: tiny errors get amplified enormously. Orthogonal and identity matrices have condition number 1, while singular matrices have infinite condition number.</p> </li> <li> <p>For example, the following matrix has condition number \\(10^8\\). One direction is scaled normally while the other is nearly squashed to zero, so small perturbations along that direction get wildly distorted:</p> </li> </ul> \\[ \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; 10^{-8} \\end{bmatrix} \\] <ul> <li>Just as vectors have norms (length), matrices have norms that measure their \"size.\" The most common is the Frobenius norm, which treats the matrix as a long vector and computes its length:</li> </ul> \\[ \\|A\\|_F = \\sqrt{\\sum_{i}\\sum_{j} A_{ij}^2} \\] <ul> <li>For example:</li> </ul> \\[ \\left\\|\\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix}\\right\\|_F = \\sqrt{1 + 4 + 9 + 16} = \\sqrt{30} \\approx 5.48 \\] <ul> <li> <p>The spectral norm \\(\\|A\\|_2\\) is the largest singular value of \\(A\\). It measures the maximum amount the matrix can stretch any unit vector. In ML, matrix norms are used for weight regularisation (penalising large weights) and monitoring training stability.</p> </li> <li> <p>A symmetric matrix \\(A\\) is positive definite if for every non-zero vector \\(\\mathbf{x}\\): \\(\\mathbf{x}^T A \\mathbf{x} &gt; 0\\). This quadratic form always produces a positive number.</p> </li> <li> <p>For example, the following matrix is positive definite:</p> </li> </ul> \\[ A = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix} \\] <p>Pick any vector, say \\(\\mathbf{x} = [1, -1]^T\\): \\(\\mathbf{x}^T A \\mathbf{x} = 2 - 1 - 1 + 3 = 3 &gt; 0\\). No matter which non-zero \\(\\mathbf{x}\\) you try, you always get a positive result.</p> <ul> <li> <p>Positive definite matrices are important because they guarantee that optimisation problems have a unique minimum.</p> </li> <li> <p>If the condition is relaxed to \\(\\mathbf{x}^T A \\mathbf{x} \\geq 0\\) (allowing zero), the matrix is positive semi-definite (PSD). PSD matrices come up constantly: covariance matrices, kernel matrices in SVMs, and Hessians at local minima are all PSD. The difference is that PSD allows some directions to be \"flat\" (zero curvature) rather than strictly curving upward.</p> </li> </ul>"},{"location":"chapter%2002%3A%20matrices/01.%20matrix%20properties/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Compute the trace, rank, and determinant of a matrix. Try making one row a multiple of another and see how rank and determinant change. <pre><code>import jax.numpy as jnp\n\nA = jnp.array([[1.0, 2.0],\n               [3.0, 4.0]])\n\nprint(f\"Trace: {jnp.trace(A)}\")\nprint(f\"Rank: {jnp.linalg.matrix_rank(A)}\")\nprint(f\"Determinant: {jnp.linalg.det(A):.2f}\")\n</code></pre></p> </li> <li> <p>Compute the inverse of a matrix, multiply it by the original, and verify you get the identity. Then try a singular matrix and observe what happens. <pre><code>import jax.numpy as jnp\n\nA = jnp.array([[1.0, 2.0],\n               [3.0, 4.0]])\n\nA_inv = jnp.linalg.inv(A)\nprint(f\"A * A_inv:\\n{A @ A_inv}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2002%3A%20matrices/02.%20matrix%20types/","title":"Matrix Types","text":"<p>Special matrix structures unlock computational shortcuts and mathematical guarantees. This file covers identity, diagonal, symmetric, triangular, orthogonal, positive definite, sparse, and stochastic matrices -- types that appear in covariance estimation, graph algorithms, regularisation, and Markov chains.</p> <ul> <li> <p>Not all matrices are the same. Different structures give matrices special properties that make them faster to compute with, easier to reason about, or both. Here are the types you will encounter most.</p> </li> <li> <p>A square matrix has the same number of rows and columns (\\(n \\times n\\)). Most of the interesting properties (determinant, eigenvalues, inverse) only apply to square matrices. </p> </li> <li> <p>The identity matrix \\(I\\) is a square matrix with 1s on the diagonal and 0s everywhere else. It is the \"do nothing\" transformation: \\(AI = IA = A\\) for any compatible matrix \\(A\\).</p> </li> </ul> \\[ I = \\begin{bmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] <ul> <li> <p>The zero matrix \\(O\\) has all elements equal to zero. It maps every vector to the zero vector, destroying all information.</p> </li> <li> <p>A diagonal matrix is all zeros except on the main diagonal. Multiplying a vector by a diagonal matrix simply scales each component independently, making it very efficient.</p> </li> </ul> \\[ D = \\begin{bmatrix} 3 &amp; 0 \\\\ 0 &amp; 7 \\end{bmatrix} \\] <ul> <li>A symmetric matrix equals its own transpose: \\(A = A^T\\), meaning \\(A_{ij} = A_{ji}\\). Symmetric matrices have the special property that their eigenvectors are always perpendicular to each other. Covariance matrices are always symmetric.</li> </ul> \\[ S = \\begin{bmatrix} 3 &amp; -1 \\\\ -1 &amp; 6 \\end{bmatrix} \\] <ul> <li>A triangular matrix has all zeros on one side of the diagonal. Lower triangular has zeros above, upper triangular has zeros below. They are essential for solving systems of equations efficiently through forward or back substitution.</li> </ul> \\[ L = \\begin{bmatrix} 2 &amp; 0 &amp; 0 \\\\ 1 &amp; 3 &amp; 0 \\\\ -1 &amp; 2 &amp; 4 \\end{bmatrix} \\qquad U = \\begin{bmatrix} 5 &amp; -1 &amp; 2 \\\\ 0 &amp; 1 &amp; 3 \\\\ 0 &amp; 0 &amp; -2 \\end{bmatrix} \\] <ul> <li> <p>The determinant of a triangular matrix is simply the product of its diagonal elements.</p> </li> <li> <p>An orthogonal matrix has the property that its transpose equals its inverse: \\(Q^TQ = QQ^T = I\\). </p> </li> <li> <p>This means you can \"undo\" the transformation just by transposing, which is computationally cheap. Its columns are orthonormal (unit length and mutually perpendicular).</p> </li> <li> <p>A sparse matrix has most of its elements equal to zero, while a dense matrix has most elements nonzero.</p> </li> </ul> <p></p> <ul> <li> <p>In practice, many real-world matrices are extremely sparse. </p> </li> <li> <p>A social network with a million users could be represented as a \\(10^6 \\times 10^6\\) matrix, but each person only connects to a handful of others, so nearly all entries are zero.</p> </li> </ul> <p></p> <ul> <li> <p>A permutation matrix is obtained by rearranging the rows of an identity matrix. Multiplying by it shuffles the elements of a vector. Every row and every column has exactly one 1 and the rest are 0s. </p> </li> <li> <p>For example, the matrix below moves element 3 to position 1, element 1 to position 2, and element 2 to position 3:</p> </li> </ul> \\[ P = \\begin{bmatrix} 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{bmatrix} \\] <ul> <li>A Toeplitz matrix has the same value along every diagonal (upper-left to lower-right). Notice how each diagonal is constant:</li> </ul> \\[ T = \\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; a &amp; b \\\\ e &amp; d &amp; a \\end{bmatrix} \\] <ul> <li> <p>This structure appears in signal processing and convolution, because sliding a fixed filter across a signal is equivalent to multiplying by a Toeplitz matrix.</p> </li> <li> <p>A circulant matrix is a special Toeplitz matrix where each row is a cyclic shift of the one above. When a row reaches the end, it wraps around:</p> </li> </ul> \\[ C = \\begin{bmatrix} 1 &amp; 3 &amp; 2 \\\\ 2 &amp; 1 &amp; 3 \\\\ 3 &amp; 2 &amp; 1 \\end{bmatrix} \\] <ul> <li> <p>Circulant matrices are closely connected to the discrete Fourier transform (DFT) and are central to how circular convolution works.</p> </li> <li> <p>A Hermitian matrix is the complex equivalent of a symmetric matrix: \\(A = A^\\ast\\) (where \\(A^\\ast\\) is the conjugate transpose). </p> </li> <li> <p>For real-valued matrices, Hermitian and symmetric are the same thing. You will encounter these in quantum computing and signal processing.</p> </li> <li> <p>A unitary matrix is the complex equivalent of an orthogonal matrix: \\(U^\\ast U = UU^\\ast = I\\). Just as orthogonal matrices preserve lengths in real spaces, unitary matrices preserve lengths in complex spaces.</p> </li> <li> <p>An idempotent matrix satisfies \\(A^2 = A\\). Applying the transformation twice is the same as applying it once, which makes it a projection. Once you have projected, projecting again changes nothing.</p> </li> <li> <p>A nilpotent matrix satisfies \\(A^k = O\\) (the zero matrix) for some power \\(k\\). Apply the transformation enough times and everything collapses to zero. For example:</p> </li> </ul> \\[ \\begin{bmatrix} 0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix}^2 = \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} \\] <ul> <li>A Boolean matrix (or binary matrix) contains only 0s and 1s. It represents yes/no relationships. For example, in a graph with 3 nodes, the adjacency matrix records which nodes are connected:</li> </ul> \\[ B = \\begin{bmatrix} 0 &amp; 1 &amp; 1 \\\\ 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{bmatrix} \\] <ul> <li> <p>Here, node 1 connects to nodes 2 and 3, but nodes 2 and 3 are not connected to each other.</p> </li> <li> <p>A Vandermonde matrix is built from consecutive powers of a set of values. Given values \\(x_1, x_2, x_3\\):</p> </li> </ul> \\[ V = \\begin{bmatrix} 1 &amp; x_1 &amp; x_1^2 \\\\ 1 &amp; x_2 &amp; x_2^2 \\\\ 1 &amp; x_3 &amp; x_3^2 \\end{bmatrix} \\] <ul> <li> <p>This structure appears in polynomial interpolation: finding the unique polynomial that passes through a given set of points.</p> </li> <li> <p>A Hessenberg matrix is \"almost\" triangular, with zeros below the first subdiagonal:</p> </li> </ul> \\[ H = \\begin{bmatrix} 4 &amp; 2 &amp; 1 \\\\ 3 &amp; 5 &amp; -1 \\\\ 0 &amp; 1 &amp; 6 \\end{bmatrix} \\] <ul> <li>It is a useful intermediate form for computing eigenvalues efficiently. Reducing a matrix to Hessenberg form first makes iterative algorithms converge faster.</li> </ul>"},{"location":"chapter%2002%3A%20matrices/02.%20matrix%20types/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Create an orthogonal matrix (rotation matrix), multiply it by its transpose, and verify you get the identity. Try different angles. <pre><code>import jax.numpy as jnp\n\ntheta = jnp.pi / 4\nQ = jnp.array([[jnp.cos(theta), -jnp.sin(theta)],\n               [jnp.sin(theta),  jnp.cos(theta)]])\n\nprint(f\"Q @ Q.T:\\n{Q @ Q.T}\")\nprint(f\"Determinant: {jnp.linalg.det(Q):.2f}\")\n</code></pre></p> </li> <li> <p>Create a symmetric matrix and verify that it equals its transpose. Then compute its eigenvalues and check that the eigenvectors are perpendicular. <pre><code>import jax.numpy as jnp\n\nS = jnp.array([[4.0, 2.0],\n               [2.0, 3.0]])\n\nprint(f\"Symmetric: {jnp.allclose(S, S.T)}\")\n\neigenvalues, eigenvectors = jnp.linalg.eigh(S)\nprint(f\"Eigenvalues: {eigenvalues}\")\nprint(f\"Dot product of eigenvectors: {jnp.dot(eigenvectors[:, 0], eigenvectors[:, 1]):.6f}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2002%3A%20matrices/03.%20operations/","title":"Matrix Operations","text":"<p>Matrix operations are the computational engine of deep learning. This file covers matrix addition, scalar multiplication, matrix-vector products, matrix multiplication, element-wise operations, Kronecker products, and broadcasting -- the operations behind every forward pass and gradient update.</p> <ul> <li> <p>Matrices can be added and scaled just like vectors.</p> </li> <li> <p>For addition, both matrices must have the same dimensions, and you add element by element:</p> </li> </ul> \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix} + \\begin{bmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{bmatrix} = \\begin{bmatrix} 6 &amp; 8 \\\\ 10 &amp; 12 \\end{bmatrix} \\] <ul> <li>For scalar multiplication, you multiply every element by the scalar:</li> </ul> \\[ 3 \\times \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix} = \\begin{bmatrix} 3 &amp; 6 \\\\ 9 &amp; 12 \\end{bmatrix} \\] <ul> <li>The simplest thing you can do with a matrix is multiply it by a vector. Matrix-vector multiplication \\(A\\mathbf{x}\\) combines the columns of \\(A\\) using the entries of \\(\\mathbf{x}\\) as weights:</li> </ul> \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix} \\begin{bmatrix} 5 \\\\ 6 \\end{bmatrix} = 5 \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} + 6 \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 17 \\\\ 39 \\end{bmatrix} \\] <ul> <li> <p>This is the core operation in ML. Every neural network layer computes \\(A\\mathbf{x} + \\mathbf{b}\\): a matrix times an input vector, plus a bias.</p> </li> <li> <p>The general case is matrix multiplication. Given \\(A\\) (\\(m \\times n\\)) and \\(B\\) (\\(n \\times p\\)), the product \\(C = AB\\) is an \\(m \\times p\\) matrix where each element is a dot product:</p> </li> </ul> \\[C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}\\] <ul> <li> <p>Each entry in the result is the dot product of a row from \\(A\\) with a column from \\(B\\). The inner dimensions must match (\\(n\\)), and the result takes the outer dimensions (\\(m \\times p\\)).</p> </li> <li> <p>Another way to see it: each column of the result is a weighted sum of the columns of \\(A\\), where the weights come from the corresponding column of \\(B\\). </p> </li> <li> <p>If \\(B\\) has column \\([2, 3]^T\\), the result column is \\(2 \\times (\\text{column 1 of } A) + 3 \\times (\\text{column 2 of } A)\\).</p> </li> <li> <p>A useful special case: multiplying a matrix by its transpose always gives a square matrix. \\(AA^T\\) is \\(m \\times m\\) and \\(A^TA\\) is \\(n \\times n\\):</p> </li> </ul> \\[ \\begin{bmatrix} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\begin{bmatrix} 1 &amp; 4 \\\\ 2 &amp; 5 \\\\ 3 &amp; 6 \\end{bmatrix} = \\begin{bmatrix} 14 &amp; 32 \\\\ 32 &amp; 77 \\end{bmatrix} \\] <ul> <li> <p>Matrix multiplication has important rules:</p> <ul> <li> <p>Not commutative: \\(AB \\neq BA\\) in general. The order matters.</p> </li> <li> <p>Associative: \\((AB)C = A(BC)\\). You can group multiplications however you like.</p> </li> <li> <p>Distributive: \\(A(B + C) = AB + AC\\).</p> </li> <li> <p>Identity: \\(AI = IA = A\\).</p> </li> </ul> </li> <li> <p>The Hadamard product (element-wise product) multiplies two matrices of the same size entry by entry, written \\(A \\odot B\\):</p> </li> </ul> \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 4 \\end{bmatrix} \\odot \\begin{bmatrix} 5 &amp; 6 \\\\ 7 &amp; 8 \\end{bmatrix} = \\begin{bmatrix} 5 &amp; 12 \\\\ 21 &amp; 32 \\end{bmatrix} \\] <ul> <li> <p>Unlike standard matrix multiplication, the Hadamard product is commutative (\\(A \\odot B = B \\odot A\\)) and requires both matrices to have the same dimensions. It is used heavily in ML for gating: multiplying element-wise by a mask of values between 0 and 1 controls how much of each entry \"passes through.\"</p> </li> <li> <p>The outer product of two vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) produces a matrix: \\(\\mathbf{u}\\mathbf{v}^T\\). Each entry is the product of one element from \\(\\mathbf{u}\\) and one from \\(\\mathbf{v}\\):</p> </li> </ul> \\[ \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix} \\begin{bmatrix} 4 &amp; 5 \\end{bmatrix} = \\begin{bmatrix} 4 &amp; 5 \\\\ 8 &amp; 10 \\\\ 12 &amp; 15 \\end{bmatrix} \\] <ul> <li> <p>The result always has rank 1, because every row is a scaled version of \\(\\mathbf{v}^T\\). Any matrix can be written as a sum of rank-1 outer products, which is exactly what SVD does (covered in decompositions).</p> </li> <li> <p>Matrix multiplication is computationally expensive. Multiplying two \\(n \\times n\\) matrices takes \\(O(n^3)\\) operations. For a \\(1000 \\times 1000\\) matrix, that is a billion multiplications.</p> </li> <li> <p>When matrices are sparse (mostly zeros), naive multiplication wastes time multiplying by zero. The Compressed Sparse Row (CSR) format stores only the nonzero elements along with their positions:</p> <ul> <li>Values: the nonzero entries in row order</li> <li>Column indices: which column each value belongs to</li> <li>Row offsets: where each row starts in the values list</li> </ul> </li> <li> <p>For example, the matrix:</p> </li> </ul> \\[ A = \\begin{bmatrix} 5 &amp; 0 &amp; 0 &amp; 2 \\\\ 0 &amp; 0 &amp; 3 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; -1 \\end{bmatrix} \\] <ul> <li> <p>Is stored as: values = [5, 2, 3, -1], columns = [0, 3, 2, 3], row offsets = [0, 2, 3, 4]. This skips all the zeros and makes sparse operations much faster.</p> </li> <li> <p>A core use of matrices is solving systems of linear equations. The system \\(A\\mathbf{x} = \\mathbf{b}\\) asks: \"what vector \\(\\mathbf{x}\\), when transformed by \\(A\\), produces \\(\\mathbf{b}\\)?\"</p> </li> <li> <p>For example, say you are buying fruit. Apples cost \\(x_1\\) dollars each and bananas cost \\(x_2\\) dollars each. You know that 2 apples and 1 banana cost $5, and 1 apple and 3 bananas cost $10. In matrix form:</p> </li> </ul> \\[ \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 10 \\end{bmatrix} \\] <ul> <li>Multiplying the matrix by the vector row by row (each row dotted with \\([x_1, x_2]^T\\)) gives two equations:</li> </ul> \\[2x_1 + 1x_2 = 5 \\qquad \\text{(row 1)} \\qquad \\qquad x_1 + 3x_2 = 10 \\qquad \\text{(row 2)}\\] <ul> <li> <p>From row 1, \\(x_2 = 5 - 2x_1\\). Substituting into row 2: \\(x_1 + 3(5 - 2x_1) = 10\\), which gives \\(x_1 = 1\\), then \\(x_2 = 3\\). Apples cost $1 and bananas cost $3.</p> </li> <li> <p>Verify \u2014 it checks out:</p> </li> </ul> \\[ \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 3 \\end{bmatrix} = \\begin{bmatrix} 2 + 3 \\\\ 1 + 9 \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 10 \\end{bmatrix} \\] <ul> <li> <p>If \\(A\\) has an inverse, the solution is simply \\(\\mathbf{x} = A^{-1}\\mathbf{b}\\). But computing the inverse directly is expensive and numerically unstable. In practice, we use decompositions instead.</p> </li> <li> <p>Not every matrix is square, and not every square matrix is invertible. The pseudo-inverse \\(A^+\\) generalises the inverse to any matrix. It always exists and provides the \"best possible\" inverse:</p> </li> </ul> \\[A^+ = (A^TA)^{-1}A^T\\] <ul> <li> <p>When \\(A\\) is lower triangular, solving \\(L\\mathbf{x} = \\mathbf{b}\\) is easy by forward substitution: solve for \\(x_1\\) first, then use it to find \\(x_2\\), and so on down.</p> </li> <li> <p>When \\(A\\) is upper triangular, solving \\(U\\mathbf{x} = \\mathbf{b}\\) works by back substitution: solve for the last variable first, then work upward.</p> </li> <li> <p>This is why decomposing a matrix into triangular factors (as we will see in decompositions) is so useful. It turns a hard problem into two easy ones.</p> </li> </ul>"},{"location":"chapter%2002%3A%20matrices/03.%20operations/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Multiply two matrices and verify the dimensions. Then swap the order and observe that the result changes (or that it fails if dimensions don't match). <pre><code>import jax.numpy as jnp\n\nA = jnp.array([[1.0, 2.0],\n               [3.0, 4.0]])\nB = jnp.array([[5.0, 6.0],\n               [7.0, 8.0]])\n\nprint(f\"A @ B:\\n{A @ B}\")\nprint(f\"B @ A:\\n{B @ A}\")\nprint(f\"Equal: {jnp.allclose(A @ B, B @ A)}\")\n</code></pre></p> </li> <li> <p>Solve a system of linear equations \\(A\\mathbf{x} = \\mathbf{b}\\) and verify the solution by multiplying back. Try changing \\(\\mathbf{b}\\) to see how the solution shifts. <pre><code>import jax.numpy as jnp\n\nA = jnp.array([[2.0, 1.0],\n               [5.0, 3.0]])\nb = jnp.array([4.0, 7.0])\n\nx = jnp.linalg.solve(A, b)\nprint(f\"Solution x: {x}\")\nprint(f\"A @ x: {A @ x}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2002%3A%20matrices/04.%20linear%20transformations/","title":"Linear Transformations","text":"<p>Every matrix multiplication is a linear transformation -- a function that reshapes, rotates, or projects vectors while preserving linearity. This file covers rotation, reflection, scaling, shearing, projection, the kernel and image of a map, and how neural network layers chain these transformations.</p> <ul> <li> <p>A linear transformation (or linear map) is a function that takes a vector and produces another vector, while preserving addition and scaling. If \\(T\\) is linear, then:</p> <ul> <li>\\(T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\\)</li> <li>\\(T(c\\mathbf{u}) = cT(\\mathbf{u})\\)</li> </ul> </li> <li> <p>Every linear transformation can be represented as multiplication by a matrix. The matrix is the transformation. When you multiply a vector by a matrix, you are applying a linear transformation to it.</p> </li> <li> <p>Think of a \\(2 \\times 2\\) matrix as a machine that takes in 2D vectors and outputs new 2D vectors. The columns of the matrix tell you where the standard basis vectors \\(\\hat{\\mathbf{i}}\\) and \\(\\hat{\\mathbf{j}}\\) end up after the transformation. Everything else follows from linearity.</p> </li> </ul> <p></p> <ul> <li>For example, if</li> </ul> \\[ A = \\begin{bmatrix} 2 &amp; 1 \\\\ 1 &amp; 2 \\end{bmatrix} \\] <p>then \\(\\hat{\\mathbf{i}} = [1, 0]^T\\) lands at \\([2, 1]^T\\) (column 1) and \\(\\hat{\\mathbf{j}} = [0, 1]^T\\) lands at \\([1, 2]^T\\) (column 2). Every other vector is a combination of these two, so its output follows automatically.</p> <ul> <li> <p>Multiplying two matrices can be thought of as applying one transformation after another. If \\(B\\) transforms vectors from one space and \\(A\\) transforms the result, then \\(AB\\) does both in sequence. In a game engine, rotating a character and then moving them forward is a different result from moving them first and then rotating, which is why matrix multiplication is not commutative.</p> </li> <li> <p>Rotation turns vectors by an angle \\(\\theta\\) without changing their length. The vector stays the same size, it just points in a new direction.</p> </li> </ul> <p></p> <ul> <li>In 2D, the rotation matrix is:</li> </ul> \\[ R(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta \\\\ \\sin\\theta &amp; \\cos\\theta \\end{bmatrix} \\] <ul> <li>For \\(\\theta = 90\u00b0\\):</li> </ul> \\[ R = \\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{bmatrix} \\] <p>so \\([1, 0]^T\\) becomes \\([0, 1]^T\\). The vector pointing right now points up. Rotation matrices are orthogonal and always have determinant 1. When you rotate a photo on your phone, this is the exact matrix being applied to every pixel coordinate.</p> <ul> <li>In 3D, there are separate rotation matrices for each axis. A robotic arm rotates each joint around a specific axis, and each joint is one rotation matrix. Rotation around the z-axis looks like the 2D case embedded in 3D:</li> </ul> \\[ R_z(\\theta) = \\begin{bmatrix} \\cos\\theta &amp; -\\sin\\theta &amp; 0 \\\\ \\sin\\theta &amp; \\cos\\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] <ul> <li>Scaling stretches or shrinks vectors along each axis independently:</li> </ul> \\[ S(s_x, s_y) = \\begin{bmatrix} s_x &amp; 0 \\\\ 0 &amp; s_y \\end{bmatrix} \\] <p></p> <ul> <li> <p>\\(S(2, 1.5)\\) doubles the x-component and multiplies the y-component by 1.5. Scaling by \\(-1\\) along an axis flips that component. A diagonal matrix is always a scaling transformation. When you resize an image to 50%, you are applying \\(S(0.5, 0.5)\\) to every pixel coordinate.</p> </li> <li> <p>Reflection flips vectors across an axis or line, like a mirror. Reflecting across the x-axis keeps the x-component and negates the y-component:</p> </li> </ul> \\[ \\text{Ref}_x = \\begin{bmatrix} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{bmatrix} \\] <p></p> <ul> <li>For example, \\([3, 2]^T\\) becomes \\([3, -2]^T\\). When your phone flips a selfie horizontally so text reads correctly, it is applying a reflection matrix. Reflecting across the line \\(y = x\\) swaps the two components:</li> </ul> \\[ \\text{Ref}_{y=x} = \\begin{bmatrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{bmatrix} \\] <ul> <li> <p>Reflection matrices have determinant \\(-1\\), confirming they flip orientation.</p> </li> <li> <p>Rotations and reflections are both rigid transformations: they preserve distances and angles. The matrices that represent them are orthogonal matrices, which is why orthogonal matrices always have determinant \\(+1\\) (rotation) or \\(-1\\) (reflection).</p> </li> <li> <p>Shearing skews vectors along one axis proportionally to the other. A horizontal shear by factor \\(k\\):</p> </li> </ul> \\[ \\text{Sh}_x(k) = \\begin{bmatrix} 1 &amp; k \\\\ 0 &amp; 1 \\end{bmatrix} \\] <p></p> <ul> <li> <p>Each point slides horizontally by \\(k\\) times its height. With \\(k = 0.5\\), a point at height 2 shifts right by 1. The bottom row stays put, the top row slides. This is how italic text works: upright letters are sheared so they slant to the right.</p> </li> <li> <p>All of the above (rotation, scaling, reflection, shearing) are linear transformations. They keep the origin fixed and preserve straight lines. But what about translation (shifting everything by a fixed amount)?</p> </li> <li> <p>Translation is not a linear transformation because it moves the origin. If you shift every point right by 3, the zero vector moves to \\([3, 0]^T\\), breaking linearity. To handle it, we use an affine transformation, which combines a linear transformation with a translation:</p> </li> </ul> \\[\\mathbf{y} = A\\mathbf{x} + \\mathbf{t}\\] <ul> <li>To represent this as a single matrix multiplication, we use homogeneous coordinates: add an extra 1 to every vector and use an \\((n+1) \\times (n+1)\\) matrix:</li> </ul> \\[ \\begin{bmatrix} A &amp; \\mathbf{t} \\\\ \\mathbf{0}^T &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} A\\mathbf{x} + \\mathbf{t} \\\\ 1 \\end{bmatrix} \\] <ul> <li> <p>Affine transformations preserve straight lines and parallelism, but not necessarily angles or lengths. Every object in a video game is positioned using affine transformations: rotate it, scale it, then place it at the right location, all encoded in a single matrix.</p> </li> <li> <p>A degenerate transformation (singular matrix) collapses space into a lower dimension. </p> </li> <li> <p>For example, the matrix</p> </li> </ul> \\[ \\begin{bmatrix} 1 &amp; 2 \\\\ 2 &amp; 4 \\end{bmatrix} \\] <p>maps every 2D vector onto a single line, because both columns point in the same direction. The determinant is zero, information is lost, and the transformation cannot be undone.</p> <ul> <li> <p>Converting a colour image (3 values per pixel: red, green, blue) to grayscale (1 value per pixel) is a degenerate transformation: the colour information is permanently gone.</p> </li> <li> <p>In ML, linear transformations are the core of neural networks, data is represented as a matrix (a stack of vectors representing features of an object like humans, planes, text, image...anything!)</p> </li> <li> <p>Each layer applies a matrix multiplication (linear transformation), details are provided in other chapters, we need to explain hpw to structure these data and motivate neural networks properly. </p> </li> <li> <p>However, the most used techniques today often almost exclusively passes the data through a bunch of linear transformations, we call these Transformers. </p> </li> <li> <p>Gemini, ChatGPT, Claude, Qwen, DeepSeek and the best performing AI in the world today, are transformers!</p> </li> </ul>"},{"location":"chapter%2002%3A%20matrices/04.%20linear%20transformations/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Apply a rotation matrix to a vector and plot both the original and rotated vector. Try different angles. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ntheta = jnp.pi / 3\nR = jnp.array([[jnp.cos(theta), -jnp.sin(theta)],\n               [jnp.sin(theta),  jnp.cos(theta)]])\n\nv = jnp.array([1.0, 0.0])\nv_rot = R @ v\n\nplt.figure(figsize=(5, 5))\nplt.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1, color='red', label='original')\nplt.quiver(0, 0, v_rot[0], v_rot[1], angles='xy', scale_units='xy', scale=1, color='blue', label='rotated')\nplt.xlim(-1.5, 1.5); plt.ylim(-1.5, 1.5)\nplt.grid(True); plt.legend(); plt.gca().set_aspect('equal')\nplt.show()\n</code></pre></p> </li> <li> <p>Apply a shearing transformation to a set of points forming a square and visualise the deformed shape. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nsquare = jnp.array([[0,0],[1,0],[1,1],[0,1],[0,0]]).T\n\nk = 0.5\nshear = jnp.array([[1, k],\n                    [0, 1]])\nsheared = shear @ square\n\nplt.figure(figsize=(6, 4))\nplt.plot(square[0], square[1], 'r-o', label='original')\nplt.plot(sheared[0], sheared[1], 'b-o', label='sheared')\nplt.grid(True); plt.legend(); plt.gca().set_aspect('equal')\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2002%3A%20matrices/05.%20decompositions/","title":"Matrix Decompositions","text":"<p>Matrix decompositions break complex matrices into simpler factors for solving systems, computing inverses, and compressing data. This file covers Gaussian elimination, LU, QR, Cholesky, eigendecomposition, and SVD -- the algorithms behind PCA, recommender systems, and numerical stability in ML.</p> <ul> <li> <p>A matrix decomposition (or factorisation) breaks a matrix into simpler pieces that are easier to work with. Think of it like factoring a number: \\(12 = 3 \\times 4\\) is easier to reason about than 12 alone.</p> </li> <li> <p>We decompose matrices to solve systems of equations faster, compute inverses stably, find eigenvalues, compress data, and understand the geometry of transformations.</p> </li> <li> <p>The most fundamental technique is Gaussian elimination (row reduction). The idea is simple: given a system \\(A\\mathbf{x} = \\mathbf{b}\\), use three allowed operations to simplify \\(A\\) until the answer is obvious. </p> </li> <li> <p>The operations are: swap two rows, multiply a row by a nonzero scalar, or add a multiple of one row to another.</p> </li> <li> <p>For example, to eliminate the first column below the pivot, subtract multiples of row 1 from the rows below:</p> </li> </ul> \\[ \\begin{bmatrix} 2 &amp; 1 &amp; 5 \\\\ 4 &amp; 3 &amp; 7 \\\\ 6 &amp; 5 &amp; 9 \\end{bmatrix} \\xrightarrow{R_2 - 2R_1} \\begin{bmatrix} 2 &amp; 1 &amp; 5 \\\\ 0 &amp; 1 &amp; -3 \\\\ 6 &amp; 5 &amp; 9 \\end{bmatrix} \\xrightarrow{R_3 - 3R_1} \\begin{bmatrix} 2 &amp; 1 &amp; 5 \\\\ 0 &amp; 1 &amp; -3 \\\\ 0 &amp; 2 &amp; -6 \\end{bmatrix} \\] <ul> <li>The goal is row echelon form (REF): zeros below each pivot (the first nonzero entry in each row), with each pivot to the right of the one above it. The matrix becomes a staircase shape.</li> </ul> <p></p> <ul> <li> <p>Going further to reduced row echelon form (RREF) makes every pivot equal to 1 and the only nonzero entry in its column. Every matrix has a unique RREF.</p> </li> <li> <p>Once in triangular form, we solve by back substitution: the bottom row gives the last variable directly, then work upward. </p> </li> <li> <p>This is the foundation that all other decompositions build upon, the goal of decompositions is to reduce a matrix to a triangular form, so we can back substitute and solve for the variables. </p> </li> <li> <p>LU decomposition formalises Gaussian elimination by factoring a square matrix into \\(A = LU\\) (or \\(A = PLU\\) with row swaps), where \\(L\\) is lower triangular and \\(U\\) is upper triangular.</p> </li> </ul> <p></p> <ul> <li> <p>To solve \\(A\\mathbf{x} = \\mathbf{b}\\): first solve \\(L\\mathbf{y} = \\mathbf{b}\\) by forward substitution (top to bottom), then solve \\(U\\mathbf{x} = \\mathbf{y}\\) by back substitution (bottom to top). Two easy triangular solves instead of one hard general solve.</p> </li> <li> <p>The advantage over raw Gaussian elimination is reuse. Once you have \\(L\\) and \\(U\\), you can solve for many different \\(\\mathbf{b}\\) vectors without redoing the factorisation. </p> </li> <li> <p>If you need to solve the same system with 1000 different right-hand sides (common in simulations), you factorise once and reuse.</p> </li> <li> <p>When a matrix is symmetric and positive definite (like a covariance matrix), we can do even better. </p> </li> <li> <p>Cholesky decomposition factors it as \\(A = LL^T\\), where \\(L\\) is lower triangular. For example:</p> </li> </ul> \\[ \\begin{bmatrix} 4 &amp; 2 \\\\ 2 &amp; 5 \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 0 \\\\ 1 &amp; 2 \\end{bmatrix} \\begin{bmatrix} 2 &amp; 1 \\\\ 0 &amp; 2 \\end{bmatrix} \\] <ul> <li> <p>This is roughly twice as fast as LU and is guaranteed to be numerically stable. Think of it as a \"square root\" of the matrix.</p> </li> <li> <p>If the decomposition fails (a negative value under a square root), the matrix is not positive definite. Cholesky thus doubles as a test for positive definiteness. </p> </li> <li> <p>The eigenvectors of a square matrix \\(A\\) are the special directions that the transformation only stretches or shrinks, without rotating. The eigenvalue is the scaling factor:</p> </li> </ul> \\[A\\mathbf{x} = \\lambda\\mathbf{x}\\] <p></p> <ul> <li> <p>Most vectors change direction when multiplied by a matrix. But eigenvectors are special: the output points in the same direction as the input, just scaled by \\(\\lambda\\). If \\(\\lambda = 2\\), the eigenvector doubles in length. If \\(\\lambda = -1\\), it flips direction. If \\(\\lambda = 0\\), it gets squashed to zero.</p> </li> <li> <p>For example, with:</p> </li> </ul> \\[ A = \\begin{bmatrix} 3 &amp; 1 \\\\ 0 &amp; 2 \\end{bmatrix} \\] <p>the vector \\([1, 0]^T\\) is an eigenvector with \\(\\lambda = 3\\) because \\(A[1, 0]^T = [3, 0]^T = 3[1, 0]^T\\).</p> <ul> <li> <p>To find eigenvalues, solve the characteristic polynomial \\(\\det(A - \\lambda I) = 0\\). The roots are the eigenvalues. Then substitute each \\(\\lambda\\) back into \\((A - \\lambda I)\\mathbf{x} = \\mathbf{0}\\) to find the corresponding eigenvectors.</p> </li> <li> <p>Key properties:</p> <ul> <li>The trace of \\(A\\) equals the sum of its eigenvalues.</li> <li>The determinant of \\(A\\) equals the product of its eigenvalues.</li> <li>Symmetric matrices have perpendicular eigenvectors and real eigenvalues.</li> <li>Positive definite matrices have all positive eigenvalues.</li> <li>Covariance matrices (which we will encounter in statistics) are always positive semi-definite.</li> </ul> </li> <li> <p>Computing eigenvalues via the characteristic polynomial is impractical for large matrices. Instead, iterative methods are used:</p> <ul> <li> <p>Power iteration: repeatedly multiply by \\(A\\) and normalise. Converges to the dominant eigenvector (largest eigenvalue). Simple but only finds one eigenpair.</p> </li> <li> <p>QR algorithm: the workhorse method. Repeatedly decompose and recombine using QR factorisation until the matrix converges to triangular form, revealing all eigenvalues on the diagonal.</p> </li> <li> <p>Inverse iteration: finds the eigenvector closest to a given target value. Useful when you know roughly which eigenvalue you want.</p> </li> <li> <p>For large sparse matrices, Arnoldi and Lanczos iterations exploit the sparsity for efficiency.</p> </li> </ul> </li> <li> <p>If a square matrix has a full set of linearly independent eigenvectors, it can be diagonalised: \\(A = PDP^{-1}\\), where \\(D\\) is a diagonal matrix of eigenvalues and the columns of \\(P\\) are the eigenvectors.</p> </li> <li> <p>Why is this useful? Diagonal matrices are trivial to work with. Need \\(A^{100}\\)? Instead of multiplying \\(A\\) by itself 100 times, compute \\(PD^{100}P^{-1}\\), and raising a diagonal matrix to a power just raises each entry independently. This turns an expensive operation into a cheap one.</p> </li> <li> <p>An eigenbasis is a basis made entirely of eigenvectors. In this basis, the matrix becomes diagonal and the transformation is just independent scaling along each eigenvector direction. This is like finding the natural coordinate system for the transformation.</p> </li> <li> <p>QR decomposition factors any matrix \\(A\\) into \\(A = QR\\), where \\(Q\\) is orthogonal (its columns are orthonormal) and \\(R\\) is upper triangular. Think of it as separating the \"direction\" information (\\(Q\\)) from the \"scaling and mixing\" information (\\(R\\)).</p> </li> <li> <p>The Gram-Schmidt process builds \\(Q\\) column by column. Take the first column of \\(A\\) and normalise it. Take the second column, subtract its projection onto the first (to make it perpendicular), and normalise. Repeat for each column. The result is an orthonormal set of vectors.</p> </li> <li> <p>QR decomposition is the engine behind the QR algorithm for eigenvalues. It is also used directly for solving least-squares problems: when \\(A\\mathbf{x} = \\mathbf{b}\\) has no exact solution (more equations than unknowns), QR finds the best approximate answer.</p> </li> <li> <p>SVD (Singular Value Decomposition) is the most general and arguably the most important decomposition. Every matrix (any shape, any rank) has an SVD: \\(A = U\\Sigma V^T\\)</p> <ul> <li>\\(V^T\\) (\\(n \\times n\\), orthogonal): rotates the input</li> <li>\\(\\Sigma\\) (\\(m \\times n\\), diagonal): scales along orthogonal axes (the singular values, non-negative, in descending order)</li> <li>\\(U\\) (\\(m \\times m\\), orthogonal): rotates the output</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Geometrically, SVD says that every linear transformation, no matter how complicated, is just a rotation, followed by a stretch along the axes, followed by another rotation. A circle becomes an ellipse.</p> </li> <li> <p>The singular values (\\(\\sigma_1 \\geq \\sigma_2 \\geq \\ldots\\)) reveal the \"importance\" of each direction. Large singular values correspond to directions that matter most. The rank of \\(A\\) equals the number of nonzero singular values.</p> </li> <li> <p>Low-rank approximation: by keeping only the \\(k\\) largest singular values and zeroing the rest, you get the best possible rank-\\(k\\) approximation of \\(A\\). This is how image compression works: a \\(1000 \\times 1000\\) image might need only \\(k = 50\\) singular values to look nearly identical, compressing it by 20x.</p> </li> <li> <p>SVD also provides the pseudo-inverse: \\(A^+ = V\\Sigma^+U^T\\), where \\(\\Sigma^+\\) inverts the nonzero singular values.</p> </li> <li> <p>While eigendecomposition only works for square matrices, SVD works for any matrix. This is its key advantage.</p> </li> <li> <p>PCA (Principal Component Analysis) uses eigendecomposition (or SVD) for dimensionality reduction. </p> </li> <li> <p>Imagine a dataset with 100 features per sample (vector of dim 100 stacked into a matrix). Many of those features are correlated and redundant. </p> </li> <li> <p>PCA finds the directions along which the data actually varies, letting you keep only what matters.</p> </li> </ul> <p></p> <ul> <li> <p>The first principal component (PC1) is the direction of greatest variance. </p> </li> <li> <p>The second (PC2) captures the most variance of what remains, and is perpendicular to the first. </p> </li> <li> <p>If most of the variance lives along just a few directions, you can project the data down to those dimensions and discard the rest with minimal loss.</p> </li> <li> <p>The steps:</p> <ul> <li>Standardise the data (subtract mean, divide by standard deviation) so all features contribute equally</li> <li>Compute the covariance matrix</li> <li>Find its eigenvalues and eigenvectors</li> <li>Select the \\(k\\) eigenvectors with the largest eigenvalues (these are the principal components)</li> <li>Project the data onto these components</li> </ul> </li> <li> <p>Standardisation is critical: without it, a feature measured in kilometres would dominate one measured in centimetres, regardless of actual importance.</p> </li> <li> <p>In practice, PCA is used for visualisation (projecting high-dimensional data to 2D or 3D), noise reduction (discarding low-variance directions that are mostly noise), and speeding up ML models by reducing the number of input features.</p> </li> <li> <p>Kernel PCA extends PCA to nonlinear relationships. It maps the data through a kernel function into a higher-dimensional space where the structure becomes linear, then applies standard PCA and projects back.</p> </li> <li> <p>Schur decomposition factors a square matrix as \\(A = QTQ^\\ast\\), where \\(Q\\) is unitary and \\(T\\) is upper triangular. Every square matrix has a Schur decomposition, even if it cannot be diagonalised.</p> </li> <li> <p>Non-negative Matrix Factorisation (NMF) decomposes a matrix into two non-negative matrices: \\(A \\approx WH\\), where all entries in \\(W\\) and \\(H\\) are \\(\\geq 0\\). Unlike SVD, which can produce negative entries, NMF only adds, never subtracts. This makes the parts interpretable: in topic modelling, \\(W\\) gives the topic weights per document and \\(H\\) gives the word weights per topic, all non-negative, matching how we think about \"how much of each topic\" a document contains.</p> </li> <li> <p>The spectral theorem states that symmetric (or Hermitian) matrices can always be diagonalised with an orthogonal (or unitary) matrix. Their eigenvalues are always real and their eigenvectors always orthogonal. This is the theoretical foundation behind PCA.</p> </li> </ul>"},{"location":"chapter%2002%3A%20matrices/05.%20decompositions/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Compute the eigenvalues and eigenvectors of a symmetric matrix. Verify that the eigenvectors are perpendicular and reconstruct the matrix from its eigendecomposition. <pre><code>import jax.numpy as jnp\n\nA = jnp.array([[4.0, 2.0],\n               [2.0, 3.0]])\n\neigenvalues, eigenvectors = jnp.linalg.eigh(A)\nprint(f\"Eigenvalues: {eigenvalues}\")\nprint(f\"Eigenvectors orthogonal: {jnp.dot(eigenvectors[:,0], eigenvectors[:,1]):.6f}\")\n\n# Reconstruct: A = P D P^T\nD = jnp.diag(eigenvalues)\nA_reconstructed = eigenvectors @ D @ eigenvectors.T\nprint(f\"Reconstruction matches: {jnp.allclose(A, A_reconstructed)}\")\n</code></pre></p> </li> <li> <p>Implement power iteration to find the largest eigenvalue, and inverse iteration to find the smallest. Compare with <code>jnp.linalg.eigh</code>. Then try implementing the QR algorithm yourself. <pre><code>import jax.numpy as jnp\n\nA = jnp.array([[4.0, 2.0],\n               [2.0, 3.0]])\n\n# Power iteration: finds the LARGEST eigenvalue\nv = jnp.array([1.0, 0.0])\nfor _ in range(20):\n    v = A @ v\n    v = v / jnp.linalg.norm(v)\nprint(f\"Largest eigenvalue:  {v @ A @ v:.4f}\")\n\n# Inverse iteration: multiply by A^{-1} instead of A, finds the SMALLEST eigenvalue\nv = jnp.array([1.0, 0.0])\nfor _ in range(20):\n    v = jnp.linalg.solve(A, v)\n    v = v / jnp.linalg.norm(v)\nprint(f\"Smallest eigenvalue: {1.0 / (v @ jnp.linalg.solve(A, v)):.4f}\")\n\nprint(f\"jnp.linalg.eigh:    {jnp.linalg.eigh(A)[0]}\")\n</code></pre></p> </li> <li> <p>Compute the SVD of a matrix, then reconstruct it using only the top-k singular values and observe how the approximation quality changes with k. <pre><code>import jax.numpy as jnp\n\nA = jnp.array([[1.0, 2.0, 3.0],\n               [4.0, 5.0, 6.0],\n               [7.0, 8.0, 9.0]])\n\nU, S, Vt = jnp.linalg.svd(A)\n\nfor k in [1, 2, 3]:\n    approx = U[:, :k] @ jnp.diag(S[:k]) @ Vt[:k, :]\n    error = jnp.linalg.norm(A - approx)\n    print(f\"k={k}, reconstruction error: {error:.4f}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2003%3A%20calculus/01.%20differential%20calculus/","title":"Differential Calculus","text":"<p>Differential calculus captures instantaneous rates of change. This file covers limits, derivatives, differentiation rules, the chain rule (the foundation of backpropagation), and common derivatives used throughout ML.</p> <ul> <li> <p>In the previous chapters, we learned how to represent data as vectors and transform it with matrices. But many real-world phenomena are not static. A car accelerates, a stock price fluctuates, a neural network's loss changes as weights update. Calculus is the mathematics of change.</p> </li> <li> <p>Calculus asks two questions: how fast is something changing right now? (differential calculus) and how much has it accumulated over time? (integral calculus). This section tackles the \"how fast\" question.</p> </li> <li> <p>Imagine you are driving and glance at your speedometer. It reads 60 km/h. That number is not the average speed of your entire trip; it is your speed at this exact instant. Differential calculus gives us the tools to compute such instantaneous rates of change.</p> </li> <li> <p>But first, let us revisit the equation of a straight line: \\(y = mx + b\\). </p> </li> <li> <p>This is the simplest relationship between two quantities. </p> <ul> <li>\\(b\\) is the y-intercept, where the line crosses the y-axis (the starting value when \\(x = 0\\)). </li> <li>\\(m\\) is the slope, the rate of change: for every 1 unit increase in \\(x\\), \\(y\\) changes by \\(m\\). </li> <li>If \\(m = 3\\), the line rises steeply; if \\(m = 0\\), the line is flat; if \\(m = -2\\), the line falls. </li> </ul> </li> <li> <p>The slope is computed as \\(m = \\frac{\\Delta y}{\\Delta x} = \\frac{y_2 - y_1}{x_2 - x_1}\\), the ratio of \"how much did \\(y\\) change\" to \"how much did \\(x\\) change.\"</p> </li> </ul> <p></p> <ul> <li> <p>Once you know \\(m\\) and \\(b\\), you can compute \\(y\\) for any \\(x\\). </p> </li> <li> <p>For example, if \\(m = 2\\) and \\(b = 3\\), then at \\(x = 5\\): \\(y = 2(5) + 3 = 13\\). </p> </li> <li> <p>The two parameters fully determine the line, and predicting any output is just plugging in.</p> </li> <li> <p>For a straight line, the slope is the same everywhere. </p> </li> <li> <p>This idea generalises beyond lines. Any function is a rule that maps inputs to outputs, and once you know its formula (its parameters and shape), you can compute the output for any input and plot the result. </p> </li> <li> <p>\\(y = x^2\\) gives a parabola, \\(y = \\sin(x)\\) gives a wave, \\(y = e^x\\) gives exponential growth. Each formula defines a specific curve, and being comfortable reading a function as a shape is essential for everything that follows.</p> </li> <li> <p>For a straight line, the slope is the same everywhere. But most interesting functions are curved, so the slope varies from point to point. Calculus gives us a way to find the slope at any single point on a curve.</p> </li> <li> <p>We also need the concept of a limit. A limit describes what value a function approaches as its input gets closer and closer to some target, without necessarily reaching it.</p> </li> </ul> \\[\\lim_{x \\to a} f(x) = L\\] <ul> <li> <p>This reads: \"as \\(x\\) approaches \\(a\\), \\(f(x)\\) approaches \\(L\\).\" The function does not need to actually equal \\(L\\) at \\(x = a\\). It just needs to get arbitrarily close.</p> </li> <li> <p>For example, take \\(f(x) = \\frac{x^2 - 1}{x - 1}\\). If you plug in \\(x = 1\\) directly, you get \\(\\frac{0}{0}\\), which is undefined. </p> </li> <li> <p>But try values close to 1: \\(f(0.9) = 1.9\\), \\(f(0.99) = 1.99\\), \\(f(1.01) = 2.01\\). The outputs are clearly heading towards 2. </p> </li> <li> <p>Algebraically, we can see why: factor the numerator as \\((x-1)(x+1)\\), cancel the \\((x-1)\\) terms, and we get \\(f(x) = x + 1\\) for all \\(x \\neq 1\\). So as \\(x \\to 1\\), \\(f(x) \\to 2\\). </p> </li> <li> <p>The function has a hole at \\(x = 1\\), but the limit still exists.</p> </li> <li> <p>Limits are the foundation that everything else in calculus rests on.</p> </li> <li> <p>The derivative of a function \\(f(x)\\) at a point \\(x = a\\) measures the instantaneous rate of change. Geometrically, it is the slope of the tangent line to the curve at that point.</p> </li> </ul> <p></p> <ul> <li>To compute this slope, we start with two points on the curve and compute the slope of the line through them (a secant line). Then we slide the second point closer and closer to the first, and see what slope the secant line approaches. This is the difference quotient:</li> </ul> \\[f'(a) = \\lim_{h \\to 0} \\frac{f(a + h) - f(a)}{h}\\] <p></p> <ul> <li> <p>The numerator \\(f(a+h) - f(a)\\) is the change in output. The denominator \\(h\\) is the change in input. Their ratio is the average rate of change over a tiny interval. As \\(h \\to 0\\), this average becomes the instantaneous rate.</p> </li> <li> <p>For example, let \\(f(x) = x^2\\). At \\(x = 3\\):</p> </li> </ul> \\[f'(3) = \\lim_{h \\to 0} \\frac{(3+h)^2 - 9}{h} = \\lim_{h \\to 0} \\frac{9 + 6h + h^2 - 9}{h} = \\lim_{h \\to 0} (6 + h) = 6\\] <ul> <li> <p>So at \\(x = 3\\), the function \\(x^2\\) is increasing at a rate of 6 units of output per unit of input.</p> </li> <li> <p>A function is differentiable at a point if this limit exists. For that to happen, the function must be continuous (no jumps), smooth (no sharp corners), and defined in a neighbourhood around the point. </p> </li> <li> <p>If you can draw the curve without lifting your pen and without any kinks, it is probably differentiable there.</p> </li> <li> <p>Computing derivatives from the limit definition every time would be tedious. Fortunately, a handful of rules let us differentiate almost any function quickly.</p> </li> <li> <p>Constant rule: the derivative of a constant is zero. If \\(f(x) = 5\\), then \\(f'(x) = 0\\). A flat line has zero slope.</p> </li> <li> <p>Power rule: the workhorse of differentiation. Bring the exponent down and reduce it by one:</p> </li> </ul> \\[\\frac{d}{dx} x^n = n x^{n-1}\\] <ul> <li> <p>For example: \\(\\frac{d}{dx} x^3 = 3x^2\\). The cubic becomes a quadratic. This works for any real exponent, including negatives and fractions: \\(\\frac{d}{dx} x^{-1} = -x^{-2}\\) and \\(\\frac{d}{dx} \\sqrt{x} = \\frac{d}{dx} x^{1/2} = \\frac{1}{2}x^{-1/2}\\).</p> </li> <li> <p>Sum/Difference rule: differentiate term by term.</p> </li> </ul> \\[\\frac{d}{dx}[f(x) \\pm g(x)] = f'(x) \\pm g'(x)\\] <ul> <li>Product rule: when two functions are multiplied, the derivative is not simply the product of the derivatives. Instead:</li> </ul> \\[\\frac{d}{dx}[f(x) \\cdot g(x)] = f'(x)g(x) + f(x)g'(x)\\] <ul> <li> <p>Think of it as: \"the rate of change of the first times the second, plus the first times the rate of change of the second.\" For example, \\(\\frac{d}{dx}[x^2 \\sin x] = 2x \\sin x + x^2 \\cos x\\).</p> </li> <li> <p>Quotient rule: for a ratio of functions:</p> </li> </ul> \\[\\frac{d}{dx}\\left[\\frac{f(x)}{g(x)}\\right] = \\frac{f'(x)g(x) - f(x)g'(x)}{[g(x)]^2}\\] <ul> <li> <p>A useful mnemonic: \"low d-high minus high d-low, over the square of what's below.\"</p> </li> <li> <p>Chain rule: the most important rule for ML. When functions are composed (one inside another), the derivative is the product of the derivatives along the chain:</p> </li> </ul> \\[\\frac{d}{dx} f(g(x)) = f'(g(x)) \\cdot g'(x)\\] <ul> <li>Think of it as peeling an onion. Differentiate the outer function (keeping the inner function untouched), then multiply by the derivative of the inner function.</li> </ul> <p></p> <ul> <li> <p>For example, \\(\\frac{d}{dx} (3x + 1)^5 = 5(3x+1)^4 \\cdot 3 = 15(3x+1)^4\\). The outer function is \\((\\cdot)^5\\) and the inner is \\(3x+1\\).</p> </li> <li> <p>The chain rule is the mathematical foundation of backpropagation in neural networks. A deep network is a long chain of composed functions. To compute how the loss changes with respect to each weight, we apply the chain rule repeatedly from the output layer back to the input, multiplying local derivatives at each step.</p> </li> <li> <p>Here are the most common derivatives you will encounter. Each one can be derived from the limit definition, but knowing them by heart saves time:</p> </li> </ul> Function Derivative Notes \\(e^x\\) \\(e^x\\) The only function that is its own derivative \\(a^x\\) \\(a^x \\ln a\\) Generalises the exponential \\(\\ln x\\) \\(\\frac{1}{x}\\) The natural logarithm \\(\\log_a x\\) \\(\\frac{1}{x \\ln a}\\) General logarithm \\(\\sin x\\) \\(\\cos x\\) \\(\\cos x\\) \\(-\\sin x\\) Note the negative sign \\(\\tan x\\) \\(\\sec^2 x\\) <ul> <li> <p>The exponential function \\(e^x\\) is remarkable: it is the only function that equals its own derivative. This is why \\(e\\) appears everywhere in ML, from softmax activations to probability distributions.</p> </li> <li> <p>L'Hopital's Rule handles limits that produce indeterminate forms like \\(\\frac{0}{0}\\) or \\(\\frac{\\infty}{\\infty}\\). When direct substitution gives one of these forms, you can take the derivative of the numerator and denominator separately and try the limit again:</p> </li> </ul> \\[\\lim_{x \\to a} \\frac{f(x)}{g(x)} = \\lim_{x \\to a} \\frac{f'(x)}{g'(x)}\\] <ul> <li> <p>Conditions: both \\(f\\) and \\(g\\) must be differentiable near \\(a\\), and \\(g'(x) \\neq 0\\) near \\(a\\) (except possibly at \\(a\\) itself). The original limit must give an indeterminate form.</p> </li> <li> <p>For example: \\(\\lim_{x \\to 0} \\frac{\\sin x}{x}\\). Direct substitution gives \\(\\frac{0}{0}\\). Applying L'Hopital's Rule: \\(\\lim_{x \\to 0} \\frac{\\cos x}{1} = 1\\). This limit is fundamental, it appears in signal processing and Fourier analysis.</p> </li> <li> <p>You can apply the rule repeatedly if the result is still indeterminate. For instance, \\(\\lim_{x \\to 0} \\frac{1 - \\cos x}{x^2}\\) gives \\(\\frac{0}{0}\\). First application: \\(\\lim_{x \\to 0} \\frac{\\sin x}{2x}\\), still \\(\\frac{0}{0}\\). Second application: \\(\\lim_{x \\to 0} \\frac{\\cos x}{2} = \\frac{1}{2}\\).</p> </li> <li> <p>If two functions are differentiable, their sum, difference, product, composition, and quotient (where the denominator is non-zero) are also differentiable. This is why we can confidently differentiate complex expressions built from simple pieces.</p> </li> </ul>"},{"location":"chapter%2003%3A%20calculus/01.%20differential%20calculus/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Visualise common functions. Plot \\(x^2\\), \\(\\sin(x)\\), and \\(e^x\\) side by side to build intuition for how different formulas produce different shapes. Try changing parameters (e.g. \\(2x^2\\), \\(\\sin(2x)\\)) and observe how the curves change. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx = jnp.linspace(-3, 3, 300)\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 3))\naxes[0].plot(x, x**2, color=\"#e74c3c\")\naxes[0].set_title(\"x\u00b2  (parabola)\")\naxes[1].plot(x, jnp.sin(x), color=\"#3498db\")\naxes[1].set_title(\"sin(x)  (wave)\")\naxes[2].plot(x, jnp.exp(x), color=\"#27ae60\")\naxes[2].set_title(\"e\u02e3  (exponential)\")\nfor ax in axes:\n    ax.axhline(0, color=\"gray\", linewidth=0.5)\n    ax.axvline(0, color=\"gray\", linewidth=0.5)\nplt.tight_layout()\nplt.show()\n</code></pre></p> </li> <li> <p>Use JAX's automatic differentiation to compute the derivative of \\(f(x) = x^3 - 2x + 1\\) at several points. Compare with the analytical derivative \\(f'(x) = 3x^2 - 2\\). <pre><code>import jax\nimport jax.numpy as jnp\n\nf = lambda x: x**3 - 2*x + 1\ndf = jax.grad(f)\n\nfor x in [0.0, 1.0, 2.0, -1.0]:\n    print(f\"x={x:5.1f}  autodiff: {df(x):.4f}  analytical: {3*x**2 - 2:.4f}\")\n</code></pre></p> </li> <li> <p>Verify the chain rule numerically. Define \\(f(x) = \\sin(x^2)\\), compute its derivative via <code>jax.grad</code>, and compare with the analytical result \\(2x\\cos(x^2)\\). <pre><code>import jax\nimport jax.numpy as jnp\n\nf = lambda x: jnp.sin(x**2)\ndf = jax.grad(f)\n\nfor x in [0.5, 1.0, 2.0]:\n    auto = df(x)\n    analytical = 2*x * jnp.cos(x**2)\n    print(f\"x={x:.1f}  autodiff: {auto:.6f}  analytical: {analytical:.6f}\")\n</code></pre></p> </li> <li> <p>Visualise the derivative. Plot \\(f(x) = x^3 - 3x\\) and its derivative \\(f'(x) = 3x^2 - 3\\) on the same graph. Notice where \\(f'(x) = 0\\) corresponds to the peaks and valleys of \\(f\\). <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nf = lambda x: x**3 - 3*x\n# jax.grad works on scalars; jax.vmap vectorises it to operate on an array of inputs at once\ndf = jax.vmap(jax.grad(f))\n\nx = jnp.linspace(-2.5, 2.5, 200)\nplt.plot(x, jax.vmap(f)(x), label=\"f(x)\")\nplt.plot(x, df(x), label=\"f'(x)\", linestyle=\"--\")\nplt.axhline(0, color=\"gray\", linewidth=0.5)\nplt.legend()\nplt.title(\"A function and its derivative\")\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2003%3A%20calculus/02.%20integral%20calculus/","title":"Integral Calculus","text":"<p>Integral calculus accumulates quantities over intervals, turning local rates back into totals. This file covers definite and indefinite integrals, the Fundamental Theorem of Calculus, integration techniques, and applications to probability densities and expected values in ML.</p> <ul> <li> <p>Differentiation tells us the rate of change at a single point. Integration goes the other way: it accumulates many tiny pieces to compute a total. </p> </li> <li> <p>If the derivative answers \"how fast?\", the integral answers \"how much?\"</p> </li> <li> <p>The simplest way to think about integration is as the area under a curve. If you plot a function \\(f(x)\\) and shade the region between the curve and the x-axis from \\(x = a\\) to \\(x = b\\), the integral gives the signed area of that region.</p> </li> </ul> <p></p> <ul> <li> <p>Why \"signed\"? Regions above the x-axis contribute positive area, regions below contribute negative area. This makes physical sense: if \\(f(x)\\) represents velocity, the integral gives net displacement (forward minus backward), not total distance.</p> </li> <li> <p>To compute this area, imagine slicing the region into \\(n\\) thin vertical rectangles, each of width \\(\\Delta x\\). The height of each rectangle is the function value at some point in that slice. Sum them up:</p> </li> </ul> \\[\\text{Area} \\approx \\sum_{i=1}^{n} f(x_i^\\ast) \\, \\Delta x\\] <ul> <li>As we make the rectangles thinner and thinner (\\(n \\to \\infty\\), \\(\\Delta x \\to 0\\)), the sum becomes exact. This limiting process defines the definite integral:</li> </ul> \\[\\int_a^b f(x)\\, dx = \\lim_{n \\to \\infty} \\sum_{i=1}^{n} f(x_i^\\ast) \\, \\Delta x\\] <ul> <li> <p>The \\(\\int\\) symbol is an elongated \"S\" for \"sum.\" The \\(dx\\) reminds us that we are summing infinitesimally thin slices along the x-axis.</p> </li> <li> <p>An indefinite integral (or antiderivative) is a function \\(F(x)\\) whose derivative is \\(f(x)\\). We write:</p> </li> </ul> \\[\\int f(x)\\, dx = F(x) + C\\] <ul> <li> <p>The \\(+ C\\) is the constant of integration. Since the derivative of any constant is zero, there are infinitely many antiderivatives that differ only by a constant. For example, \\(\\int 2x\\, dx = x^2 + C\\), because the derivative of \\(x^2 + 7\\) or \\(x^2 - 3\\) is still \\(2x\\).</p> </li> <li> <p>The Fundamental Theorem of Calculus is the bridge that connects differentiation and integration. It has two parts:</p> </li> <li> <p>Part 1: If \\(F(x)\\) is an antiderivative of \\(f(x)\\), then the definite integral equals the difference of \\(F\\) at the endpoints:</p> </li> </ul> \\[\\int_a^b f(x)\\, dx = F(b) - F(a)\\] <ul> <li> <p>This is remarkably practical. Instead of computing a limit of sums (which is hard), we find an antiderivative and evaluate it at two points (which is usually easy).</p> </li> <li> <p>Part 2: If we define \\(F(x) = \\int_a^x f(t)\\, dt\\), then \\(F'(x) = f(x)\\). Differentiation and integration are inverse operations, they undo each other.</p> </li> <li> <p>For example, to compute \\(\\int_1^3 x^2\\, dx\\): the antiderivative of \\(x^2\\) is \\(\\frac{x^3}{3}\\). So \\(\\int_1^3 x^2\\, dx = \\frac{27}{3} - \\frac{1}{3} = \\frac{26}{3} \\approx 8.67\\).</p> </li> <li> <p>Just as differentiation has rules, integration has corresponding rules that reverse them:</p> </li> </ul> Function Integral Condition \\(x^n\\) \\(\\frac{x^{n+1}}{n+1} + C\\) \\(n \\neq -1\\) \\(\\frac{1}{x}\\) \\(\\ln\\|x\\| + C\\) \\(e^x\\) \\(e^x + C\\) \\(a^x\\) \\(\\frac{a^x}{\\ln a} + C\\) \\(\\sin x\\) \\(-\\cos x + C\\) \\(\\cos x\\) \\(\\sin x + C\\) \\(k\\) (constant) \\(kx + C\\) <ul> <li> <p>The sum/difference rule carries over: \\(\\int [f(x) \\pm g(x)]\\, dx = \\int f(x)\\, dx \\pm \\int g(x)\\, dx\\). Constants can be pulled out: \\(\\int k\\, f(x)\\, dx = k \\int f(x)\\, dx\\).</p> </li> <li> <p>When a function is too complex to integrate directly, we have techniques to simplify it.</p> </li> <li> <p>u-substitution is the reverse of the chain rule. If you spot a composite function \\(f(g(x))\\) multiplied by \\(g'(x)\\), substitute \\(u = g(x)\\) so that \\(du = g'(x)\\, dx\\), and the integral simplifies.</p> </li> <li> <p>For example: \\(\\int 2x \\cos(x^2)\\, dx\\). Let \\(u = x^2\\), so \\(du = 2x\\, dx\\). The integral becomes \\(\\int \\cos(u)\\, du = \\sin(u) + C = \\sin(x^2) + C\\).</p> </li> <li> <p>Integration by parts is the reverse of the product rule. If the integrand is a product of two functions:</p> </li> </ul> \\[\\int u\\, dv = uv - \\int v\\, du\\] <ul> <li> <p>Choose \\(u\\) and \\(dv\\) strategically so that the remaining integral \\(\\int v\\, du\\) is simpler than the original. A common mnemonic for choosing \\(u\\) is LIATE: Logarithmic, Inverse trig, Algebraic, Trigonometric, Exponential (pick \\(u\\) from the earlier category).</p> </li> <li> <p>For example: \\(\\int x\\, e^x\\, dx\\). Let \\(u = x\\) (algebraic) and \\(dv = e^x\\, dx\\). Then \\(du = dx\\) and \\(v = e^x\\). So: \\(\\int x\\, e^x\\, dx = x\\, e^x - \\int e^x\\, dx = x\\, e^x - e^x + C = e^x(x - 1) + C\\).</p> </li> <li> <p>In ML, integration appears in probability theory (computing probabilities by integrating density functions), in expected values (weighted averages over continuous distributions), and in computing the area under ROC curves. While we rarely integrate by hand in practice, understanding what integration means helps interpret these quantities.</p> </li> </ul>"},{"location":"chapter%2003%3A%20calculus/02.%20integral%20calculus/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Numerically approximate \\(\\int_0^1 x^2\\, dx\\) using a Riemann sum with increasing numbers of rectangles. Compare with the exact answer \\(\\frac{1}{3}\\). <pre><code>import jax.numpy as jnp\n\nfor n in [10, 100, 1000, 10000]:\n    x = jnp.linspace(0, 1, n, endpoint=False)\n    dx = 1.0 / n\n    area = jnp.sum(x**2 * dx)\n    print(f\"n={n:5d}  approx: {area:.6f}  exact: {1/3:.6f}\")\n</code></pre></p> </li> <li> <p>Verify the Fundamental Theorem of Calculus numerically. Define \\(F(x) = \\int_0^x t^2\\, dt = \\frac{x^3}{3}\\) and check that its derivative (computed via <code>jax.grad</code>) equals \\(x^2\\). <pre><code>import jax\nimport jax.numpy as jnp\n\nF = lambda x: x**3 / 3\ndF = jax.grad(F)\n\nfor x in [0.5, 1.0, 2.0, 3.0]:\n    print(f\"x={x:.1f}  F'(x)={dF(x):.4f}  x^2={x**2:.4f}\")\n</code></pre></p> </li> <li> <p>Visualise the area under \\(f(x) = \\sin(x)\\) from \\(0\\) to \\(\\pi\\). Use <code>plt.fill_between</code> to shade the area and compute it numerically with a Riemann sum. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx = jnp.linspace(0, jnp.pi, 500)\ny = jnp.sin(x)\n\nplt.plot(x, y, color=\"purple\", linewidth=2)\nplt.fill_between(x, y, alpha=0.2, color=\"purple\")\nplt.title(f\"Area = {jnp.sum(jnp.sin(x) * (jnp.pi / 500)):.4f}  (exact: 2.0)\")\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2003%3A%20calculus/03.%20multivariate%20calculus/","title":"Multivariate Calculus","text":"<p>Multivariate calculus extends derivatives and integrals to functions of many variables, which is essential since ML models have millions of parameters. This file covers partial derivatives, gradients, the Jacobian, the Hessian, and the multivariable chain rule that makes backpropagation possible.</p> <ul> <li> <p>So far, our functions have taken a single input \\(x\\) and produced a single output \\(f(x)\\). But in ML, we almost never work with just one variable. </p> </li> <li> <p>Consider a function of two variables, like \\(f(x, y) = x^2 + y^2\\). This defines a surface in 3D space, a bowl shape. We want to know: if we nudge \\(x\\) a little while keeping \\(y\\) fixed, how does \\(f\\) change? That is a partial derivative.</p> </li> <li> <p>The partial derivative of \\(f\\) with respect to \\(x\\), written \\(\\frac{\\partial f}{\\partial x}\\), treats every other variable as a constant and differentiates normally with respect to \\(x\\).</p> </li> <li> <p>For \\(f(x, y) = x^2y + 3x - 2y\\):</p> </li> </ul> \\[\\frac{\\partial f}{\\partial x} = 2xy + 3 \\qquad \\frac{\\partial f}{\\partial y} = x^2 - 2\\] <ul> <li> <p>To compute \\(\\frac{\\partial f}{\\partial x}\\), we treated \\(y\\) as a constant, so \\(x^2y\\) differentiated to \\(2xy\\), \\(3x\\) to \\(3\\), and \\(-2y\\) to \\(0\\). </p> </li> <li> <p>To compute \\(\\frac{\\partial f}{\\partial y}\\), we treated \\(x\\) as a constant, so \\(x^2y\\) differentiated to \\(x^2\\), \\(3x\\) to \\(0\\), and \\(-2y\\) to \\(-2\\).</p> </li> <li> <p>Geometrically, taking a partial derivative with respect to \\(x\\) is like slicing the 3D surface with a plane parallel to the \\(xz\\)-plane (at a fixed \\(y\\) value) and finding the slope of the resulting curve.</p> </li> </ul> <p></p> <ul> <li>The gradient collects all the partial derivatives into a single vector:</li> </ul> \\[\\nabla f = \\left(\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n}\\right)\\] <ul> <li> <p>For \\(f(x, y) = x^2 + y^2\\): \\(\\nabla f(x, y) = (2x, 2y)\\). At the point \\((1, 2)\\): \\(\\nabla f(1, 2) = (2, 4)\\).</p> </li> <li> <p>The gradient has two key properties:</p> <ul> <li> <p>Direction: it points in the direction of steepest increase. Imagine a hiker on a mountain. The gradient at their position points straight uphill, along the steepest path.</p> </li> <li> <p>Magnitude: \\(\\|\\nabla f\\|\\) gives the rate of increase in that steepest direction. A large gradient means the terrain is steep; a small gradient means it is nearly flat.</p> </li> </ul> </li> </ul> <p></p> <ul> <li> <p>Since the gradient points uphill, moving in the opposite direction (\\(-\\nabla f\\)) goes downhill, towards lower values. This simple idea is the basis of gradient descent, an optimisation technique we will explore in detail in later chapters. For now, the key takeaway is that the gradient tells you which way is \"up\" and how steep the climb is.</p> </li> <li> <p>The directional derivative generalises partial derivatives. Instead of asking \"how does \\(f\\) change along the \\(x\\)-axis?\", it asks \"how does \\(f\\) change along any direction \\(\\mathbf{u}\\)?\" It is computed as the dot product of the gradient with a unit vector:</p> </li> </ul> \\[D_{\\mathbf{u}} f = \\nabla f \\cdot \\mathbf{u}\\] <ul> <li> <p>For \\(f(x, y) = x^2 + y^2\\) at \\((1, 2)\\) in the direction of \\(\\mathbf{v} = (3, 4)\\): first normalise to get \\(\\mathbf{u} = (3/5, 4/5)\\), then \\(D_{\\mathbf{u}} f = (2, 4) \\cdot (3/5, 4/5) = 6/5 + 16/5 = 22/5\\).</p> </li> <li> <p>Partial derivatives are special cases of directional derivatives where the direction is along a coordinate axis. If the directional derivative is zero in some direction, the function is flat in that direction at that point.</p> </li> <li> <p>Contour lines (or level curves) connect points where a function has the same value. For \\(f(x, y) = x^2 + y^2\\), the contour lines are circles centred at the origin: \\(x^2 + y^2 = c\\) for different values of \\(c\\).</p> </li> <li> <p>Contour lines never cross each other (a point cannot have two different function values). </p> </li> <li> <p>The gradient is always perpendicular to the contour lines, pointing from lower to higher values. </p> </li> <li> <p>Closely spaced contour lines indicate steep terrain; widely spaced lines indicate gentle slopes.</p> </li> <li> <p>So far, our functions produced a single output. But many functions produce multiple outputs. A function \\(\\mathbf{F}: \\mathbb{R}^n \\to \\mathbb{R}^m\\) takes \\(n\\) inputs and produces \\(m\\) outputs. The Jacobian matrix organises all the partial derivatives of such a vector-valued function:</p> </li> </ul> \\[ J = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\cdots &amp; \\frac{\\partial f_m}{\\partial x_n} \\end{bmatrix} \\] <ul> <li> <p>Each row of the Jacobian is the gradient of one output component. For a function with 3 inputs and 2 outputs, the Jacobian is a \\(2 \\times 3\\) matrix.</p> </li> <li> <p>The Jacobian generalises the derivative to vector-valued functions. </p> </li> <li> <p>Just as the derivative of a scalar function tells you how much the output changes per unit input change, the Jacobian tells you how each output changes with respect to each input.</p> </li> <li> <p>The determinant of the Jacobian measures how much a transformation locally stretches or compresses space. </p> </li> <li> <p>If the determinant is 2, small regions double in area. If it is 0, the transformation squashes space to a lower dimension (recall from our chapter on matrices that a zero determinant means a singular, non-invertible transformation).</p> </li> <li> <p>When several transformations are composed (one feeding into the next), the Jacobian of the overall mapping is the product of the individual Jacobians. We will see this idea become central in later chapters.</p> </li> <li> <p>Where the gradient captures first-order information (slopes), the Hessian matrix captures second-order information (curvature). </p> </li> <li> <p>For a scalar function \\(f(x_1, \\ldots, x_n)\\), the Hessian is the \\(n \\times n\\) matrix of all second partial derivatives:</p> </li> </ul> \\[ H = \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\cdots \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots \\end{bmatrix} \\] <ul> <li>For \\(f(x, y) = x^3 + 2xy^2 - y^3\\), the gradient is \\((3x^2 + 2y^2,\\; 4xy - 3y^2)\\), and the Hessian is:</li> </ul> \\[ H = \\begin{bmatrix} 6x &amp; 4y \\\\ 4y &amp; 4x - 6y \\end{bmatrix} \\] <ul> <li> <p>The diagonal entries (\\(6x\\) and \\(4x - 6y\\)) tell you how the slope in the \\(x\\)-direction changes as you move in \\(x\\), and similarly for \\(y\\). </p> </li> <li> <p>The off-diagonal entries (\\(4y\\)) tell you how the slope in one direction changes as you move in the other direction.</p> </li> <li> <p>Clairaut's theorem guarantees that for functions with continuous second derivatives, the mixed partial derivatives are equal: \\(\\frac{\\partial^2 f}{\\partial x \\partial y} = \\frac{\\partial^2 f}{\\partial y \\partial x}\\). </p> </li> <li> <p>This means the Hessian is symmetric, which (as we saw in the matrices chapter) guarantees real eigenvalues and orthogonal eigenvectors.</p> </li> <li> <p>The Hessian tells us about the shape of the function near a critical point (where the gradient is zero):</p> <ul> <li>If \\(H\\) is positive definite (all eigenvalues positive), the point is a local minimum, the surface curves upward in every direction like a bowl.</li> <li>If \\(H\\) is negative definite (all eigenvalues negative), the point is a local maximum, the surface curves downward like an inverted bowl.</li> <li>If \\(H\\) has both positive and negative eigenvalues, the point is a saddle point, the surface curves up in some directions and down in others, like a mountain pass.</li> </ul> </li> <li> <p>The multivariate chain rule extends the chain rule to functions of several variables. If \\(z = f(x, y)\\) where \\(x = g(t)\\) and \\(y = h(t)\\), then:</p> </li> </ul> \\[\\frac{dz}{dt} = \\frac{\\partial f}{\\partial x}\\frac{dx}{dt} + \\frac{\\partial f}{\\partial y}\\frac{dy}{dt}\\] <ul> <li> <p>Each path from \\(t\\) to \\(z\\) contributes a term: the partial derivative along that path times the derivative of the intermediate variable with respect to \\(t\\). </p> </li> <li> <p>For example, if \\(z = x^2 y + 3x - y^2\\), \\(x = \\cos(t)\\), \\(y = \\sin(t)\\):</p> </li> </ul> \\[\\frac{dz}{dt} = (2xy + 3)(-\\sin t) + (x^2 - 2y)(\\cos t)\\] <ul> <li> <p>Beyond computing derivatives by hand, there are three approaches:</p> <ul> <li>Numerical differentiation: approximate \\(f'(x) \\approx \\frac{f(x+h) - f(x-h)}{2h}\\) for small \\(h\\). Simple but noisy and inaccurate.</li> <li>Symbolic differentiation: apply differentiation rules algebraically to produce an exact formula. Can produce expressions that grow exponentially large.</li> <li>Automatic differentiation (autodiff): tracks the chain of operations and computes exact derivatives efficiently. This is what JAX, PyTorch, and TensorFlow use. It gives exact numerical values (not approximate) without producing bloated symbolic expressions.</li> </ul> </li> </ul>"},{"location":"chapter%2003%3A%20calculus/03.%20multivariate%20calculus/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Compute the gradient of \\(f(x, y) = x^2 y + 3x - 2y\\) at the point \\((1, 2)\\) using <code>jax.grad</code>. Since \\(f\\) takes a vector input, use <code>jax.grad</code> with <code>argnums</code>. <pre><code>import jax\nimport jax.numpy as jnp\n\ndef f(x, y):\n    return x**2 * y + 3*x - 2*y\n\ndf_dx = jax.grad(f, argnums=0)\ndf_dy = jax.grad(f, argnums=1)\n\nx, y = 1.0, 2.0\nprint(f\"\u2202f/\u2202x = {df_dx(x, y):.4f}  (expected: {2*x*y + 3:.4f})\")\nprint(f\"\u2202f/\u2202y = {df_dy(x, y):.4f}  (expected: {x**2 - 2:.4f})\")\n</code></pre></p> </li> <li> <p>Compute the Jacobian of a vector-valued function using <code>jax.jacobian</code>. Compare with manual calculation. <pre><code>import jax\nimport jax.numpy as jnp\n\ndef F(x):\n    return jnp.array([x[0]**2 + x[1], x[0] * x[1]**2])\n\nJ = jax.jacobian(F)\nx = jnp.array([1.0, 2.0])\nprint(f\"Jacobian at (1,2):\\n{J(x)}\")\n# Expected: [[2*x[0], 1], [x[1]**2, 2*x[0]*x[1]]] = [[2, 1], [4, 4]]\n</code></pre></p> </li> <li> <p>Compute the Hessian of \\(f(x, y) = x^3 + 2xy^2 - y^3\\) using <code>jax.hessian</code> and verify it is symmetric. <pre><code>import jax\nimport jax.numpy as jnp\n\ndef f(xy):\n    x, y = xy[0], xy[1]\n    return x**3 + 2*x*y**2 - y**3\n\nH = jax.hessian(f)\npoint = jnp.array([1.0, 2.0])\nhess = H(point)\nprint(f\"Hessian:\\n{hess}\")\nprint(f\"Symmetric: {jnp.allclose(hess, hess.T)}\")\n# Expected: [[6x, 4y], [4y, 4x-6y]] = [[6, 8], [8, -8]]\n</code></pre></p> </li> <li> <p>Build a minimal autodiff engine from scratch. </p> <ul> <li>Each <code>Var</code> tracks its value and how to propagate gradients backward through the chain rule. </li> <li>Try extending it with more operations (division, power, etc.).</li> <li>This is the foundations of how JAX, PyTorch and Numpy were designed.  <pre><code>class Var:\n    def __init__(self, val, children=(), backward_fn=None):\n        self.val = val\n        self.grad = 0.0\n        self.children = children\n        self.backward_fn = backward_fn\n\n    def __add__(self, other):\n        out = Var(self.val + other.val, children=(self, other))\n        def _backward():\n            self.grad += out.grad    # d(a+b)/da = 1\n            other.grad += out.grad   # d(a+b)/db = 1\n        out.backward_fn = _backward\n        return out\n\n    def __mul__(self, other):\n        out = Var(self.val * other.val, children=(self, other))\n        def _backward():\n            self.grad += other.val * out.grad  # d(a*b)/da = b\n            other.grad += self.val * out.grad  # d(a*b)/db = a\n        out.backward_fn = _backward\n        return out\n\n    def backward(self):\n        # topological sort then propagate gradients\n        # we will go through this in data structures and algorithms\n        order, visited = [], set()\n        def topo(v):\n            if v not in visited:\n                visited.add(v)\n                for c in v.children:\n                    topo(c)\n                order.append(v)\n        topo(self)\n        self.grad = 1.0\n        for v in reversed(order):\n            if v.backward_fn:\n                v.backward_fn()\n\n# f(x, y) = x*x*y + x  at (3, 2)\nx = Var(3.0)\ny = Var(2.0)\nf = x * x * y + x       # = 3*3*2 + 3 = 21\n\nf.backward()\nprint(f\"f = {f.val}\")           # 21.0\nprint(f\"df/dx = {x.grad}\")     # 2*x*y + 1 = 13.0\nprint(f\"df/dy = {y.grad}\")     # x*x = 9.0\n</code></pre></li> </ul> </li> </ol>"},{"location":"chapter%2003%3A%20calculus/04.%20function%20approximation/","title":"Function Approximation","text":"<p>Function approximation replaces complex functions with simpler ones that are close enough to be useful. This file covers linearisation, Taylor series, polynomial approximation, Fourier series, and the universal approximation theorem -- the theoretical backbone of why neural networks can learn arbitrary mappings.</p> <ul> <li> <p>Many functions we encounter are too complex to work with directly. Computing \\(e^{0.1}\\) on paper, predicting the trajectory of a satellite, etc. all involve functions that do not have simple closed-form answers. </p> </li> <li> <p>Function approximation replaces a complicated function with a simpler one that is \"close enough\" over the region we care about.</p> </li> <li> <p>The most natural approximation is a polynomial. Polynomials are just sums of powers of \\(x\\) with coefficients, and they are easy to evaluate, differentiate, and integrate.</p> </li> <li> <p>But why do polynomials work so well as approximators? Consider what each power of \\(x\\) contributes. </p> <ul> <li>The constant term \\(a_0\\) sets the baseline value. </li> <li>The \\(a_1 x\\) term adds a slope. </li> <li>The \\(a_2 x^2\\) term adds curvature. </li> <li>Each higher power captures finer detail about the function's shape.</li> </ul> </li> </ul> <p></p> <ul> <li> <p>By choosing the right coefficients, we can match a function's value, slope, curvature, and higher-order behaviour at a point, one piece at a time. </p> </li> <li> <p>With enough terms, the polynomial can mimic almost any smooth function.</p> </li> <li> <p>The question becomes: how do we find the right coefficients?</p> </li> <li> <p>Linearisation is the simplest approximation. Near a point \\(x = a\\), we replace the function with its tangent line:</p> </li> </ul> \\[L(x) = f(a) + f'(a)(x - a)\\] <ul> <li> <p>This is the first-order Taylor approximation. It says: start at the known value \\(f(a)\\), then adjust by the slope times the distance from \\(a\\).</p> </li> <li> <p>For example, linearise \\(\\sin(x)\\) at \\(x = 0\\): \\(f(0) = 0\\), \\(f'(0) = \\cos(0) = 1\\), so \\(L(x) = x\\). Near zero, \\(\\sin(x) \\approx x\\). Try it: \\(\\sin(0.1) = 0.0998\\ldots \\approx 0.1\\).</p> </li> <li> <p>But linearisation is only good very close to \\(a\\). Move further away and the approximation falls apart. To do better, we include higher-order terms.</p> </li> <li> <p>The Taylor series represents a function as an infinite sum of polynomial terms, each capturing finer detail about the function's behaviour near a point \\(a\\):</p> </li> </ul> \\[f(x) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(a)}{n!}(x - a)^n = f(a) + f'(a)(x-a) + \\frac{f''(a)}{2!}(x-a)^2 + \\frac{f'''(a)}{3!}(x-a)^3 + \\cdots\\] <p></p> <ul> <li> <p>Each successive term adds a correction. The first term matches the value, the second matches the slope, the third matches the curvature, and so on. The more terms we include, the larger the region where the approximation is accurate.</p> </li> <li> <p>The \\(n!\\) in the denominator is not arbitrary. When you differentiate \\((x - a)^n\\) exactly \\(n\\) times, you get \\(n!\\). The factorial cancels this out, ensuring that the \\(n\\)-th derivative of the Taylor polynomial equals the \\(n\\)-th derivative of the original function at \\(x = a\\).</p> </li> <li> <p>A Maclaurin series is simply a Taylor series centred at \\(a = 0\\):</p> </li> </ul> \\[f(x) = \\sum_{n=0}^{\\infty} \\frac{f^{(n)}(0)}{n!} x^n\\] <ul> <li>Some famous Maclaurin series:</li> </ul> \\[e^x = 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\cdots\\] \\[\\sin x = x - \\frac{x^3}{3!} + \\frac{x^5}{5!} - \\frac{x^7}{7!} + \\cdots\\] \\[\\cos x = 1 - \\frac{x^2}{2!} + \\frac{x^4}{4!} - \\frac{x^6}{6!} + \\cdots\\] <ul> <li> <p>Notice that \\(\\sin x\\) has only odd powers (it is an odd function) and \\(\\cos x\\) has only even powers (it is an even function). The alternating signs cause the approximation to oscillate around the true value, converging from both sides.</p> </li> <li> <p>Let us approximate \\(e^{0.5}\\) using four terms: \\(1 + 0.5 + \\frac{0.25}{2} + \\frac{0.125}{6} = 1 + 0.5 + 0.125 + 0.02083 \\approx 1.6458\\). The true value is \\(1.6487\\ldots\\), so four terms already give us three correct decimal places.</p> </li> <li> <p>Not every Taylor series converges everywhere. The radius of convergence tells us how far from the centre \\(a\\) the series gives valid results. Within that radius, the polynomial approximation can be made as accurate as we want by adding more terms. Outside it, the series diverges.</p> </li> <li> <p>A power series is the general form: \\(\\sum_{n=0}^{\\infty} a_n (x - c)^n\\). Taylor series are power series where the coefficients are determined by derivatives. Other power series might be defined by some other rule. The ratio test determines convergence: compute \\(\\lim_{n \\to \\infty} \\left|\\frac{a_{n+1}}{a_n}\\right|\\). If this limit is \\(L\\), the radius of convergence is \\(R = 1/L\\).</p> </li> <li> <p>When we truncate a Taylor series after \\(n\\) terms, we incur an error. The Lagrange remainder bounds this error:</p> </li> </ul> \\[R_n(x) = \\frac{f^{(n+1)}(c)}{(n+1)!}(x-a)^{n+1}\\] <ul> <li> <p>Here \\(c\\) is some unknown point between \\(a\\) and \\(x\\). We do not know \\(c\\) exactly, but we can often bound \\(|f^{(n+1)}(c)|\\) to get a worst-case error estimate. The \\((n+1)!\\) in the denominator grows extremely fast, so the error shrinks rapidly as we add more terms (for functions within the radius of convergence).</p> </li> <li> <p>For a function of multiple variables, the Taylor expansion includes mixed partial derivatives. The second-order approximation of \\(f(\\mathbf{x})\\) around a point \\(\\mathbf{a}\\) is:</p> </li> </ul> \\[f(\\mathbf{x}) \\approx f(\\mathbf{a}) + \\nabla f(\\mathbf{a})^T (\\mathbf{x} - \\mathbf{a}) + \\frac{1}{2} (\\mathbf{x} - \\mathbf{a})^T H(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a})\\] <ul> <li> <p>The first term is the value, the second uses the gradient (a vector, as we saw in multivariate calculus), and the third uses the Hessian matrix (which captures curvature). This connects our matrices chapter directly to calculus: the Hessian is a matrix of second derivatives that describes the shape of the function's surface.</p> </li> <li> <p>This multivariate second-order approximation is the foundation of Newton's method and other second-order optimisation techniques, which we will see in the next file.</p> </li> <li> <p>Beyond polynomials, there are other approximation methods worth knowing about:</p> <ul> <li>Spline interpolation: instead of one high-degree polynomial, use many low-degree polynomials stitched together smoothly. This avoids the wild oscillations that high-degree polynomials can produce.</li> <li>Fourier series: approximate periodic functions as sums of sines and cosines. Essential in signal processing and audio.</li> <li>Neural networks: universal function approximators. With enough neurons, they can approximate any continuous function to arbitrary accuracy. This is the theoretical justification for deep learning.</li> </ul> </li> <li> <p>A function is called \"well-behaved\" if it has properties that make approximation reliable: continuity (no jumps), differentiability (no sharp corners), smoothness (derivatives of all orders exist), and boundedness (outputs stay finite). </p> </li> <li> <p>Polynomials, exponentials, and trigonometric functions are all well-behaved. The better-behaved a function is, the fewer Taylor terms you need for a good approximation.</p> </li> </ul>"},{"location":"chapter%2003%3A%20calculus/04.%20function%20approximation/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Approximate \\(e^x\\) using increasing numbers of Taylor terms and visualise how the approximation improves. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx = jnp.linspace(-2, 3, 300)\nplt.plot(x, jnp.exp(x), \"k-\", linewidth=2, label=\"e\u02e3 (exact)\")\n\ncolors = [\"#e74c3c\", \"#3498db\", \"#27ae60\", \"#9b59b6\"]\nfor n, color in zip([1, 2, 4, 8], colors):\n    approx = sum(x**k / jnp.array(float(jnp.prod(jnp.arange(1, k+1)) if k &gt; 0 else 1))\n                 for k in range(n+1))\n    plt.plot(x, approx, color=color, linestyle=\"--\", label=f\"{n} terms\")\n\nplt.ylim(-2, 15)\nplt.legend()\nplt.title(\"Taylor approximation of e\u02e3\")\nplt.show()\n</code></pre></p> </li> <li> <p>Compute the Lagrange remainder to bound the error of approximating \\(\\sin(1)\\) with different numbers of Taylor terms. <pre><code>import jax.numpy as jnp\n\nx = 1.0\nexact = jnp.sin(x)\n\ntaylor = 0.0\nfor n in range(8):\n    sign = (-1)**n\n    factorial = float(jnp.prod(jnp.arange(1, 2*n+2)))\n    taylor += sign * x**(2*n+1) / factorial\n    error = abs(exact - taylor)\n    bound = x**(2*n+3) / float(jnp.prod(jnp.arange(1, 2*n+4)))\n    print(f\"terms={n+1}  approx={taylor:.10f}  error={error:.2e}  bound={bound:.2e}\")\n</code></pre></p> </li> <li> <p>Compare linearisation vs quadratic Taylor approximation of \\(\\cos(x)\\) near \\(x = 0\\). Plot both approximations alongside the true function and observe the range where each is accurate. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx = jnp.linspace(-3, 3, 300)\nplt.plot(x, jnp.cos(x), \"k-\", linewidth=2, label=\"cos(x)\")\nplt.plot(x, jnp.ones_like(x), \"--\", color=\"#e74c3c\", label=\"linear: 1\")\nplt.plot(x, 1 - x**2/2, \"--\", color=\"#3498db\", label=\"quadratic: 1 - x\u00b2/2\")\nplt.plot(x, 1 - x**2/2 + x**4/24, \"--\", color=\"#27ae60\", label=\"4th order\")\nplt.ylim(-2, 2)\nplt.legend()\nplt.title(\"Taylor approximations of cos(x)\")\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2003%3A%20calculus/05.%20optimisation/","title":"Optimisation","text":"<p>Optimisation is the mathematical core of model training -- finding the parameters that minimise a loss function. This file covers critical points, convexity, gradient descent, Newton's method, constrained optimisation with Lagrange multipliers, and the optimisers (SGD, Adam) that power modern deep learning.</p> <ul> <li> <p>Training a neural network, fitting a regression line, tuning hyperparameters: at the core of almost every ML algorithm is an optimisation problem. </p> </li> <li> <p>We have some function (a loss, a cost, an objective) and we want to find the inputs that make it as small (or large) as possible. </p> </li> <li> <p>Before optimising, we need to understand zeros (or roots) of functions. A zero of \\(f(x)\\) is a value \\(x\\) where \\(f(x) = 0\\). Graphically, these are the x-intercepts.</p> </li> <li> <p>For example, \\(f(x) = x^2 - 3x + 2 = (x-1)(x-2)\\) has zeros at \\(x = 1\\) and \\(x = 2\\). Between the zeros, the function is negative (\\(f(1.5) = -0.25\\)); outside the zeros, it is positive. The zeros divide the number line into regions where the function has constant sign.</p> </li> <li> <p>The multiplicity of a zero is how many times the corresponding factor appears. </p> </li> <li> <p>At a simple zero (multiplicity 1), the graph crosses the x-axis. At a double zero (multiplicity 2), the graph touches the x-axis but bounces back without crossing, appearing \"flat\" at that point.</p> </li> <li> <p>Finding zeros matters because the zeros of the derivative \\(f'(x)\\) are the critical points of \\(f(x)\\), the candidates for maxima and minima. </p> </li> <li> <p>At a maximum or minimum, the tangent line is flat (slope = 0), so \\(f'(x) = 0\\).</p> </li> </ul> <p></p> <ul> <li> <p>But not every critical point is a maximum or minimum. A point where \\(f'(x) = 0\\) could also be an inflection point (like \\(x = 0\\) for \\(f(x) = x^3\\)), where the function flattens momentarily but does not change direction.</p> </li> <li> <p>The second derivative test resolves this. At a critical point \\(x = c\\) where \\(f'(c) = 0\\):</p> <ul> <li>If \\(f''(c) &gt; 0\\): the curve is concave up (like a bowl), so \\(c\\) is a local minimum.</li> <li>If \\(f''(c) &lt; 0\\): the curve is concave down (like a hill), so \\(c\\) is a local maximum.</li> <li>If \\(f''(c) = 0\\): the test is inconclusive; higher derivatives or other methods are needed.</li> </ul> </li> <li> <p>For example, \\(f(x) = x^3 - 3x\\). The derivative is \\(f'(x) = 3x^2 - 3 = 3(x-1)(x+1)\\), so critical points are at \\(x = -1\\) and \\(x = 1\\). The second derivative is \\(f''(x) = 6x\\). At \\(x = -1\\): \\(f''(-1) = -6 &lt; 0\\) (local max). At \\(x = 1\\): \\(f''(1) = 6 &gt; 0\\) (local min).</p> </li> <li> <p>A function is convex if the line segment between any two points on its graph lies above (or on) the graph. Think of it as a bowl shape, curving upward everywhere. Mathematically, \\(f\\) is convex if \\(f''(x) \\geq 0\\) for all \\(x\\).</p> </li> </ul> <p></p> <ul> <li> <p>Convexity is powerful because convex functions have a remarkable property: every local minimum is also the global minimum. There are no deceptive local valleys to get trapped in. If you roll a ball into a convex bowl, it will always reach the bottom.</p> </li> <li> <p>A function is concave (curving downward) if \\(-f\\) is convex. Points where the function transitions between concave and convex are inflection points, occurring where \\(f''(x) = 0\\).</p> </li> <li> <p>Newton's method finds zeros of functions (and by extension, critical points of their derivatives) using tangent lines. Starting from an initial guess \\(x_0\\), it iteratively refines:</p> </li> </ul> \\[x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\\] <p></p> <ul> <li> <p>The idea: at \\(x_n\\), draw the tangent line and find where it crosses the x-axis. That crossing point becomes \\(x_{n+1}\\). For well-behaved functions with a good starting point, Newton's method converges very quickly (quadratically, meaning the number of correct digits roughly doubles each step).</p> </li> <li> <p>For example, to find \\(\\sqrt{5}\\) (a zero of \\(f(x) = x^2 - 5\\)): \\(f'(x) = 2x\\), so \\(x_{n+1} = x_n - \\frac{x_n^2 - 5}{2x_n}\\). Starting at \\(x_0 = 2\\): \\(x_1 = 2.25\\), \\(x_2 = 2.2361\\ldots\\), which is already accurate to four decimal places.</p> </li> <li> <p>Newton's method can fail if the initial guess is far from the root, if \\(f'(x) = 0\\) near the root, or if the function has inflection points nearby. It also requires computing the derivative, which may be expensive.</p> </li> <li> <p>For optimisation (finding minima instead of zeros), we apply Newton's method to \\(f'(x) = 0\\), which gives the update:</p> </li> </ul> \\[x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)}\\] <ul> <li> <p>In multiple dimensions, this becomes \\(\\mathbf{x}_{n+1} = \\mathbf{x}_n - H^{-1} \\nabla f(\\mathbf{x}_n)\\), where \\(H\\) is the Hessian matrix. This is the second-order Taylor approximation from the previous file in action: approximate the function as a quadratic, jump to the minimum of that quadratic, repeat.</p> </li> <li> <p>Lagrange multipliers solve constrained optimisation: find the optimum of \\(f(x, y)\\) subject to a constraint \\(g(x, y) = c\\). Instead of searching all of \\(\\mathbb{R}^n\\), we are restricted to the set where the constraint holds (a curve or surface).</p> </li> <li> <p>The key insight is geometric: at the constrained optimum, the gradient of \\(f\\) must be parallel to the gradient of \\(g\\). If they were not parallel, we could move along the constraint in a direction that still improves \\(f\\), so we would not be at the optimum yet.</p> </li> <li> <p>We introduce a new variable \\(\\lambda\\) (the Lagrange multiplier) and define the Lagrangian:</p> </li> </ul> \\[\\mathcal{L}(x, y, \\lambda) = f(x, y) - \\lambda(g(x, y) - c)\\] <ul> <li>Setting all partial derivatives to zero gives a system of equations whose solutions are the constrained optima:</li> </ul> \\[\\frac{\\partial \\mathcal{L}}{\\partial x} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial y} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = 0\\] <p></p> <ul> <li>For example, maximise \\(f(x,y) = x^2 y\\) subject to \\(x^2 + y^2 = 1\\). The Lagrangian is \\(\\mathcal{L} = x^2 y - \\lambda(x^2 + y^2 - 1)\\). Taking partials:</li> </ul> \\[2xy - 2\\lambda x = 0, \\quad x^2 - 2\\lambda y = 0, \\quad x^2 + y^2 = 1\\] <ul> <li> <p>From the first equation (assuming \\(x \\neq 0\\)): \\(\\lambda = y\\). Substituting into the second: \\(x^2 = 2y^2\\). Combined with the constraint: \\(2y^2 + y^2 = 1\\), so \\(y = \\frac{1}{\\sqrt{3}}\\). The maximum value is \\(f = \\frac{2}{3\\sqrt{3}}\\).</p> </li> <li> <p>For inequality constraints (\\(g(x,y) \\leq c\\) instead of \\(= c\\)), the Karush-Kuhn-Tucker (KKT) conditions generalise Lagrange multipliers. The constraint is either active (binding, treated as equality) or inactive (the solution lies in the interior and the constraint is irrelevant).</p> </li> <li> <p>In practice, we rarely optimise by hand. Here are the main algorithmic families:</p> <ul> <li> <p>First-order methods (use only gradient): gradient descent, stochastic gradient descent (SGD), Adam. These are cheap per step but can converge slowly, especially on ill-conditioned problems.</p> </li> <li> <p>Second-order methods (use gradient and Hessian): Newton's method converges fast but computing and inverting the Hessian is expensive (\\(O(n^3)\\) for \\(n\\) parameters). Quasi-Newton methods (like BFGS and L-BFGS) approximate the Hessian using only gradient information, achieving faster convergence than first-order methods without the full cost of second-order methods.</p> </li> <li> <p>Conjugate gradient: efficient for large sparse systems, using only matrix-vector products instead of storing the full Hessian.</p> </li> <li> <p>Gauss-Newton and Levenberg-Marquardt: specialised for least-squares problems (common in regression), approximating the Hessian via the Jacobian.</p> </li> <li> <p>Natural gradient descent: accounts for the geometry of the parameter space using the Fisher information matrix, which can be more effective for probabilistic models.</p> </li> </ul> </li> <li> <p>The choice of optimiser depends on the problem. For deep learning, first-order methods (especially Adam) dominate because the number of parameters is enormous (millions to billions), making Hessian computation impractical. For smaller problems with smooth objectives, second-order methods can be dramatically faster.</p> </li> </ul>"},{"location":"chapter%2003%3A%20calculus/05.%20optimisation/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement Newton's method to find \\(\\sqrt{7}\\) (a zero of \\(f(x) = x^2 - 7\\)). Observe the rapid convergence. <pre><code>import jax.numpy as jnp\n\nf = lambda x: x**2 - 7\ndf = lambda x: 2*x\n\nx = 3.0  # initial guess\nfor i in range(6):\n    x = x - f(x) / df(x)\n    print(f\"step {i+1}: x = {x:.10f}  (error: {abs(x - jnp.sqrt(7.0)):.2e})\")\n</code></pre></p> </li> <li> <p>Use gradient descent to minimise \\(f(x, y) = (x - 3)^2 + (y + 1)^2\\). The minimum is at \\((3, -1)\\). Experiment with different learning rates. <pre><code>import jax\nimport jax.numpy as jnp\n\ndef f(params):\n    x, y = params\n    return (x - 3)**2 + (y + 1)**2\n\ngrad_f = jax.grad(f)\nparams = jnp.array([0.0, 0.0])\nlr = 0.1\n\nfor i in range(20):\n    g = grad_f(params)\n    params = params - lr * g\n    if i % 5 == 0 or i == 19:\n        print(f\"step {i:2d}: ({params[0]:.4f}, {params[1]:.4f})  loss={f(params):.6f}\")\n</code></pre></p> </li> <li> <p>Solve a constrained optimisation problem numerically. Maximise \\(f(x,y) = xy\\) subject to \\(x + y = 10\\) by parameterising \\(y = 10 - x\\) and finding the optimum of the single-variable function. <pre><code>import jax\nimport jax.numpy as jnp\n\n# Substitute constraint: y = 10 - x, so f = x(10 - x) = 10x - x\u00b2\nf = lambda x: x * (10 - x)\ndf = jax.grad(f)\n\n# Gradient ascent (we want maximum, so add gradient)\nx = 1.0\nlr = 0.1\nfor i in range(20):\n    x = x + lr * df(x)\nprint(f\"x={x:.4f}, y={10-x:.4f}, f={f(x):.4f}\")  # should be x=5, y=5, f=25\n</code></pre></p> </li> </ol>"},{"location":"chapter%2004%3A%20statistics/01.%20fundamentals/","title":"Fundamentals of Statistics","text":"<p>Statistics provides the language for describing data and quantifying uncertainty. This file covers distributions, random variables, PMFs, PDFs, CDFs, expectation, variance, moments, and the central limit theorem -- the concepts that underpin every ML evaluation metric and loss function.</p> <ul> <li> <p>Statistics is the science of learning from data. You collect observations, summarise them, and draw conclusions, often about things you cannot measure directly.</p> </li> <li> <p>Imagine you want to know the average height of every adult in a country. You cannot measure everyone, so you measure a sample and use statistics to make an informed guess about the whole population.</p> </li> <li> <p>There are two main branches:</p> <ul> <li>Descriptive statistics: summarising data you already have (averages, charts, tables)</li> <li>Inferential statistics: using a sample to make claims about a larger group</li> </ul> </li> <li> <p>The building block of statistics is the distribution, a description of how values are spread out. Everything else, averages, tests, predictions, flows from understanding distributions.</p> </li> <li> <p>A frequency distribution counts how often each value (or range of values) appears in your data. Think of sorting exam scores into bins and counting how many students fall in each bin. The result is a histogram.</p> </li> <li> <p>A probability distribution replaces raw counts with probabilities. Instead of \"12 students scored between 70 and 80,\" it says \"there is a 0.24 probability of scoring between 70 and 80.\" The histogram bars become a smooth curve when the data is continuous.</p> </li> </ul> <p></p> <ul> <li> <p>The histogram on the left is built from actual data you collected. The smooth curve on the right is a mathematical model that describes the pattern behind the data. One is empirical, the other is theoretical.</p> </li> <li> <p>To work with distributions mathematically, we need a way to assign numbers to outcomes. That is exactly what a random variable does.</p> </li> <li> <p>A random variable is a function that maps each outcome of an experiment to a real number. Flip a coin: the outcome is \"heads\" or \"tails,\" but a random variable \\(X\\) converts this to \\(X(\\text{heads}) = 1\\) and \\(X(\\text{tails}) = 0\\). Now we can do arithmetic.</p> </li> </ul> <p></p> <ul> <li> <p>A discrete random variable takes on a countable set of values: the number of heads in 10 flips, the roll of a die, the number of emails you receive in an hour.</p> </li> <li> <p>A continuous random variable can take any value in an interval: your exact height, the time until the next bus arrives, the temperature at noon.</p> </li> <li> <p>The distinction matters because it changes how we compute probabilities. For discrete variables, we sum. For continuous variables, we integrate (recall integrals from Chapter 3).</p> </li> <li> <p>For a discrete random variable, the probability mass function (PMF) gives the probability of each specific value:</p> </li> </ul> \\[P(X = x) = p(x), \\quad \\text{where } \\sum_{x} p(x) = 1\\] <ul> <li>For a continuous random variable, the probability density function (PDF) gives the probability of falling within a range. The probability of any single exact value is zero; only intervals have positive probability:</li> </ul> \\[P(a \\le X \\le b) = \\int_a^b f(x)\\, dx, \\quad \\text{where } \\int_{-\\infty}^{\\infty} f(x)\\, dx = 1\\] <ul> <li> <p>Now that we can assign numbers to outcomes, the most natural question is: what value do we expect on average?</p> </li> <li> <p>Expectation (or expected value) is the weighted average of all possible values, where the weights are the probabilities. Think of it as the \"centre of gravity\" of the distribution.</p> </li> <li> <p>If you roll a fair die many times, your average roll converges to 3.5. That is the expected value, even though you can never actually roll a 3.5.</p> </li> <li> <p>For a discrete random variable:</p> </li> </ul> \\[E[X] = \\sum_{x} x \\cdot p(x)\\] <ul> <li>For a continuous random variable (using the integral from Chapter 3):</li> </ul> \\[E[X] = \\int_{-\\infty}^{\\infty} x \\cdot f(x)\\, dx\\] <ul> <li>Example: a fair six-sided die has \\(p(x) = 1/6\\) for \\(x = 1, 2, 3, 4, 5, 6\\).</li> </ul> \\[E[X] = 1 \\cdot \\tfrac{1}{6} + 2 \\cdot \\tfrac{1}{6} + 3 \\cdot \\tfrac{1}{6} + 4 \\cdot \\tfrac{1}{6} + 5 \\cdot \\tfrac{1}{6} + 6 \\cdot \\tfrac{1}{6} = \\frac{21}{6} = 3.5\\] <ul> <li> <p>Expectation is linear, meaning \\(E[aX + b] = aE[X] + b\\). This property is extremely useful and shows up constantly in ML loss functions.</p> </li> <li> <p>Expectation tells us the centre, but it says nothing about how spread out the values are. To describe the full shape of a distribution, we need moments.</p> </li> <li> <p>A moment is an expectation of a power of \\(X\\). The \\(k\\)-th raw moment is:</p> </li> </ul> \\[\\mu_k' = E[X^k]\\] <ul> <li> <p>The first raw moment (\\(k = 1\\)) is just the mean: \\(\\mu_1' = E[X] = \\mu\\).</p> </li> <li> <p>Raw moments are measured from zero. Often we care about deviation from the mean instead. The \\(k\\)-th central moment centres the measurement:</p> </li> </ul> \\[\\mu_k = E[(X - \\mu)^k]\\] <ul> <li> <p>The first central moment is always zero (deviations above and below the mean cancel). The second central moment is the variance.</p> </li> <li> <p>To compare distributions on different scales, we standardise by dividing by the appropriate power of the standard deviation \\(\\sigma\\):</p> </li> </ul> \\[\\tilde{\\mu}_k = \\frac{\\mu_k}{\\sigma^k}\\] <ul> <li>Each moment captures a different aspect of the distribution's shape:</li> </ul> <p></p> <ul> <li>1st moment (Mean): Where the distribution is centred. The balance point.</li> <li>2nd moment (Variance): How spread out values are around the mean. Higher variance means wider.</li> <li>3rd moment (Skewness): Whether the distribution leans left or right. Zero skewness means symmetric.</li> <li> <p>4th moment (Kurtosis): How heavy the tails are. Higher kurtosis means more extreme outliers.</p> </li> <li> <p>Let us work through all four moments for a concrete dataset: \\(X = \\{2, 4, 4, 4, 5, 5, 7, 9\\}\\).</p> </li> <li> <p>Step 1: Mean (1st raw moment)</p> </li> </ul> \\[\\mu = \\frac{2 + 4 + 4 + 4 + 5 + 5 + 7 + 9}{8} = \\frac{40}{8} = 5\\] <ul> <li>Step 2: Variance (2nd central moment). Subtract the mean from each value, square, then average:</li> </ul> \\[\\sigma^2 = \\frac{(2{-}5)^2 + (4{-}5)^2 + (4{-}5)^2 + (4{-}5)^2 + (5{-}5)^2 + (5{-}5)^2 + (7{-}5)^2 + (9{-}5)^2}{8}\\] \\[= \\frac{9 + 1 + 1 + 1 + 0 + 0 + 4 + 16}{8} = \\frac{32}{8} = 4\\] <ul> <li> <p>The standard deviation is \\(\\sigma = \\sqrt{4} = 2\\).</p> </li> <li> <p>Step 3: Skewness (standardised 3rd central moment). Cube the deviations, average, divide by \\(\\sigma^3\\):</p> </li> </ul> \\[\\tilde{\\mu}_3 = \\frac{1}{8} \\cdot \\frac{(-3)^3 + (-1)^3 + (-1)^3 + (-1)^3 + 0^3 + 0^3 + 2^3 + 4^3}{2^3}\\] \\[= \\frac{1}{8} \\cdot \\frac{-27 -1 -1 -1 + 0 + 0 + 8 + 64}{8} = \\frac{42}{64} = 0.656\\] <ul> <li> <p>Positive skewness means the right tail is longer, which makes sense since 9 is far above the mean.</p> </li> <li> <p>Step 4: Kurtosis (standardised 4th central moment). Raise deviations to the 4th power:</p> </li> </ul> \\[\\tilde{\\mu}_4 = \\frac{1}{8} \\cdot \\frac{(-3)^4 + (-1)^4 + (-1)^4 + (-1)^4 + 0^4 + 0^4 + 2^4 + 4^4}{2^4}\\] \\[= \\frac{1}{8} \\cdot \\frac{81 + 1 + 1 + 1 + 0 + 0 + 16 + 256}{16} = \\frac{356}{128} = 2.781\\] <ul> <li>A normal distribution has kurtosis of 3 (called \"mesokurtic\"). Our value of 2.781 is close, suggesting the tails are roughly normal. Values above 3 (\"leptokurtic\") signal heavier tails; below 3 (\"platykurtic\") signal lighter tails. Some formulas report excess kurtosis by subtracting 3, so our excess kurtosis would be \\(-0.219\\).</li> </ul>"},{"location":"chapter%2004%3A%20statistics/01.%20fundamentals/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Compute the expected value of a loaded die where face 6 has probability 0.3 and all other faces share the remaining probability equally. Verify by simulating 100,000 rolls. <pre><code>import jax\nimport jax.numpy as jnp\n\n# Loaded die: face 6 has p=0.3, others share 0.7 equally\nprobs = jnp.array([0.14, 0.14, 0.14, 0.14, 0.14, 0.30])\nfaces = jnp.array([1, 2, 3, 4, 5, 6])\n\n# Analytical expected value\nev = jnp.sum(faces * probs)\nprint(f\"Expected value (formula): {ev:.4f}\")\n\n# Simulation\nkey = jax.random.PRNGKey(42)\nrolls = jax.random.choice(key, faces, shape=(100_000,), p=probs)\nprint(f\"Expected value (simulation): {rolls.mean():.4f}\")\n</code></pre></p> </li> <li> <p>Compute all four moments (mean, variance, skewness, kurtosis) for the dataset from the worked example, then modify the data and observe how each moment changes. <pre><code>import jax.numpy as jnp\n\nx = jnp.array([2, 4, 4, 4, 5, 5, 7, 9], dtype=jnp.float32)\n\nmean = jnp.mean(x)\nvariance = jnp.mean((x - mean) ** 2)\nstd = jnp.sqrt(variance)\nskewness = jnp.mean(((x - mean) / std) ** 3)\nkurtosis = jnp.mean(((x - mean) / std) ** 4)\n\nprint(f\"Mean:     {mean:.3f}\")\nprint(f\"Variance: {variance:.3f}\")\nprint(f\"Std Dev:  {std:.3f}\")\nprint(f\"Skewness: {skewness:.3f}\")\nprint(f\"Kurtosis: {kurtosis:.3f}\")\nprint(f\"Excess K: {kurtosis - 3:.3f}\")\n</code></pre></p> </li> <li> <p>Visualise a PMF and CDF side by side for a fair die roll. Try changing the probabilities to see how the shapes shift. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nfaces = jnp.array([1, 2, 3, 4, 5, 6])\npmf = jnp.ones(6) / 6  # fair die; try changing these!\ncdf = jnp.cumsum(pmf)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n\nax1.bar(faces, pmf, color=\"#3498db\", alpha=0.8)\nax1.set_title(\"PMF\")\nax1.set_xlabel(\"Face\")\nax1.set_ylabel(\"P(X = x)\")\nax1.set_ylim(0, 0.5)\n\nax2.step(faces, cdf, where=\"mid\", color=\"#e74c3c\", linewidth=2)\nax2.set_title(\"CDF\")\nax2.set_xlabel(\"Face\")\nax2.set_ylabel(\"P(X \u2264 x)\")\nax2.set_ylim(0, 1.1)\n\nplt.tight_layout()\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2004%3A%20statistics/02.%20measures/","title":"Statistical Measures","text":"<p>Statistical measures summarise data with single numbers that capture spread, position, shape, and association. This file covers variance, standard deviation, quartiles, skewness, kurtosis, covariance, correlation, and z-scores -- the toolkit for exploratory data analysis and feature engineering in ML.</p> <ul> <li> <p>In the previous file we introduced moments as a family of summary statistics. Here we unpack the practical tools that flow from them: measures of dispersion, position, shape, and association.</p> </li> <li> <p>Dispersion answers the question: how spread out is the data? Two classrooms can have the same average test score, but very different spreads.</p> </li> </ul> <p></p> <ul> <li> <p>The narrow (blue) distribution has low variance: most values cluster tightly around the mean. The wide (red) distribution has high variance: values are scattered further out.</p> </li> <li> <p>Variance is the average squared distance from the mean. We square to avoid positive and negative deviations cancelling each other out.</p> </li> </ul> \\[\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\mu)^2\\] <ul> <li>When working with a sample (not the full population), we divide by \\(N - 1\\) instead of \\(N\\). This correction (called Bessel's correction) accounts for the fact that a sample tends to underestimate the true variability:</li> </ul> \\[s^2 = \\frac{1}{N-1} \\sum_{i=1}^{N} (x_i - \\bar{x})^2\\] <ul> <li> <p>Standard deviation is the square root of variance: \\(\\sigma = \\sqrt{\\sigma^2}\\). It brings the measure back to the original units. If your data is in centimetres, variance is in cm\\(^2\\), but standard deviation is back in cm.</p> </li> <li> <p>Mean Absolute Deviation (MAD) is a simpler alternative. Instead of squaring, take the absolute value of each deviation:</p> </li> </ul> \\[\\text{MAD} = \\frac{1}{N} \\sum_{i=1}^{N} |x_i - \\mu|\\] <ul> <li> <p>MAD is more robust to outliers than variance because it does not amplify large deviations by squaring them. However, variance is more mathematically convenient (it decomposes nicely in proofs and ML optimisation).</p> </li> <li> <p>Position answers a different question: where does a specific value sit relative to the rest of the data?</p> </li> <li> <p>Quartiles split sorted data into four equal parts. Q1 (25th percentile) is the value below which 25% of data falls. Q2 is the median (50th percentile). Q3 is the 75th percentile.</p> </li> <li> <p>The Interquartile Range (IQR) is \\(Q3 - Q1\\). It captures the spread of the middle 50% of data, ignoring extremes.</p> </li> </ul> <p></p> <ul> <li> <p>The box plot is one of the most useful visualisations in statistics. The box spans Q1 to Q3, the line inside is the median, whiskers extend to the most extreme non-outlier values, and dots beyond the whiskers are outliers.</p> </li> <li> <p>Percentiles generalise quartiles. The \\(p\\)-th percentile is the value below which \\(p\\%\\) of observations fall. Q1 is the 25th percentile, the median is the 50th, and Q3 is the 75th.</p> </li> <li> <p>The z-score tells you how many standard deviations a value is from the mean:</p> </li> </ul> \\[z = \\frac{x - \\mu}{\\sigma}\\] <ul> <li> <p>A z-score of 2 means the value is 2 standard deviations above the mean. A z-score of \\(-1.5\\) means it is 1.5 standard deviations below. This is also called standardisation and is used heavily in ML for feature scaling, as it transforms any distribution to have mean 0 and standard deviation 1.</p> </li> <li> <p>Shape describes the geometry of a distribution beyond its centre and spread.</p> </li> <li> <p>Skewness (the standardised 3rd moment from the previous file) measures asymmetry. A perfectly symmetric distribution like the normal curve has skewness of zero. Positive skewness means a longer right tail (e.g. income distributions). Negative skewness means a longer left tail (e.g. age at retirement).</p> </li> </ul> \\[\\text{Skewness} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\frac{x_i - \\mu}{\\sigma}\\right)^3\\] <ul> <li>Kurtosis (the standardised 4th moment) measures tail heaviness. The normal distribution has kurtosis of 3. Distributions with heavier tails (more prone to outliers) have kurtosis greater than 3.</li> </ul> \\[\\text{Kurtosis} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(\\frac{x_i - \\mu}{\\sigma}\\right)^4\\] <ul> <li>Correlation measures the strength and direction of a relationship between two variables. It answers: when one variable goes up, does the other tend to go up, go down, or do nothing?</li> </ul> <p></p> <ul> <li>Pearson correlation (\\(r\\)) measures linear association. It ranges from \\(-1\\) (perfect negative) through \\(0\\) (none) to \\(+1\\) (perfect positive).</li> </ul> \\[r = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum (x_i - \\bar{x})^2} \\cdot \\sqrt{\\sum (y_i - \\bar{y})^2}}\\] <ul> <li> <p>If you recall dot products from Chapter 1, Pearson correlation is essentially the cosine similarity between the mean-centred versions of \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\).</p> </li> <li> <p>Spearman correlation (\\(\\rho\\)) measures monotonic association. Instead of using raw values, it ranks them first and then computes Pearson correlation on the ranks. This makes it robust to outliers and works even when the relationship is nonlinear, as long as it is consistently increasing or decreasing.</p> </li> <li> <p>Geometric mean is the appropriate average when values multiply together, like growth rates. If your investment grows by 10%, then 20%, then 30%, the average growth factor is not the arithmetic mean of those rates. Instead:</p> </li> </ul> \\[\\bar{x}_{\\text{geo}} = \\left(\\prod_{i=1}^{N} x_i\\right)^{1/N}\\] <ul> <li> <p>For growth rates specifically, convert percentages to factors first (1.10, 1.20, 1.30), compute the geometric mean, then subtract 1.</p> </li> <li> <p>Exponential Moving Average (EMA) gives more weight to recent observations. Unlike a simple moving average where all points in the window are equally weighted, EMA decays exponentially:</p> </li> </ul> \\[\\text{EMA}_t = \\alpha \\cdot x_t + (1 - \\alpha) \\cdot \\text{EMA}_{t-1}\\] <ul> <li> <p>The smoothing factor \\(\\alpha\\) (between 0 and 1) controls how quickly old observations lose influence. Higher \\(\\alpha\\) means more responsive to recent changes, lower \\(\\alpha\\) means smoother. In ML, EMA is used in optimisers like Adam and in batch normalisation's running statistics.</p> </li> <li> <p>Outlier detection identifies data points that are unusually far from the rest. Two common methods:</p> <ul> <li>IQR method: a point is an outlier if it falls below \\(Q1 - 1.5 \\times \\text{IQR}\\) or above \\(Q3 + 1.5 \\times \\text{IQR}\\)</li> <li>Z-score method: a point is an outlier if \\(|z| &gt; 3\\) (more than 3 standard deviations from the mean)</li> </ul> </li> <li> <p>The IQR method is more robust because it does not assume a normal distribution. The z-score method works well when data is approximately normal but can fail when the distribution is heavily skewed.</p> </li> </ul>"},{"location":"chapter%2004%3A%20statistics/02.%20measures/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Compute variance, standard deviation, and MAD for a dataset and compare them. Observe what happens when you add an extreme outlier. <pre><code>import jax.numpy as jnp\n\ndata = jnp.array([4, 8, 6, 5, 3, 7, 9, 5, 6, 7], dtype=jnp.float32)\n\nmean = jnp.mean(data)\nvariance = jnp.var(data)\nstd = jnp.std(data)\nmad = jnp.mean(jnp.abs(data - mean))\n\nprint(\"Original data:\")\nprint(f\"  Variance: {variance:.3f}, Std: {std:.3f}, MAD: {mad:.3f}\")\n\n# Add an outlier and recompute\ndata_outlier = jnp.append(data, 100.0)\nmean2 = jnp.mean(data_outlier)\nprint(f\"\\nWith outlier (100):\")\nprint(f\"  Variance: {jnp.var(data_outlier):.3f}, Std: {jnp.std(data_outlier):.3f}, MAD: {jnp.mean(jnp.abs(data_outlier - mean2)):.3f}\")\n</code></pre></p> </li> <li> <p>Compute Pearson and Spearman correlation between two variables. Experiment with different relationships. <pre><code>import jax\nimport jax.numpy as jnp\n\n# Perfect linear relationship\nx = jnp.array([1, 2, 3, 4, 5, 6, 7, 8], dtype=jnp.float32)\ny = 2 * x + 1  # try changing this!\n\ndef pearson(a, b):\n    a_c = a - jnp.mean(a)\n    b_c = b - jnp.mean(b)\n    return jnp.sum(a_c * b_c) / (jnp.sqrt(jnp.sum(a_c**2)) * jnp.sqrt(jnp.sum(b_c**2)))\n\ndef spearman(a, b):\n    rank_a = jnp.argsort(jnp.argsort(a)).astype(jnp.float32)\n    rank_b = jnp.argsort(jnp.argsort(b)).astype(jnp.float32)\n    return pearson(rank_a, rank_b)\n\nprint(f\"Pearson r:  {pearson(x, y):.4f}\")\nprint(f\"Spearman \u03c1: {spearman(x, y):.4f}\")\n</code></pre></p> </li> <li> <p>Implement outlier detection using both the IQR and z-score methods, then compare their results on skewed data. <pre><code>import jax.numpy as jnp\n\ndata = jnp.array([2, 3, 3, 4, 5, 5, 5, 6, 6, 7, 50], dtype=jnp.float32)\n\n# IQR method\nq1, q3 = jnp.percentile(data, 25), jnp.percentile(data, 75)\niqr = q3 - q1\nlower, upper = q1 - 1.5 * iqr, q3 + 1.5 * iqr\niqr_outliers = data[(data &lt; lower) | (data &gt; upper)]\nprint(f\"IQR bounds: [{lower:.1f}, {upper:.1f}]\")\nprint(f\"IQR outliers: {iqr_outliers}\")\n\n# Z-score method\nz_scores = (data - jnp.mean(data)) / jnp.std(data)\nz_outliers = data[jnp.abs(z_scores) &gt; 3]\nprint(f\"\\nZ-scores: {z_scores}\")\nprint(f\"Z-score outliers (|z| &gt; 3): {z_outliers}\")\n</code></pre></p> </li> <li> <p>Compute and plot an Exponential Moving Average with different smoothing factors on noisy data. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Generate noisy data\nkey = __import__(\"jax\").random.PRNGKey(0)\nnoise = __import__(\"jax\").random.normal(key, shape=(50,))\nsignal = jnp.linspace(0, 5, 50) + noise\n\ndef ema(data, alpha):\n    result = jnp.zeros_like(data)\n    result = result.at[0].set(data[0])\n    for t in range(1, len(data)):\n        result = result.at[t].set(alpha * data[t] + (1 - alpha) * result[t - 1])\n    return result\n\nplt.figure(figsize=(10, 4))\nplt.plot(signal, \"o\", alpha=0.3, label=\"raw data\", color=\"#999\")\nfor alpha, color in [(0.1, \"#e74c3c\"), (0.3, \"#3498db\"), (0.7, \"#27ae60\")]:\n    plt.plot(ema(signal, alpha), label=f\"\u03b1={alpha}\", color=color, linewidth=2)\nplt.legend()\nplt.title(\"EMA with different smoothing factors\")\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2004%3A%20statistics/03.%20sampling/","title":"Sampling","text":"<p>Sampling determines how we collect data and directly controls the quality of every conclusion we draw. This file covers random, stratified, cluster, and systematic sampling, sampling distributions, the law of large numbers, and bootstrapping -- methods essential for training/test splits and dataset curation in ML.</p> <ul> <li> <p>In an ideal world, you would measure every single member of the group you care about. In practice, that is almost never possible. You cannot survey every voter, test every light bulb, or scan every patient. So you take a sample and use it to learn about the whole.</p> </li> <li> <p>The population is the complete set of individuals or items you want to study. The sample is the subset you actually observe.</p> </li> <li> <p>A parameter is a number that describes the population (e.g. the true average height of all adults in a country). </p> </li> <li> <p>A statistic is a number computed from your sample (e.g. the average height of the 500 people you measured). Statistics are used to estimate parameters.</p> </li> <li> <p>The quality of your conclusions depends entirely on how you select your sample. A biased sample leads to biased conclusions, no matter how sophisticated your analysis.</p> </li> <li> <p>The sampling frame is the list of all individuals from which you actually draw your sample. Ideally this matches the population perfectly, but in practice there are gaps. </p> </li> <li> <p>For instnce, if you survey people by phone, you miss everyone without a phone. The difference between the frame and the population is called coverage error.</p> </li> <li> <p>Sampling error is the natural discrepancy between a sample statistic and the population parameter. </p> </li> <li> <p>Even a perfectly random sample will not match the population exactly. Larger samples reduce sampling error.</p> </li> <li> <p>There are two broad families of sampling: probability and non-probability.</p> </li> <li> <p>Probability sampling means every member of the population has a known, nonzero chance of being selected. This lets you quantify uncertainty and generalise results.</p> </li> <li> <p>Simple random sampling: every individual has an equal chance of being selected, and every possible sample of size \\(n\\) is equally likely. Think of putting every name in a hat and drawing blindly.</p> </li> <li> <p>Stratified sampling: divide the population into non-overlapping groups (strata) based on a shared characteristic (e.g. age group, region), then randomly sample from each stratum. This guarantees representation from every group and reduces variance when strata differ from each other.</p> </li> <li> <p>Cluster sampling: divide the population into groups (clusters), randomly select some clusters, then include everyone in the chosen clusters. This is practical when the population is spread out geographically, like sampling entire schools rather than individual students across a district.</p> </li> <li> <p>Systematic sampling: pick a random starting point, then select every \\(k\\)-th individual from the list. For example, start at person 7 and then take every 10th person (7, 17, 27, ...). Simple to implement but can introduce bias if the list has a hidden pattern.</p> </li> </ul> <p></p> <ul> <li> <p>Non-probability sampling does not give every member a known chance of selection. Results cannot be rigorously generalised, but these methods are often faster and cheaper.</p> </li> <li> <p>Convenience sampling: select whoever is easiest to reach. Surveying people at a shopping mall is convenient but misses those who do not shop there.</p> </li> <li> <p>Quota sampling: like stratified sampling, but without randomness. The researcher fills quotas (e.g. 50 men and 50 women) by picking accessible individuals from each group.</p> </li> <li> <p>Snowball sampling: start with a few participants and ask them to recruit others. Useful for hard-to-reach populations (e.g. studying rare diseases), but heavily biased toward connected individuals.</p> </li> <li> <p>Once you have a sampling method, a natural question arises: if I took a different sample, would I get a different statistic? Almost certainly yes. The sampling distribution is the distribution of a statistic (like the sample mean) across all possible samples of the same size.</p> </li> <li> <p>Imagine drawing 1,000 different samples of 30 people and computing the mean height of each. Those 1,000 means form a distribution. Some will be a bit above the true population mean, some a bit below, and most will cluster around the true value.</p> </li> <li> <p>The standard deviation of this sampling distribution is called the standard error:</p> </li> </ul> \\[SE = \\frac{\\sigma}{\\sqrt{n}}\\] <ul> <li> <p>Notice that the standard error shrinks as \\(n\\) grows. Larger samples give more precise estimates. Quadrupling the sample size halves the standard error.</p> </li> <li> <p>The most important result in statistics is the Central Limit Theorem (CLT). It says: no matter what the shape of the original population, the distribution of sample means approaches a normal distribution as the sample size increases.</p> </li> </ul> <p></p> <ul> <li>More precisely, if \\(X_1, X_2, \\ldots, X_n\\) are independent observations from any distribution with mean \\(\\mu\\) and finite variance \\(\\sigma^2\\), then as \\(n\\) grows:</li> </ul> \\[\\bar{X} \\approx \\text{Normal}\\!\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\] <ul> <li> <p>The CLT is what makes most of inferential statistics work. It lets us use the normal distribution as an approximation even when the underlying data is not normal, as long as the sample is large enough.</p> </li> <li> <p>How large is \"large enough\"? A common rule of thumb is \\(n \\ge 30\\), but this depends on how non-normal the population is. For highly skewed distributions, you may need more. For roughly symmetric populations, even \\(n = 10\\) can be sufficient.</p> </li> <li> <p>The CLT has three key conditions:</p> <ul> <li>Independence: each observation must not influence the others</li> <li>Finite variance: the population variance must exist (rules out some exotic distributions)</li> <li>Identical distribution: all observations come from the same distribution</li> </ul> </li> </ul>"},{"location":"chapter%2004%3A%20statistics/03.%20sampling/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Demonstrate the CLT visually: draw samples from a highly skewed distribution, compute sample means, and watch the histogram of means become bell-shaped. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nkey = jax.random.PRNGKey(0)\n\n# Exponential distribution (very skewed)\npopulation = jax.random.exponential(key, shape=(100_000,))\n\nfig, axes = plt.subplots(1, 4, figsize=(14, 3))\nsample_sizes = [1, 5, 30, 100]\n\nfor ax, n in zip(axes, sample_sizes):\n    keys = jax.random.split(key, 2000)\n    means = jnp.array([jax.random.choice(k, population, shape=(n,)).mean() for k in keys])\n    ax.hist(means, bins=40, color=\"#3498db\", alpha=0.7, density=True)\n    ax.set_title(f\"n = {n}\")\n    ax.set_xlim(0, 4)\n\nfig.suptitle(\"CLT: sample means become normal as n increases\", fontsize=13)\nplt.tight_layout()\nplt.show()\n</code></pre></p> </li> <li> <p>Compare simple random sampling with stratified sampling. Create a population with distinct groups and show that stratified sampling gives lower variance in estimates. <pre><code>import jax\nimport jax.numpy as jnp\n\nkey = jax.random.PRNGKey(42)\n\n# Population: two distinct groups\ngroup_a = jax.random.normal(key, shape=(500,)) + 10   # mean ~10\nkey, subkey = jax.random.split(key)\ngroup_b = jax.random.normal(subkey, shape=(500,)) + 20  # mean ~20\npopulation = jnp.concatenate([group_a, group_b])\n\n# Simple random sampling: 1000 trials, sample size 20\nsrs_means = []\nfor i in range(1000):\n    key, subkey = jax.random.split(key)\n    sample = jax.random.choice(subkey, population, shape=(20,), replace=False)\n    srs_means.append(sample.mean())\nsrs_means = jnp.array(srs_means)\n\n# Stratified sampling: 10 from each group\nstrat_means = []\nfor i in range(1000):\n    key, k1, k2 = jax.random.split(key, 3)\n    s_a = jax.random.choice(k1, group_a, shape=(10,), replace=False)\n    s_b = jax.random.choice(k2, group_b, shape=(10,), replace=False)\n    strat_means.append(jnp.concatenate([s_a, s_b]).mean())\nstrat_means = jnp.array(strat_means)\n\nprint(f\"Simple Random - Mean: {srs_means.mean():.3f}, Std: {srs_means.std():.3f}\")\nprint(f\"Stratified    - Mean: {strat_means.mean():.3f}, Std: {strat_means.std():.3f}\")\nprint(f\"Stratified sampling reduced variance by {(1 - strat_means.var()/srs_means.var())*100:.1f}%\")\n</code></pre></p> </li> <li> <p>Explore how sample size affects standard error. Plot the standard error against sample size and confirm the \\(1/\\sqrt{n}\\) relationship. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nkey = jax.random.PRNGKey(7)\npopulation = jax.random.normal(key, shape=(50_000,)) * 10 + 50\n\nsample_sizes = [5, 10, 20, 50, 100, 200, 500, 1000]\nstd_errors = []\n\nfor n in sample_sizes:\n    means = []\n    for _ in range(500):\n        key, subkey = jax.random.split(key)\n        sample = jax.random.choice(subkey, population, shape=(n,))\n        means.append(sample.mean())\n    std_errors.append(jnp.array(means).std())\n\nplt.figure(figsize=(8, 4))\nplt.plot(sample_sizes, std_errors, \"o-\", color=\"#e74c3c\", label=\"Observed SE\")\ntheoretical = population.std() / jnp.sqrt(jnp.array(sample_sizes, dtype=jnp.float32))\nplt.plot(sample_sizes, theoretical, \"--\", color=\"#3498db\", label=\"\u03c3/\u221an (theoretical)\")\nplt.xlabel(\"Sample size (n)\")\nplt.ylabel(\"Standard error\")\nplt.legend()\nplt.title(\"Standard error shrinks with larger samples\")\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2004%3A%20statistics/04.%20hypothesis%20testing/","title":"Hypothesis Testing","text":"<p>Hypothesis testing provides a rigorous framework for deciding whether observed effects are real or due to chance. This file covers null and alternative hypotheses, p-values, significance levels, t-tests, chi-squared tests, ANOVA, and Type I/II errors -- the same logic used in A/B testing and model comparison.</p> <ul> <li> <p>Statistics is not just about describing data. Often you need to make a decision: does a new drug work? Is one algorithm faster than another? Has the average changed? Hypothesis testing gives you a structured framework for answering these questions using data.</p> </li> <li> <p>The idea is simple: assume nothing has changed (the \"null hypothesis\"), then check whether the data is so extreme that this assumption becomes hard to believe.</p> </li> <li> <p>The null hypothesis (\\(H_0\\)) is the default claim, usually a statement of \"no effect\" or \"no difference.\" For example: \"the average delivery time is still 30 minutes\" or \"the new model is no better than the old one.\"</p> </li> <li> <p>The alternative hypothesis (\\(H_1\\) or \\(H_a\\)) is what you suspect might be true instead: \"the average delivery time has changed\" or \"the new model is better.\"</p> </li> <li> <p>You never prove \\(H_1\\) directly. Instead, you ask: if \\(H_0\\) were true, how likely is it that I would see data this extreme? If it is very unlikely, you reject \\(H_0\\) in favour of \\(H_1\\).</p> </li> <li> <p>The test statistic is a single number that summarises how far your sample result is from what \\(H_0\\) predicts. Different tests use different formulas, but the logic is always the same: measure the distance between observed and expected.</p> </li> <li> <p>The p-value is the probability of observing a test statistic at least as extreme as yours, assuming \\(H_0\\) is true. A small p-value means the data is surprising under \\(H_0\\).</p> </li> <li> <p>The significance level (\\(\\alpha\\)) is the threshold you set before looking at the data. If \\(p \\le \\alpha\\), you reject \\(H_0\\). Common choices are \\(\\alpha = 0.05\\) (5%) and \\(\\alpha = 0.01\\) (1%).</p> </li> </ul> <p></p> <ul> <li> <p>The shaded tails are the rejection regions. If your test statistic lands there, the data is surprising enough under \\(H_0\\) that you reject it. The green area shows the p-value for a particular test statistic.</p> </li> <li> <p>Here is the step-by-step procedure:</p> <ul> <li>Step 1: State \\(H_0\\) and \\(H_1\\)</li> <li>Step 2: Choose a significance level \\(\\alpha\\)</li> <li>Step 3: Collect data and compute the test statistic</li> <li>Step 4: Find the p-value (or compare the test statistic to a critical value)</li> <li>Step 5: If \\(p \\le \\alpha\\), reject \\(H_0\\). Otherwise, fail to reject \\(H_0\\)</li> </ul> </li> <li> <p>Worked example: A factory claims their bolts have a mean length of 10 cm. You measure 36 bolts and find a sample mean of 10.3 cm. The known population standard deviation is 0.9 cm. Is there evidence that the mean has changed?</p> </li> <li> <p>\\(H_0\\): \\(\\mu = 10\\), \\(H_1\\): \\(\\mu \\neq 10\\), \\(\\alpha = 0.05\\)</p> </li> <li> <p>Test statistic (z-test, since \\(\\sigma\\) is known and \\(n\\) is large):</p> </li> </ul> \\[z = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}} = \\frac{10.3 - 10}{0.9 / \\sqrt{36}} = \\frac{0.3}{0.15} = 2.0\\] <ul> <li> <p>For a two-tailed test at \\(\\alpha = 0.05\\), the critical values are \\(\\pm 1.96\\). Our \\(z = 2.0 &gt; 1.96\\), so we reject \\(H_0\\). The p-value is approximately 0.046, which is less than 0.05.</p> </li> <li> <p>Conclusion: there is statistically significant evidence that the mean bolt length differs from 10 cm.</p> </li> <li> <p>A one-tailed test checks for an effect in one specific direction (\\(H_1\\): \\(\\mu &gt; 10\\) or \\(\\mu &lt; 10\\)). The entire \\(\\alpha\\) goes into one tail, making it easier to reject \\(H_0\\) in that direction but impossible to detect an effect in the opposite direction.</p> </li> <li> <p>A two-tailed test checks for any difference (\\(H_1\\): \\(\\mu \\neq 10\\)). The \\(\\alpha\\) is split between both tails (\\(\\alpha/2\\) each). This is more conservative but catches effects in either direction.</p> </li> <li> <p>Even with a good procedure, mistakes happen. There are exactly two types of errors:</p> </li> </ul> <p></p> <ul> <li> <p>Type I Error (false positive): you reject \\(H_0\\) when it is actually true. The probability of this is \\(\\alpha\\), which you control by choosing your significance level. Like a fire alarm going off when there is no fire.</p> </li> <li> <p>Type II Error (false negative): you fail to reject \\(H_0\\) when it is actually false. The probability of this is \\(\\beta\\). Like a fire alarm staying silent during a real fire.</p> </li> <li> <p>Power is \\(1 - \\beta\\), the probability of correctly rejecting a false \\(H_0\\). Higher power means you are better at detecting real effects. Power increases when:</p> <ul> <li>The true effect size is larger (bigger differences are easier to detect)</li> <li>The sample size is larger (more data = more precision)</li> <li>The significance level \\(\\alpha\\) is larger (but this raises Type I error risk)</li> <li>The variability is lower (less noise)</li> </ul> </li> <li> <p>There is a tension between Type I and Type II errors. Lowering \\(\\alpha\\) (being more cautious about false positives) increases \\(\\beta\\) (more false negatives). You cannot minimise both simultaneously with a fixed sample size.</p> </li> <li> <p>Parametric tests assume the data follows a specific distribution (usually normal). They are more powerful when the assumptions hold.</p> </li> <li> <p>Z-test: compares a sample mean to a known value when \\(\\sigma\\) is known and \\(n\\) is large (\\(n \\ge 30\\)). Test statistic:</p> </li> </ul> \\[z = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}}\\] <ul> <li>T-test: like the z-test, but for when \\(\\sigma\\) is unknown (estimated from the sample) or \\(n\\) is small. Uses the t-distribution, which has heavier tails than the normal. The heavier tails account for the extra uncertainty from estimating \\(\\sigma\\).</li> </ul> \\[t = \\frac{\\bar{x} - \\mu_0}{s / \\sqrt{n}}\\] <ul> <li> <p>The t-distribution has a parameter called degrees of freedom (\\(df = n - 1\\)). As \\(df\\) increases, the t-distribution approaches the normal distribution.</p> </li> <li> <p>There are several flavours of t-test:</p> <ul> <li>One-sample t-test: is the sample mean different from a specific value?</li> <li>Independent two-sample t-test: are the means of two separate groups different?</li> <li>Paired t-test: are the means of two related measurements different (e.g. before and after treatment on the same subjects)?</li> </ul> </li> <li> <p>ANOVA (Analysis of Variance): tests whether three or more group means are equal. Instead of running multiple t-tests (which inflates the Type I error rate), ANOVA does a single test by comparing the variance between groups to the variance within groups.</p> </li> </ul> \\[F = \\frac{\\text{variance between groups}}{\\text{variance within groups}}\\] <ul> <li> <p>A large \\(F\\) ratio means the groups differ more than you would expect from random variation alone.</p> </li> <li> <p>Non-parametric tests make fewer assumptions about the data distribution. They work on ranks rather than raw values, making them robust to outliers and non-normality.</p> </li> <li> <p>Chi-square test (\\(\\chi^2\\)): tests whether observed frequencies match expected frequencies. Used for categorical data. For example: do the proportions of red, blue, and green cars match the manufacturer's claimed proportions?</p> </li> </ul> \\[\\chi^2 = \\sum \\frac{(O_i - E_i)^2}{E_i}\\] <ul> <li> <p>Mann-Whitney U test: the non-parametric alternative to the independent two-sample t-test. It tests whether one group tends to have larger values than the other by comparing ranks.</p> </li> <li> <p>Wilcoxon signed-rank test: the non-parametric alternative to the paired t-test. Compares paired observations by looking at the magnitude and direction of differences.</p> </li> <li> <p>Kruskal-Wallis test: the non-parametric alternative to one-way ANOVA. Tests whether multiple groups come from the same distribution by comparing ranks across all groups.</p> </li> <li> <p>Goodness-of-fit tests check whether your data follows a specific theoretical distribution. The chi-square goodness-of-fit test compares observed bin counts to expected counts under the hypothesised distribution.</p> </li> <li> <p>Normality tests specifically check whether data is normally distributed. Common ones include the Shapiro-Wilk test (powerful for small samples) and the Kolmogorov-Smirnov test (compares the sample CDF to the theoretical CDF).</p> </li> <li> <p>In ML, hypothesis testing appears when you compare model performance. If model A achieves 92% accuracy and model B achieves 91%, is the difference real or just noise? A paired t-test on cross-validation scores can answer this.</p> </li> </ul>"},{"location":"chapter%2004%3A%20statistics/04.%20hypothesis%20testing/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Perform a z-test for the bolt factory example from the text. Compute the test statistic, p-value, and make a decision. <pre><code>import jax.numpy as jnp\n\nx_bar = 10.3    # sample mean\nmu_0 = 10.0     # null hypothesis value\nsigma = 0.9     # known population std\nn = 36           # sample size\nalpha = 0.05\n\n# Test statistic\nz = (x_bar - mu_0) / (sigma / jnp.sqrt(n))\nprint(f\"z = {z:.4f}\")\n\n# p-value (two-tailed) using the normal CDF approximation\n# For |z| = 2.0, p \u2248 0.0456\nfrom jax.scipy.stats import norm\np_value = 2 * (1 - norm.cdf(jnp.abs(z)))\nprint(f\"p-value = {p_value:.4f}\")\nprint(f\"Reject H\u2080? {p_value &lt;= alpha}\")\n</code></pre></p> </li> <li> <p>Simulate Type I error: when \\(H_0\\) is true, how often do we mistakenly reject it? Run 10,000 experiments and check that the rejection rate matches \\(\\alpha\\). <pre><code>import jax\nimport jax.numpy as jnp\n\nkey = jax.random.PRNGKey(0)\nmu_0 = 50.0\nsigma = 10.0\nn = 30\nalpha = 0.05\nn_experiments = 10_000\n\nrejections = 0\nfor i in range(n_experiments):\n    key, subkey = jax.random.split(key)\n    sample = mu_0 + sigma * jax.random.normal(subkey, shape=(n,))\n    z = (sample.mean() - mu_0) / (sigma / jnp.sqrt(n))\n    p_value = 2 * (1 - __import__(\"jax\").scipy.stats.norm.cdf(jnp.abs(z)))\n    if p_value &lt;= alpha:\n        rejections += 1\n\nprint(f\"Rejection rate: {rejections/n_experiments:.4f}\")\nprint(f\"Expected (\u03b1):   {alpha}\")\n</code></pre></p> </li> <li> <p>Compare a t-test and a Mann-Whitney U test on two groups. Generate data where one group has a slightly higher mean and see which test detects the difference. <pre><code>import jax\nimport jax.numpy as jnp\n\nkey = jax.random.PRNGKey(99)\nk1, k2 = jax.random.split(key)\n\ngroup_a = jax.random.normal(k1, shape=(25,)) * 5 + 100\ngroup_b = jax.random.normal(k2, shape=(25,)) * 5 + 103  # slightly higher mean\n\n# Two-sample t-test (equal variance assumed)\nn_a, n_b = len(group_a), len(group_b)\nmean_a, mean_b = group_a.mean(), group_b.mean()\npooled_var = ((n_a - 1) * group_a.var() + (n_b - 1) * group_b.var()) / (n_a + n_b - 2)\nse = jnp.sqrt(pooled_var * (1/n_a + 1/n_b))\nt_stat = (mean_a - mean_b) / se\nprint(f\"T-test statistic: {t_stat:.4f}\")\n\n# Mann-Whitney: count how often group_a values beat group_b values\nu_stat = jnp.sum(group_a[:, None] &lt; group_b[None, :])\nprint(f\"Mann-Whitney U:   {u_stat}\")\nprint(f\"\\nGroup A mean: {mean_a:.2f}, Group B mean: {mean_b:.2f}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2004%3A%20statistics/05.%20inference/","title":"Statistical Inference","text":"<p>Statistical inference goes beyond yes/no decisions to estimate population parameters with quantified uncertainty. This file covers confidence intervals, point and interval estimation, maximum likelihood estimation, the method of moments, and regression analysis -- the bridge between raw data and predictive models in ML.</p> <ul> <li> <p>Hypothesis testing gives you a yes/no decision: reject or fail to reject. But often you want something more informative, a range of plausible values for the parameter you are estimating. That is what confidence intervals provide.</p> </li> <li> <p>A point estimate is a single number computed from your sample, like the sample mean \\(\\bar{x}\\). It is your best guess for the population parameter, but on its own it gives no sense of how precise the estimate is.</p> </li> <li> <p>A confidence interval wraps that point estimate with a range that reflects uncertainty. It takes the form:</p> </li> </ul> \\[\\text{CI} = \\bar{x} \\pm \\text{ME}\\] <ul> <li>The margin of error (ME) depends on three things: how confident you want to be, how much variability is in the data, and how large your sample is:</li> </ul> \\[\\text{ME} = z^\\ast \\cdot \\frac{\\sigma}{\\sqrt{n}}\\] <ul> <li>Here \\(z^\\ast\\) is the critical value from the normal distribution that matches your desired confidence level. For 95% confidence, \\(z^\\ast = 1.96\\). For 99% confidence, \\(z^\\ast = 2.576\\).</li> </ul> <p></p> <ul> <li> <p>A 95% confidence interval means: if you repeated the experiment many times and built an interval each time, about 95% of those intervals would contain the true population parameter. It does not mean there is a 95% probability the parameter is in this specific interval. The parameter is fixed; the intervals are what vary.</p> </li> <li> <p>Worked example: You measure the heights of 50 people and find \\(\\bar{x} = 170\\) cm with \\(\\sigma = 8\\) cm. Construct a 95% confidence interval.</p> </li> </ul> \\[\\text{ME} = 1.96 \\cdot \\frac{8}{\\sqrt{50}} = 1.96 \\cdot 1.131 = 2.22 \\text{ cm}\\] \\[\\text{CI} = [170 - 2.22, \\; 170 + 2.22] = [167.78, \\; 172.22]\\] <ul> <li> <p>You can say with 95% confidence that the true mean height lies between 167.78 and 172.22 cm.</p> </li> <li> <p>When \\(\\sigma\\) is unknown (the usual case), use the sample standard deviation \\(s\\) and the t-distribution instead:</p> </li> </ul> \\[\\text{CI} = \\bar{x} \\pm t^\\ast_{n-1} \\cdot \\frac{s}{\\sqrt{n}}\\] <ul> <li> <p>Wider intervals are more confident but less precise. Narrower intervals are more precise but less confident. You can narrow an interval without losing confidence by increasing the sample size.</p> </li> <li> <p>Power analysis helps you plan an experiment before you run it. The question is: how large a sample do I need to detect an effect of a given size with a specified power?</p> </li> <li> <p>Recall from the previous file that power = \\(1 - \\beta\\), the probability of correctly rejecting a false \\(H_0\\). A common target is 80% power.</p> </li> <li> <p>The required sample size for a z-test detecting a difference \\(\\delta\\) with significance \\(\\alpha\\) and power \\(1-\\beta\\) is:</p> </li> </ul> \\[n = \\left(\\frac{(z_{\\alpha/2} + z_{\\beta}) \\cdot \\sigma}{\\delta}\\right)^2\\] <ul> <li>For example, to detect a 2 cm difference in mean height (\\(\\sigma = 8\\)) with \\(\\alpha = 0.05\\) and 80% power (\\(z_{0.025} = 1.96\\), \\(z_{0.20} = 0.84\\)):</li> </ul> \\[n = \\left(\\frac{(1.96 + 0.84) \\cdot 8}{2}\\right)^2 = \\left(\\frac{22.4}{2}\\right)^2 = 11.2^2 \\approx 126\\] <ul> <li> <p>You would need about 126 people per group.</p> </li> <li> <p>Power analysis prevents two common mistakes: running an experiment too small to detect a real effect (underpowered), or wasting resources on an experiment far larger than necessary (overpowered).</p> </li> <li> <p>Monte Carlo methods use random sampling to solve problems that are difficult or impossible to solve analytically. The core idea: if you cannot compute something exactly, simulate it many times and use the results as an approximation.</p> </li> <li> <p>The name comes from the Monte Carlo casino, a nod to the role of randomness. These methods are workhorses in ML for tasks like estimating integrals, evaluating model uncertainty, and approximating complex distributions.</p> </li> <li> <p>The general Monte Carlo recipe:</p> <ul> <li>Define a domain of possible inputs</li> <li>Generate random inputs from that domain</li> <li>Evaluate a function on each input</li> <li>Aggregate the results (average, count, etc.)</li> </ul> </li> <li> <p>A classic example is estimating \\(\\pi\\). Imagine a square with side length 2, centred at the origin, with a circle of radius 1 inscribed inside it. The area of the square is 4, and the area of the circle is \\(\\pi\\).</p> </li> </ul> <p></p> <ul> <li>Drop random points uniformly in the square. The fraction that land inside the circle approximates \\(\\pi/4\\):</li> </ul> \\[\\pi \\approx 4 \\times \\frac{\\text{points inside circle}}{\\text{total points}}\\] <ul> <li> <p>A point \\((x, y)\\) is inside the circle if \\(x^2 + y^2 \\le 1\\). The more points you throw, the closer your estimate gets to the true value of \\(\\pi\\).</p> </li> <li> <p>In ML, Monte Carlo methods appear in:</p> <ul> <li>Monte Carlo dropout: run inference multiple times with dropout enabled to estimate prediction uncertainty</li> <li>MCMC (Markov Chain Monte Carlo): sample from complex posterior distributions in Bayesian models</li> <li>Policy gradient methods: estimate gradients in reinforcement learning by sampling trajectories</li> </ul> </li> <li> <p>Factor analysis is a technique for discovering hidden (latent) variables that explain the correlations among observed variables. If 10 personality survey questions can be explained by 3 underlying traits (extraversion, agreeableness, conscientiousness), factor analysis finds those traits.</p> </li> <li> <p>The model assumes each observed variable \\(x_i\\) is a linear combination of a few latent factors \\(f_j\\) plus noise:</p> </li> </ul> \\[x_i = \\lambda_{i1} f_1 + \\lambda_{i2} f_2 + \\ldots + \\lambda_{ik} f_k + \\epsilon_i\\] <ul> <li> <p>The \\(\\lambda\\) values are called factor loadings and tell you how strongly each observed variable relates to each factor. This connects directly to the matrix decompositions from Chapter 2; factor analysis is closely related to eigenvalue decomposition and SVD.</p> </li> <li> <p>Experimental design is the art of structuring an experiment so that you can draw valid conclusions. Poor design can make even a large dataset useless.</p> </li> <li> <p>Key components of a well-designed experiment:</p> <ul> <li>Independent variable (IV): what you manipulate (e.g. drug dose, model architecture)</li> <li>Dependent variable (DV): what you measure (e.g. recovery time, accuracy)</li> <li>Control group: receives no treatment (or a placebo), providing a baseline for comparison</li> <li>Random assignment: participants are assigned to groups randomly, which balances out confounding variables you did not measure</li> </ul> </li> <li> <p>Common experimental designs:</p> <ul> <li>Completely randomised design: subjects are randomly assigned to treatment groups. Simple and effective when groups are comparable.</li> <li>Randomised block design: subjects are first grouped into blocks (e.g. by age), then randomly assigned to treatments within each block. This reduces variability from the blocking factor, similar in spirit to stratified sampling.</li> <li>Factorial design: tests multiple IVs simultaneously. A \\(2 \\times 3\\) factorial design has 2 levels of one variable and 3 of another, giving 6 treatment combinations. This lets you detect interactions, where the effect of one variable depends on the level of another.</li> <li>Crossover design: each subject receives all treatments in sequence (with washout periods in between). Every subject serves as their own control, reducing the effect of individual differences.</li> </ul> </li> <li> <p>In ML experiments, these principles are critical. When comparing models, you should control for random seed, dataset split, and hardware. Cross-validation is a form of crossover design. Ablation studies, where you remove one component at a time, follow the logic of factorial designs.</p> </li> </ul>"},{"location":"chapter%2004%3A%20statistics/05.%20inference/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Construct a 95% confidence interval for the height example, then experiment with different confidence levels and sample sizes. <pre><code>import jax.numpy as jnp\n\nx_bar = 170.0    # sample mean\nsigma = 8.0      # population std (known)\nn = 50           # sample size\n\n# Critical values for common confidence levels\nz_stars = {0.90: 1.645, 0.95: 1.960, 0.99: 2.576}\n\nfor conf, z_star in z_stars.items():\n    me = z_star * (sigma / jnp.sqrt(n))\n    lower, upper = x_bar - me, x_bar + me\n    print(f\"{conf*100:.0f}% CI: [{lower:.2f}, {upper:.2f}]  (ME = {me:.2f})\")\n</code></pre></p> </li> <li> <p>Estimate \\(\\pi\\) using Monte Carlo simulation. Plot how the estimate converges as you increase the number of points. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nkey = jax.random.PRNGKey(42)\n\n# Generate random points in [-1, 1] x [-1, 1]\nn_points = 100_000\nk1, k2 = jax.random.split(key)\nx = jax.random.uniform(k1, shape=(n_points,), minval=-1, maxval=1)\ny = jax.random.uniform(k2, shape=(n_points,), minval=-1, maxval=1)\n\n# Check which points are inside the unit circle\ninside = (x**2 + y**2) &lt;= 1.0\ncumulative_inside = jnp.cumsum(inside)\ncounts = jnp.arange(1, n_points + 1)\npi_estimates = 4.0 * cumulative_inside / counts\n\nplt.figure(figsize=(10, 4))\nplt.plot(pi_estimates, color=\"#3498db\", alpha=0.7, linewidth=0.5)\nplt.axhline(y=jnp.pi, color=\"#e74c3c\", linestyle=\"--\", label=f\"\u03c0 = {jnp.pi:.6f}\")\nplt.xlabel(\"Number of points\")\nplt.ylabel(\"Estimate of \u03c0\")\nplt.title(\"Monte Carlo estimation of \u03c0\")\nplt.legend()\nplt.ylim(2.8, 3.5)\nplt.show()\n\nprint(f\"Final estimate: {pi_estimates[-1]:.6f}\")\nprint(f\"True value:     {jnp.pi:.6f}\")\nprint(f\"Error:          {abs(pi_estimates[-1] - jnp.pi):.6f}\")\n</code></pre></p> </li> <li> <p>Perform a simple power analysis: for a given effect size and standard deviation, compute the required sample size and verify it by simulation. <pre><code>import jax\nimport jax.numpy as jnp\n\n# Parameters\ndelta = 2.0      # effect size (difference in means)\nsigma = 8.0      # population std\nalpha = 0.05\npower_target = 0.80\n\n# Analytical sample size\nz_alpha = 1.96   # two-tailed, alpha=0.05\nz_beta = 0.84    # power=0.80\nn_required = ((z_alpha + z_beta) * sigma / delta) ** 2\nprint(f\"Required n per group: {n_required:.0f}\")\n\n# Verify by simulation\nkey = jax.random.PRNGKey(7)\nn = int(jnp.ceil(n_required))\nn_sims = 5000\nrejections = 0\n\nfor _ in range(n_sims):\n    key, k1, k2 = jax.random.split(key, 3)\n    group_a = jax.random.normal(k1, shape=(n,)) * sigma + 50\n    group_b = jax.random.normal(k2, shape=(n,)) * sigma + 50 + delta\n    pooled_se = jnp.sqrt(2 * sigma**2 / n)\n    z = (group_b.mean() - group_a.mean()) / pooled_se\n    p = 2 * (1 - __import__(\"jax\").scipy.stats.norm.cdf(jnp.abs(z)))\n    if p &lt;= alpha:\n        rejections += 1\n\nprint(f\"Simulated power: {rejections/n_sims:.3f}\")\nprint(f\"Target power:    {power_target:.3f}\")\n</code></pre></p> </li> <li> <p>Visualise how confidence interval width changes with sample size. This shows why collecting more data gives more precise estimates. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nsigma = 8.0\nz_star = 1.96  # 95% confidence\n\nsample_sizes = jnp.array([10, 20, 30, 50, 100, 200, 500, 1000], dtype=jnp.float32)\nmargins = z_star * sigma / jnp.sqrt(sample_sizes)\n\nplt.figure(figsize=(8, 4))\nplt.bar([str(int(n)) for n in sample_sizes], margins, color=\"#3498db\", alpha=0.7)\nplt.xlabel(\"Sample size\")\nplt.ylabel(\"Margin of error (cm)\")\nplt.title(\"95% CI margin of error shrinks with larger samples\")\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2005%3A%20probability/01.%20counting/","title":"Counting","text":"<p>Counting is the prerequisite for computing probabilities -- you must know how many outcomes exist before you can assign likelihoods. This file covers the multiplication and addition rules, factorials, permutations, combinations, and the binomial coefficient -- combinatorial tools that underpin sampling, hashing, and probabilistic analysis in ML.</p> <ul> <li> <p>Before we can compute probabilities, we need to count outcomes. If you want to know the chance of drawing a winning hand in poker, you first need to know how many possible hands exist and how many of those are winners. Counting is the machinery that makes probability precise.</p> </li> <li> <p>The simplest counting principle is the multiplication rule. If one decision has \\(m\\) options and a second independent decision has \\(n\\) options, the total number of combined outcomes is \\(m \\times n\\).</p> </li> <li> <p>Picture getting dressed in the morning. You have 3 shirts and 4 pants. Each shirt can pair with every pant, giving you \\(3 \\times 4 = 12\\) possible outfits.</p> </li> </ul> <p></p> <ul> <li> <p>The multiplication rule extends to any number of choices. If you also have 2 pairs of shoes, the total outfits become \\(3 \\times 4 \\times 2 = 24\\). Each new independent choice multiplies the count.</p> </li> <li> <p>The addition rule handles \"or\" scenarios. If event A can happen in \\(m\\) ways and event B can happen in \\(n\\) ways, and they cannot both happen at the same time (mutually exclusive), the total number of ways is \\(m + n\\).</p> </li> <li> <p>Suppose you can travel from city X to city Y by car (3 routes) or by train (2 routes). You cannot take both simultaneously, so the total options are \\(3 + 2 = 5\\).</p> </li> <li> <p>When events overlap, you need to subtract the double-counted outcomes. If \\(A\\) and \\(B\\) can co-occur, the count is \\(|A \\cup B| = |A| + |B| - |A \\cap B|\\). This is the inclusion-exclusion principle, and it will reappear when we discuss probability addition rules.</p> </li> <li> <p>The factorial of a non-negative integer \\(n\\) is the product of all positive integers up to \\(n\\):</p> </li> </ul> \\[n! = n \\times (n-1) \\times (n-2) \\times \\cdots \\times 2 \\times 1\\] <ul> <li> <p>Think of the factorial as answering: in how many ways can you arrange \\(n\\) distinct objects in a line? Three books on a shelf can be arranged in \\(3! = 3 \\times 2 \\times 1 = 6\\) ways. By convention, \\(0! = 1\\).</p> </li> <li> <p>Factorials grow extremely fast. \\(10! = 3{,}628{,}800\\) and \\(20!\\) is already over \\(2.4 \\times 10^{18}\\). This explosive growth is why brute-force search becomes impractical in combinatorial problems.</p> </li> <li> <p>A permutation is an ordered arrangement of objects. When you pick \\(r\\) items from \\(n\\) distinct objects and the order matters, the number of permutations is:</p> </li> </ul> \\[P(n, r) = \\frac{n!}{(n - r)!}\\] <ul> <li> <p>Imagine picking a president, vice president, and treasurer from a club of 10 people. The first role has 10 candidates, the second has 9 remaining, the third has 8. That gives \\(P(10, 3) = 10 \\times 9 \\times 8 = 720\\). The formula confirms this: \\(\\frac{10!}{7!} = 720\\).</p> </li> <li> <p>A combination is an unordered selection. When you pick \\(r\\) items from \\(n\\) and the order does not matter, we divide out the redundant orderings:</p> </li> </ul> \\[C(n, r) = \\binom{n}{r} = \\frac{n!}{r!(n - r)!}\\] <ul> <li>The notation \\(\\binom{n}{r}\\) is read \"n choose r.\" The key insight is that every combination corresponds to \\(r!\\) permutations (the \\(r\\) chosen items can be rearranged in \\(r!\\) ways), so we divide the permutation count by \\(r!\\).</li> </ul> <p></p> <ul> <li>Example: from a group of 10 people, how many ways can you form a committee of 3? Order does not matter (there is no president or vice president, just members), so we use combinations:</li> </ul> \\[\\binom{10}{3} = \\frac{10!}{3! \\cdot 7!} = \\frac{10 \\times 9 \\times 8}{3 \\times 2 \\times 1} = 120\\] <ul> <li> <p>The same 10 people produce 720 permutations but only 120 combinations, because each group of 3 has \\(3! = 6\\) internal orderings.</p> </li> <li> <p>Combinations are central to probability. The binomial coefficient \\(\\binom{n}{r}\\) counts the number of ways to get exactly \\(r\\) successes in \\(n\\) trials, which is the heart of the binomial distribution (covered in file 03).</p> </li> <li> <p>Let us work through a classic committee problem that combines multiple counting ideas.</p> </li> <li> <p>Problem: A club has 8 men and 6 women. How many ways can you form a committee of 5 that includes exactly 3 men and 2 women?</p> </li> <li> <p>Step 1: Choose 3 men from 8.</p> </li> </ul> \\[\\binom{8}{3} = \\frac{8!}{3! \\cdot 5!} = \\frac{8 \\times 7 \\times 6}{3 \\times 2 \\times 1} = 56\\] <ul> <li>Step 2: Choose 2 women from 6.</li> </ul> \\[\\binom{6}{2} = \\frac{6!}{2! \\cdot 4!} = \\frac{6 \\times 5}{2 \\times 1} = 15\\] <ul> <li>Step 3: Apply the multiplication rule. Each selection of men can pair with each selection of women:</li> </ul> \\[56 \\times 15 = 840 \\text{ committees}\\] <ul> <li> <p>This pattern, breaking a complex counting problem into independent sub-choices and multiplying, is the standard approach in combinatorics.</p> </li> <li> <p>There are also permutations with repetition. When items can repeat, choosing \\(r\\) items from \\(n\\) types gives \\(n^r\\) outcomes. A 4-digit PIN using digits 0-9 has \\(10^4 = 10{,}000\\) possibilities. Each position has 10 options, and the multiplication rule handles the rest.</p> </li> <li> <p>Combinations with repetition (also called \"stars and bars\") count how many ways to choose \\(r\\) items from \\(n\\) types when repeats are allowed and order does not matter:</p> </li> </ul> \\[\\binom{n + r - 1}{r} = \\frac{(n + r - 1)!}{r!(n - 1)!}\\] <ul> <li> <p>Example: choosing 3 scoops from 4 ice cream flavours (repeats allowed) gives \\(\\binom{4 + 3 - 1}{3} = \\binom{6}{3} = 20\\) options.</p> </li> <li> <p>To summarise the counting toolkit:</p> </li> </ul> Scenario Formula Ordered, no repetition (permutation) \\(P(n,r) = \\frac{n!}{(n-r)!}\\) Unordered, no repetition (combination) \\(\\binom{n}{r} = \\frac{n!}{r!(n-r)!}\\) Ordered, with repetition \\(n^r\\) Unordered, with repetition \\(\\binom{n+r-1}{r}\\) <ul> <li>Every probability calculation involving equally likely outcomes uses the formula \\(P(\\text{event}) = \\frac{\\text{favourable outcomes}}{\\text{total outcomes}}\\). Counting gives us both numbers. With this foundation, we are ready to formalise probability itself in the next file.</li> </ul>"},{"location":"chapter%2005%3A%20probability/01.%20counting/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Compute \\(P(10, 3)\\) and \\(\\binom{10}{3}\\) using both the factorial formula and direct computation. Verify that the permutation count is always \\(r!\\) times the combination count. <pre><code>import jax.numpy as jnp\nfrom math import factorial\n\nn, r = 10, 3\n\nperm = factorial(n) // factorial(n - r)\ncomb = factorial(n) // (factorial(r) * factorial(n - r))\n\nprint(f\"P({n},{r}) = {perm}\")\nprint(f\"C({n},{r}) = {comb}\")\nprint(f\"P / C = {perm // comb} (should equal {r}! = {factorial(r)})\")\n</code></pre></p> </li> <li> <p>Solve the committee problem (3 men from 8, 2 women from 6) programmatically and verify by enumerating all valid committees. <pre><code>from itertools import combinations\nfrom math import factorial\n\ndef comb_count(n, r):\n    return factorial(n) // (factorial(r) * factorial(n - r))\n\n# Formula approach\nmen_ways = comb_count(8, 3)\nwomen_ways = comb_count(6, 2)\nprint(f\"Formula: {men_ways} \u00d7 {women_ways} = {men_ways * women_ways}\")\n\n# Enumeration approach\nmen = [f\"M{i}\" for i in range(1, 9)]\nwomen = [f\"W{i}\" for i in range(1, 7)]\ncount = sum(1 for _ in combinations(men, 3) for _ in combinations(women, 2))\nprint(f\"Enumeration: {count}\")\n</code></pre></p> </li> <li> <p>Count how many 4-character passwords can be made from 26 lowercase letters (with repetition allowed). Then count how many contain no repeated letters. <pre><code>from math import factorial\n\nn = 26\nr = 4\n\nwith_rep = n ** r\nwithout_rep = factorial(n) // factorial(n - r)\n\nprint(f\"With repetition:    {with_rep:&gt;10,}\")\nprint(f\"Without repetition: {without_rep:&gt;10,}\")\nprint(f\"Fraction with repeats: {1 - without_rep/with_rep:.2%}\")\n</code></pre></p> </li> <li> <p>Simulate the birthday problem: in a group of \\(k\\) people, what is the probability that at least two share a birthday? Plot the probability for \\(k = 1\\) to \\(60\\) and find where it crosses 50%. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef birthday_prob_exact(k):\n    \"\"\"Probability of at least one shared birthday in group of k.\"\"\"\n    p_no_match = 1.0\n    for i in range(k):\n        p_no_match *= (365 - i) / 365\n    return 1 - p_no_match\n\nks = list(range(1, 61))\nprobs = [birthday_prob_exact(k) for k in ks]\n\nplt.figure(figsize=(8, 4))\nplt.plot(ks, probs, color=\"#3498db\", linewidth=2)\nplt.axhline(y=0.5, color=\"#e74c3c\", linestyle=\"--\", alpha=0.7, label=\"50%\")\ncross = next(k for k, p in zip(ks, probs) if p &gt;= 0.5)\nplt.axvline(x=cross, color=\"#e74c3c\", linestyle=\"--\", alpha=0.7)\nplt.xlabel(\"Group size (k)\")\nplt.ylabel(\"P(at least one shared birthday)\")\nplt.title(f\"Birthday Problem (crosses 50% at k={cross})\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2005%3A%20probability/02.%20probability%20concepts/","title":"Probability Concepts","text":"<p>Probability theory formalises uncertainty and provides the rules for reasoning under it. This file covers sample spaces, events, axioms of probability, conditional probability, independence, Bayes' theorem, and the frequentist vs. Bayesian interpretations -- the mathematical framework behind every generative and discriminative model in ML.</p> <ul> <li> <p>Probability assigns a number between 0 and 1 to an event, measuring how likely it is to happen. </p> </li> <li> <p>A probability of 0 means impossible, 1 means certain, and 0.5 means a coin toss.</p> </li> <li> <p>There are two main interpretations. The frequentist view says probability is the long-run relative frequency: flip a fair coin 10,000 times and heads will appear roughly 50% of the time.</p> </li> <li> <p>The Bayesian view says probability is a degree of belief: you might say there is a 70% chance it rains tomorrow, even though tomorrow only happens once.</p> </li> <li> <p>Both interpretations use the same mathematical rules. The difference is philosophical, but it matters in ML. Frequentist methods give you point estimates. Bayesian methods give you full distributions over parameters.</p> </li> <li> <p>The sample space \\(S\\) is the set of all possible outcomes of an experiment. Flip a coin: \\(S = \\{H, T\\}\\). Roll a die: \\(S = \\{1, 2, 3, 4, 5, 6\\}\\).</p> </li> <li> <p>An event is any subset of the sample space. \"Rolling an even number\" is the event \\(A = \\{2, 4, 6\\}\\), which is a subset of \\(S\\).</p> </li> <li> <p>The probability of an event when all outcomes are equally likely is simply counting (from file 01):</p> </li> </ul> \\[P(A) = \\frac{|A|}{|S|} = \\frac{\\text{favourable outcomes}}{\\text{total outcomes}}\\] <ul> <li>For the even-number example: \\(P(\\text{even}) = \\frac{3}{6} = 0.5\\).</li> </ul> <p></p> <ul> <li>The complement of event \\(A\\), written \\(A'\\) or \\(A^c\\), is everything in \\(S\\) that is not in \\(A\\). Since every outcome is either in \\(A\\) or not:</li> </ul> \\[P(A') = 1 - P(A)\\] <ul> <li> <p>Complements are often the easier route. Instead of counting all the ways to get at least one head in 5 coin flips, count the one way to get no heads and subtract: \\(P(\\text{at least one head}) = 1 - P(\\text{all tails}) = 1 - (0.5)^5 = 0.969\\).</p> </li> <li> <p>Two events are mutually exclusive (disjoint) if they cannot both happen: \\(A \\cap B = \\emptyset\\). Rolling a 2 and rolling a 5 on a single die are mutually exclusive.</p> </li> <li> <p>The addition rule for mutually exclusive events is straightforward:</p> </li> </ul> \\[P(A \\cup B) = P(A) + P(B) \\quad \\text{(if } A \\cap B = \\emptyset\\text{)}\\] <ul> <li>When events can overlap, you need the general addition rule to avoid double-counting the intersection:</li> </ul> \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\] <ul> <li> <p>This mirrors the inclusion-exclusion principle from counting. The Venn diagram above shows why: the purple region (intersection) gets counted once in \\(P(A)\\) and again in \\(P(B)\\), so we subtract it once.</p> </li> <li> <p>Joint probability \\(P(A \\cap B)\\) is the probability that both \\(A\\) and \\(B\\) occur. In a deck of cards, \\(P(\\text{red} \\cap \\text{king}) = \\frac{2}{52}\\) because there are 2 red kings.</p> </li> <li> <p>Marginal probability is the probability of a single event regardless of others. \\(P(\\text{red}) = \\frac{26}{52} = 0.5\\) is a marginal probability. If you have a joint distribution over two variables, the marginal is obtained by summing (or integrating) over the other variable.</p> </li> <li> <p>Conditional probability answers: given that \\(B\\) has already happened, what is the probability of \\(A\\)? We shrink the sample space from \\(S\\) down to \\(B\\), and ask what fraction of \\(B\\) also belongs to \\(A\\):</p> </li> </ul> \\[P(A | B) = \\frac{P(A \\cap B)}{P(B)}, \\quad P(B) &gt; 0\\] <p></p> <ul> <li> <p>Example: you draw a card and someone tells you it is red. What is the probability it is a king? There are 26 red cards and 2 of them are kings, so \\(P(\\text{king} | \\text{red}) = \\frac{2}{26} = \\frac{1}{13}\\). Using the formula: \\(P(\\text{king} \\cap \\text{red}) / P(\\text{red}) = \\frac{2/52}{26/52} = \\frac{1}{13}\\).</p> </li> <li> <p>Two events are independent if knowing one happened tells you nothing about the other. Formally:</p> </li> </ul> \\[P(A \\cap B) = P(A) \\cdot P(B)\\] <ul> <li> <p>Equivalently, \\(P(A | B) = P(A)\\). Flipping two separate coins are independent events. Drawing two cards without replacement are not independent (the first draw changes what remains).</p> </li> <li> <p>Independence is a massive simplifier. For independent events, joint probabilities factor into products, which makes computation tractable. Many ML models assume independence between features (e.g. Naive Bayes) precisely because of this simplification.</p> </li> <li> <p>The multiplication rule for any two events rearranges the conditional probability formula:</p> </li> </ul> \\[P(A \\cap B) = P(A | B) \\cdot P(B) = P(B | A) \\cdot P(A)\\] <ul> <li> <p>For independent events, this simplifies to \\(P(A \\cap B) = P(A) \\cdot P(B)\\) since the conditional equals the marginal.</p> </li> <li> <p>Bayes' theorem is one of the most important results in probability and the foundation of Bayesian ML. It lets you reverse the direction of a conditional probability:</p> </li> </ul> \\[P(A | B) = \\frac{P(B | A) \\cdot P(A)}{P(B)}\\] <ul> <li>The theorem follows directly from writing \\(P(A \\cap B)\\) two ways: \\(P(B|A) \\cdot P(A) = P(A|B) \\cdot P(B)\\), then solving for \\(P(A|B)\\).</li> </ul> <p></p> <ul> <li> <p>Each component has a name:</p> <ul> <li>Prior \\(P(A)\\): your initial belief before seeing evidence</li> <li>Likelihood \\(P(B|A)\\): how probable the evidence is, assuming \\(A\\) is true</li> <li>Evidence \\(P(B)\\): the total probability of seeing the evidence, acts as a normaliser</li> <li>Posterior \\(P(A|B)\\): your updated belief after seeing the evidence</li> </ul> </li> <li> <p>Let us work through the classic medical diagnosis example. Suppose a disease affects 1% of the population. A test for the disease is 95% accurate: it correctly identifies 95% of sick people (sensitivity) and correctly identifies 90% of healthy people (specificity).</p> </li> <li> <p>You test positive. What is the probability you actually have the disease?</p> </li> <li> <p>Let \\(D\\) = having the disease, \\(+\\) = testing positive.</p> <ul> <li>Prior: \\(P(D) = 0.01\\)</li> <li>Likelihood: \\(P(+ | D) = 0.95\\)</li> <li>False positive rate: \\(P(+ | D') = 0.10\\)</li> </ul> </li> <li> <p>We need \\(P(+)\\). By the law of total probability:</p> </li> </ul> \\[P(+) = P(+ | D) \\cdot P(D) + P(+ | D') \\cdot P(D')$$ $$= 0.95 \\times 0.01 + 0.10 \\times 0.99 = 0.0095 + 0.099 = 0.1085\\] <ul> <li>Now apply Bayes' theorem:</li> </ul> \\[P(D | +) = \\frac{P(+ | D) \\cdot P(D)}{P(+)} = \\frac{0.95 \\times 0.01}{0.1085} \\approx 0.088\\] <ul> <li> <p>Despite the test being \"95% accurate,\" a positive result only gives you about an 8.8% chance of having the disease. The prior matters enormously. Because the disease is rare, most positive results are false positives. This is a crucial insight for any classification problem in ML: when classes are imbalanced, accuracy alone is misleading.</p> </li> <li> <p>The law of total probability partitions the sample space into mutually exclusive, exhaustive events \\(B_1, B_2, \\ldots, B_n\\) and expresses any event \\(A\\) as:</p> </li> </ul> \\[P(A) = \\sum_{i=1}^{n} P(A | B_i) \\cdot P(B_i)\\] <ul> <li> <p>This is exactly what we used to compute \\(P(+)\\) in the medical example: we split the population into \"has disease\" and \"does not have disease.\"</p> </li> <li> <p>The chain rule of probability generalises the multiplication rule to any number of events:</p> </li> </ul> \\[P(A_1 \\cap A_2 \\cap \\cdots \\cap A_n) = P(A_1) \\cdot P(A_2 | A_1) \\cdot P(A_3 | A_1 \\cap A_2) \\cdots P(A_n | A_1 \\cap \\cdots \\cap A_{n-1})\\] <ul> <li> <p>Each factor conditions on everything that came before. This is the backbone of autoregressive language models: the probability of a sentence is the product of each word's probability given all previous words.</p> </li> <li> <p>Conditional independence means two events are independent given a third. \\(A\\) and \\(B\\) are conditionally independent given \\(C\\) if:</p> </li> </ul> \\[P(A \\cap B | C) = P(A | C) \\cdot P(B | C)\\] <ul> <li> <p>Events can be marginally dependent but conditionally independent, or vice versa. For example, two students' exam scores may be correlated (both depend on the difficulty of the exam), but given the exam difficulty, their scores are independent.</p> </li> <li> <p>Conditional independence is the key assumption behind graphical models like Bayesian networks. It lets you factorise complex joint distributions into manageable pieces, making inference computationally feasible.</p> </li> </ul>"},{"location":"chapter%2005%3A%20probability/02.%20probability%20concepts/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Simulate the medical diagnosis problem. Generate a population of 100,000 people, apply the disease prevalence and test accuracy, and verify that Bayes' theorem gives the correct posterior. <pre><code>import jax\nimport jax.numpy as jnp\n\nkey = jax.random.PRNGKey(42)\nn = 100_000\n\n# Generate population\nk1, k2 = jax.random.split(key)\nhas_disease = jax.random.bernoulli(k1, p=0.01, shape=(n,))\n\n# Generate test results\nk3, k4 = jax.random.split(k2)\n# Sensitivity: P(+|D) = 0.95, Specificity: P(-|D') = 0.90\ntest_positive = jnp.where(\n    has_disease,\n    jax.random.bernoulli(k3, p=0.95, shape=(n,)),\n    jax.random.bernoulli(k4, p=0.10, shape=(n,))\n)\n\n# Among those who tested positive, what fraction actually has the disease?\npositives = test_positive.astype(bool)\ntrue_positives = (has_disease &amp; positives).sum()\ntotal_positives = positives.sum()\n\nprint(f\"Total positive tests: {total_positives}\")\nprint(f\"True positives: {true_positives}\")\nprint(f\"P(Disease | Positive) = {true_positives / total_positives:.4f}\")\nprint(f\"Bayes' formula:         {0.95 * 0.01 / 0.1085:.4f}\")\n</code></pre></p> </li> <li> <p>Verify the addition rule by simulation. Generate random events A and B with known probabilities and overlap, then check that \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\). <pre><code>import jax\nimport jax.numpy as jnp\n\nkey = jax.random.PRNGKey(0)\nn = 200_000\nk1, k2 = jax.random.split(key)\n\n# Events: A = value &lt; 0.4, B = value &lt; 0.6 (overlap at &lt; 0.4)\nvals_a = jax.random.uniform(k1, shape=(n,))\nvals_b = jax.random.uniform(k2, shape=(n,))\n\nA = vals_a &lt; 0.4\nB = vals_b &lt; 0.6\n\np_a = A.mean()\np_b = B.mean()\np_a_and_b = (A &amp; B).mean()\np_a_or_b = (A | B).mean()\n\nprint(f\"P(A) = {p_a:.4f}\")\nprint(f\"P(B) = {p_b:.4f}\")\nprint(f\"P(A \u2229 B) = {p_a_and_b:.4f}\")\nprint(f\"P(A \u222a B) simulated = {p_a_or_b:.4f}\")\nprint(f\"P(A) + P(B) - P(A\u2229B) = {p_a + p_b - p_a_and_b:.4f}\")\n</code></pre></p> </li> <li> <p>Demonstrate that conditional probability changes with evidence. Simulate rolling two dice and compute \\(P(\\text{sum} = 7)\\), then \\(P(\\text{sum} = 7 | \\text{first die} = 3)\\). <pre><code>import jax\nimport jax.numpy as jnp\n\nkey = jax.random.PRNGKey(1)\nn = 500_000\nk1, k2 = jax.random.split(key)\n\nd1 = jax.random.randint(k1, shape=(n,), minval=1, maxval=7)\nd2 = jax.random.randint(k2, shape=(n,), minval=1, maxval=7)\ntotal = d1 + d2\n\n# Unconditional\np_sum7 = (total == 7).mean()\nprint(f\"P(sum=7) = {p_sum7:.4f} (exact: {6/36:.4f})\")\n\n# Conditional on first die = 3\nmask = d1 == 3\np_sum7_given_d1_3 = (total[mask] == 7).mean()\nprint(f\"P(sum=7 | d1=3) = {p_sum7_given_d1_3:.4f} (exact: {1/6:.4f})\")\n</code></pre></p> </li> <li> <p>Implement Bayes' theorem as a function and use it to update beliefs iteratively. Start with a uniform prior over a coin's bias and update after observing each flip. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef bayes_update(prior, likelihood):\n    \"\"\"Multiply prior by likelihood and normalise.\"\"\"\n    posterior = prior * likelihood\n    return posterior / posterior.sum()\n\n# Discretise possible bias values\ntheta = jnp.linspace(0, 1, 200)\nprior = jnp.ones_like(theta)  # uniform prior\nprior = prior / prior.sum()\n\n# Observed flips: 1=heads, 0=tails\nflips = [1, 1, 0, 1, 1, 1, 0, 1, 0, 1]\n\nplt.figure(figsize=(10, 5))\nplt.plot(theta, prior, \"--\", color=\"#999\", label=\"prior\")\n\nfor i, flip in enumerate(flips):\n    likelihood = theta if flip == 1 else (1 - theta)\n    prior = bayes_update(prior, likelihood)\n    if i in [0, 2, 4, 9]:\n        plt.plot(theta, prior, label=f\"after {i+1} flips\", linewidth=2)\n\nplt.xlabel(\"Coin bias \u03b8\")\nplt.ylabel(\"Belief (normalised)\")\nplt.title(\"Bayesian updating: belief about coin bias\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2005%3A%20probability/03.%20distributions/","title":"Probability Distributions","text":"<p>Probability distributions describe how random outcomes are spread across possible values. This file catalogues the key discrete and continuous distributions -- Bernoulli, binomial, Poisson, Gaussian, exponential, beta, and more -- giving the formulas, intuitions, and ML applications (loss functions, priors, noise models) for each.</p> <ul> <li> <p>In Chapter 4 we introduced random variables, PMFs, PDFs, and CDFs. Here we catalogue the most important probability distributions you will encounter in ML and statistics, giving the intuition, formula, mean, and variance for each.</p> </li> <li> <p>Quick recap of the three core functions (see Chapter 4 for full definitions):</p> <ul> <li>PMF \\(P(X = x)\\): gives the probability of each discrete outcome. The bars in a bar chart.</li> <li>PDF \\(f(x)\\): gives the density at each point for continuous variables. The area under the curve between two points is the probability.</li> <li>CDF \\(F(x) = P(X \\le x)\\): the cumulative probability up to \\(x\\). Always goes from 0 to 1 and never decreases.</li> </ul> </li> <li> <p>The support of a distribution is the set of values where the PMF or PDF is positive. For a die roll, the support is \\(\\{1,2,3,4,5,6\\}\\). For the normal distribution, the support is all real numbers \\((-\\infty, \\infty)\\).</p> </li> <li> <p>Distributions divide cleanly into two families: discrete (countable outcomes, use PMFs) and continuous (uncountable outcomes, use PDFs).</p> </li> <li> <p>Bernoulli distribution: the simplest distribution. A single trial with two outcomes: success (1) with probability \\(p\\) and failure (0) with probability \\(1-p\\).</p> </li> </ul> \\[P(X = x) = p^x (1 - p)^{1-x}, \\quad x \\in \\{0, 1\\}\\] <ul> <li> <p>Mean: \\(E[X] = p\\). Variance: \\(\\text{Var}(X) = p(1-p)\\).</p> </li> <li> <p>Every coin flip, every yes/no classification, every binary outcome is a Bernoulli trial. In ML, the output of a sigmoid function is exactly the \\(p\\) parameter of a Bernoulli distribution.</p> </li> <li> <p>Binomial distribution: count the number of successes in \\(n\\) independent Bernoulli trials, each with the same probability \\(p\\).</p> </li> </ul> \\[P(X = k) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0, 1, \\ldots, n\\] <ul> <li> <p>The binomial coefficient \\(\\binom{n}{k}\\) from file 01 counts how many ways to arrange \\(k\\) successes among \\(n\\) trials.</p> </li> <li> <p>Mean: \\(E[X] = np\\). Variance: \\(\\text{Var}(X) = np(1-p)\\).</p> </li> </ul> <p></p> <ul> <li> <p>Example: flip a biased coin (\\(p = 0.7\\)) eight times. The probability of getting exactly 6 heads is \\(\\binom{8}{6}(0.7)^6(0.3)^2 = 28 \\times 0.1176 \\times 0.09 \\approx 0.296\\).</p> </li> <li> <p>Poisson distribution: counts the number of events in a fixed interval of time or space, given a known average rate \\(\\lambda\\). Useful when events are rare and independent.</p> </li> </ul> \\[P(X = k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k = 0, 1, 2, \\ldots\\] <ul> <li> <p>Mean: \\(E[X] = \\lambda\\). Variance: \\(\\text{Var}(X) = \\lambda\\). The mean equals the variance, which is a signature property.</p> </li> <li> <p>Examples: emails per hour (\\(\\lambda = 5\\)), typos per page, server requests per second. In ML, Poisson regression models count data where a linear model would predict negative counts.</p> </li> <li> <p>As \\(n \\to \\infty\\) and \\(p \\to 0\\) with \\(np = \\lambda\\) held constant, the Binomial\\((n,p)\\) converges to Poisson\\((\\lambda)\\). This is why the Poisson works well for rare events in large populations.</p> </li> <li> <p>Geometric distribution: counts the number of trials until the first success. \"How many coins do I flip before I get my first heads?\"</p> </li> </ul> \\[P(X = k) = (1-p)^{k-1} p, \\quad k = 1, 2, 3, \\ldots\\] <ul> <li> <p>Mean: \\(E[X] = 1/p\\). Variance: \\(\\text{Var}(X) = (1-p)/p^2\\).</p> </li> <li> <p>The geometric distribution is memoryless: the probability of waiting \\(k\\) more trials for success does not depend on how many trials you have already waited. This makes it special among discrete distributions.</p> </li> <li> <p>Negative Binomial distribution: generalises the geometric by counting trials until the \\(r\\)-th success (geometric is the special case \\(r=1\\)).</p> </li> </ul> \\[P(X = k) = \\binom{k-1}{r-1} p^r (1-p)^{k-r}, \\quad k = r, r+1, r+2, \\ldots\\] <ul> <li> <p>Mean: \\(E[X] = r/p\\). Variance: \\(\\text{Var}(X) = r(1-p)/p^2\\).</p> </li> <li> <p>The Negative Binomial is also used in practice to model overdispersed count data (where the variance exceeds the mean), which the Poisson cannot handle.</p> </li> <li> <p>Now we move to continuous distributions.</p> </li> <li> <p>Uniform distribution: all values in an interval \\([a, b]\\) are equally likely. The PDF is a flat rectangle.</p> </li> </ul> \\[f(x) = \\frac{1}{b - a}, \\quad a \\le x \\le b\\] <ul> <li> <p>Mean: \\(E[X] = \\frac{a+b}{2}\\). Variance: \\(\\text{Var}(X) = \\frac{(b-a)^2}{12}\\).</p> </li> <li> <p>Random number generators produce Uniform(0,1) samples as their starting point. Other distributions are generated by transforming these uniform samples.</p> </li> <li> <p>Normal (Gaussian) distribution: the most important distribution in statistics. It arises naturally from the Central Limit Theorem (see Chapter 4): averages of many independent random variables tend toward a normal distribution regardless of the original distribution.</p> </li> </ul> \\[f(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\\] <ul> <li> <p>Mean: \\(E[X] = \\mu\\). Variance: \\(\\text{Var}(X) = \\sigma^2\\).</p> </li> <li> <p>The standard normal has \\(\\mu = 0\\) and \\(\\sigma = 1\\). Any normal variable \\(X\\) can be standardised to a standard normal \\(Z\\) using \\(Z = (X - \\mu)/\\sigma\\).</p> </li> </ul> <p></p> <ul> <li> <p>The empirical rule (68-95-99.7 rule) says:</p> <ul> <li>About 68% of data falls within \\(\\pm 1\\sigma\\) of the mean</li> <li>About 95% falls within \\(\\pm 2\\sigma\\)</li> <li>About 99.7% falls within \\(\\pm 3\\sigma\\)</li> </ul> </li> <li> <p>In ML, normal distributions appear everywhere: weight initialisation, noise in data augmentation, the assumption behind MSE loss (which implicitly assumes Gaussian errors), and the reparameterisation trick in variational autoencoders.</p> </li> <li> <p>Exponential distribution: models the time between events in a Poisson process. If events arrive at rate \\(\\lambda\\), the waiting time between them follows Exponential\\((\\lambda)\\).</p> </li> </ul> \\[f(x) = \\lambda e^{-\\lambda x}, \\quad x \\ge 0\\] <ul> <li> <p>Mean: \\(E[X] = 1/\\lambda\\). Variance: \\(\\text{Var}(X) = 1/\\lambda^2\\).</p> </li> <li> <p>Like the geometric distribution for discrete variables, the exponential is memoryless: \\(P(X &gt; s + t | X &gt; s) = P(X &gt; t)\\). The probability of waiting another \\(t\\) units does not depend on how long you have already waited.</p> </li> <li> <p>Gamma distribution: generalises the exponential. It models the time until the \\(\\alpha\\)-th event in a Poisson process (exponential is \\(\\alpha = 1\\)).</p> </li> </ul> \\[f(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1} e^{-\\beta x}, \\quad x &gt; 0\\] <ul> <li> <p>Here \\(\\alpha\\) (shape) controls the shape and \\(\\beta\\) (rate) controls the scale. \\(\\Gamma(\\alpha)\\) is the gamma function, which extends factorials to real numbers: \\(\\Gamma(n) = (n-1)!\\) for positive integers.</p> </li> <li> <p>Mean: \\(E[X] = \\alpha/\\beta\\). Variance: \\(\\text{Var}(X) = \\alpha/\\beta^2\\).</p> </li> <li> <p>Beta distribution: defined on the interval \\([0, 1]\\), making it perfect for modelling probabilities, proportions, and rates.</p> </li> </ul> \\[f(x) = \\frac{x^{\\alpha - 1}(1 - x)^{\\beta - 1}}{B(\\alpha, \\beta)}, \\quad 0 \\le x \\le 1\\] <ul> <li> <p>The denominator \\(B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) is the beta function, a normalising constant.</p> </li> <li> <p>Mean: \\(E[X] = \\frac{\\alpha}{\\alpha + \\beta}\\). Variance: \\(\\text{Var}(X) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\).</p> </li> <li> <p>The Beta distribution is the conjugate prior for the Bernoulli and Binomial likelihoods. This means if your prior is Beta and your data is Bernoulli, the posterior is also Beta, which makes Bayesian updating analytically tractable. We will use this in file 04.</p> </li> </ul> <p></p> <ul> <li>Chi-squared distribution (\\(\\chi^2\\)): if you take \\(k\\) independent standard normal random variables and sum their squares, the result follows a \\(\\chi^2\\) distribution with \\(k\\) degrees of freedom.</li> </ul> \\[f(x) = \\frac{1}{2^{k/2}\\Gamma(k/2)} x^{k/2 - 1} e^{-x/2}, \\quad x &gt; 0\\] <ul> <li> <p>Mean: \\(E[X] = k\\). Variance: \\(\\text{Var}(X) = 2k\\).</p> </li> <li> <p>The \\(\\chi^2\\) distribution is actually a special case of the Gamma distribution with \\(\\alpha = k/2\\) and \\(\\beta = 1/2\\). It appears in hypothesis testing (the chi-squared test from Chapter 4), goodness-of-fit tests, and in computing confidence intervals for variance.</p> </li> <li> <p>Student's t-distribution: looks like a normal distribution but with heavier tails. It arises when you estimate the mean of a normally distributed population using a small sample and the population variance is unknown.</p> </li> </ul> \\[f(x) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{x^2}{\\nu}\\right)^{-(\\nu+1)/2}\\] <ul> <li> <p>The parameter \\(\\nu\\) (nu) is the degrees of freedom. As \\(\\nu \\to \\infty\\), the t-distribution converges to the standard normal. With small \\(\\nu\\), the heavier tails give more probability to extreme values, reflecting the extra uncertainty from a small sample.</p> </li> <li> <p>Mean: \\(E[X] = 0\\) (for \\(\\nu &gt; 1\\)). Variance: \\(\\text{Var}(X) = \\frac{\\nu}{\\nu - 2}\\) (for \\(\\nu &gt; 2\\)).</p> </li> <li> <p>The t-distribution is used in t-tests (Chapter 4) and shows up in Bayesian inference as a marginal distribution when integrating out unknown variance.</p> </li> <li> <p>To summarise the key distributions:</p> </li> </ul> Distribution Type Support Mean Variance Bernoulli\\((p)\\) Discrete \\(\\{0,1\\}\\) \\(p\\) \\(p(1-p)\\) Binomial\\((n,p)\\) Discrete \\(\\{0,\\ldots,n\\}\\) \\(np\\) \\(np(1-p)\\) Poisson\\((\\lambda)\\) Discrete \\(\\{0,1,2,\\ldots\\}\\) \\(\\lambda\\) \\(\\lambda\\) Geometric\\((p)\\) Discrete \\(\\{1,2,3,\\ldots\\}\\) \\(1/p\\) \\((1-p)/p^2\\) Uniform\\((a,b)\\) Continuous \\([a,b]\\) \\((a+b)/2\\) \\((b-a)^2/12\\) Normal\\((\\mu,\\sigma^2)\\) Continuous \\((-\\infty,\\infty)\\) \\(\\mu\\) \\(\\sigma^2\\) Exponential\\((\\lambda)\\) Continuous \\([0,\\infty)\\) \\(1/\\lambda\\) \\(1/\\lambda^2\\) Gamma\\((\\alpha,\\beta)\\) Continuous \\((0,\\infty)\\) \\(\\alpha/\\beta\\) \\(\\alpha/\\beta^2\\) Beta\\((\\alpha,\\beta)\\) Continuous \\([0,1]\\) \\(\\alpha/(\\alpha+\\beta)\\) see above \\(\\chi^2(k)\\) Continuous \\((0,\\infty)\\) \\(k\\) \\(2k\\) Student's \\(t(\\nu)\\) Continuous \\((-\\infty,\\infty)\\) \\(0\\) \\(\\nu/(\\nu-2)\\)"},{"location":"chapter%2005%3A%20probability/03.%20distributions/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Plot the Binomial PMF for \\(n=20\\) with several values of \\(p\\). Observe how the shape shifts from left-skewed to symmetric to right-skewed. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom math import comb\n\nn = 20\nks = jnp.arange(0, n + 1)\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 4), sharey=True)\nfor ax, p, color in zip(axes, [0.2, 0.5, 0.8], [\"#e74c3c\", \"#3498db\", \"#27ae60\"]):\n    pmf = jnp.array([comb(n, int(k)) * p**k * (1-p)**(n-k) for k in ks])\n    ax.bar(ks, pmf, color=color, alpha=0.7)\n    ax.set_title(f\"Binomial(n={n}, p={p})\")\n    ax.set_xlabel(\"k\")\naxes[0].set_ylabel(\"P(X = k)\")\nplt.tight_layout()\nplt.show()\n</code></pre></p> </li> <li> <p>Verify the Poisson approximation to the Binomial. Set \\(n = 1000\\), \\(p = 0.003\\), and compare Binomial\\((n, p)\\) with Poisson\\((\\lambda = np)\\). <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom math import comb, factorial, exp\n\nn, p = 1000, 0.003\nlam = n * p\nks = jnp.arange(0, 15)\n\nbinom_pmf = jnp.array([comb(n, int(k)) * p**k * (1-p)**(n-k) for k in ks])\npoisson_pmf = jnp.array([lam**k * exp(-lam) / factorial(int(k)) for k in ks])\n\nplt.figure(figsize=(8, 4))\nplt.bar(ks - 0.15, binom_pmf, width=0.3, color=\"#3498db\", alpha=0.7, label=f\"Binomial({n},{p})\")\nplt.bar(ks + 0.15, poisson_pmf, width=0.3, color=\"#e74c3c\", alpha=0.7, label=f\"Poisson({lam})\")\nplt.xlabel(\"k\")\nplt.ylabel(\"P(X = k)\")\nplt.title(\"Poisson approximation to Binomial\")\nplt.legend()\nplt.show()\n</code></pre></p> </li> <li> <p>Sample from a Normal distribution and verify the empirical rule. Count what fraction of samples fall within 1, 2, and 3 standard deviations. <pre><code>import jax\nimport jax.numpy as jnp\n\nkey = jax.random.PRNGKey(42)\nmu, sigma = 5.0, 2.0\nsamples = mu + sigma * jax.random.normal(key, shape=(100_000,))\n\nfor k in [1, 2, 3]:\n    within = jnp.abs(samples - mu) &lt;= k * sigma\n    print(f\"Within {k}\u03c3: {within.mean():.4f} (expected: {[0.6827, 0.9545, 0.9973][k-1]:.4f})\")\n</code></pre></p> </li> <li> <p>Explore the Beta distribution by varying \\(\\alpha\\) and \\(\\beta\\). Plot several shapes and see how the distribution changes from uniform to skewed to concentrated. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nx = jnp.linspace(0.01, 0.99, 200)\n\ndef beta_pdf(x, a, b):\n    # Unnormalised for shape comparison\n    return x**(a-1) * (1-x)**(b-1)\n\nplt.figure(figsize=(10, 5))\nparams = [(1,1,\"Uniform\"), (2,5,\"Left skew\"), (5,2,\"Right skew\"),\n          (5,5,\"Symmetric\"), (0.5,0.5,\"U-shape\")]\ncolors = [\"#999\", \"#e74c3c\", \"#3498db\", \"#27ae60\", \"#9b59b6\"]\n\nfor (a, b, label), color in zip(params, colors):\n    y = beta_pdf(x, a, b)\n    y = y / jnp.trapezoid(y, x)  # normalise\n    plt.plot(x, y, label=f\"\u03b1={a}, \u03b2={b} ({label})\", color=color, linewidth=2)\n\nplt.xlabel(\"x\")\nplt.ylabel(\"Density\")\nplt.title(\"Beta distribution shapes\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2005%3A%20probability/04.%20bayesian/","title":"Bayesian Methods and Sequential Models","text":"<p>Bayesian methods combine prior beliefs with observed data to produce posterior distributions over model parameters. This file covers maximum likelihood estimation, MAP estimation, conjugate priors, Bayesian inference, hidden Markov models, and the EM algorithm -- techniques behind spam filters, language models, and uncertainty-aware ML.</p> <ul> <li> <p>So far we have described distributions and how to compute probabilities. Now we tackle the question at the heart of ML: given observed data, how do we find the best parameters for our model?</p> </li> <li> <p>Maximum Likelihood Estimation (MLE) answers this directly. Pick the parameter values that make the observed data most probable.</p> </li> <li> <p>Formally, given data \\(D = \\{x_1, x_2, \\ldots, x_n\\}\\) and a model with parameter \\(\\theta\\), the likelihood function is:</p> </li> </ul> \\[L(\\theta | D) = P(D | \\theta) = \\prod_{i=1}^{n} P(x_i | \\theta)\\] <ul> <li>The product assumes the data points are independent and identically distributed (i.i.d.). The MLE estimate is:</li> </ul> \\[\\hat{\\theta}_{\\text{MLE}} = \\arg\\max_\\theta L(\\theta | D)\\] <ul> <li>In practice we maximise the log-likelihood instead, because the log turns products into sums and prevents numerical underflow:</li> </ul> \\[\\ell(\\theta) = \\log L(\\theta | D) = \\sum_{i=1}^{n} \\log P(x_i | \\theta)\\] <ul> <li> <p>Since \\(\\log\\) is monotonically increasing, the \\(\\theta\\) that maximises \\(\\ell(\\theta)\\) also maximises \\(L(\\theta)\\).</p> </li> <li> <p>Coin toss example: you flip a coin 10 times and get 7 heads. What is the MLE estimate of the coin's bias \\(p\\) (probability of heads)?</p> </li> <li> <p>Each flip is Bernoulli(\\(p\\)), so the likelihood of 7 heads in 10 flips is:</p> </li> </ul> \\[L(p) = \\binom{10}{7} p^7 (1-p)^3\\] <ul> <li> <p>Taking the log and differentiating: \\(\\frac{d\\ell}{dp} = \\frac{7}{p} - \\frac{3}{1-p} = 0\\), which gives \\(\\hat{p}_{\\text{MLE}} = 7/10 = 0.7\\).</p> </li> <li> <p>MLE is intuitive and simple. If you got 7 heads in 10 flips, the most likely bias is 0.7. But notice the problem: if you got 10 heads in 10 flips, MLE says \\(\\hat{p} = 1\\), meaning the coin will always land heads. That seems overconfident given only 10 observations.</p> </li> <li> <p>Maximum A Posteriori (MAP) estimation fixes this by adding prior beliefs. Instead of maximising just the likelihood, MAP maximises the posterior:</p> </li> </ul> \\[\\hat{\\theta}_{\\text{MAP}} = \\arg\\max_\\theta P(\\theta | D) = \\arg\\max_\\theta P(D | \\theta) \\cdot P(\\theta)\\] <ul> <li> <p>We dropped \\(P(D)\\) from the denominator because it does not depend on \\(\\theta\\) and does not affect the argmax.</p> </li> <li> <p>The prior \\(P(\\theta)\\) encodes what we believed about \\(\\theta\\) before seeing data. If we use a Beta(2, 2) prior for our coin bias (expressing a mild belief that the coin is roughly fair), the MAP estimate is no longer simply the proportion of heads. It gets pulled toward 0.5.</p> </li> </ul> <p></p> <ul> <li>With a Beta(\\(\\alpha\\), \\(\\beta\\)) prior and observing \\(h\\) heads and \\(t\\) tails, the posterior is Beta(\\(\\alpha + h\\), \\(\\beta + t\\)), and the MAP estimate is:</li> </ul> \\[\\hat{p}_{\\text{MAP}} = \\frac{\\alpha + h - 1}{\\alpha + \\beta + h + t - 2}\\] <ul> <li> <p>For our example with Beta(2,2) prior, 7 heads, 3 tails: \\(\\hat{p}_{\\text{MAP}} = \\frac{2 + 7 - 1}{2 + 2 + 10 - 2} = \\frac{8}{12} = 0.667\\).</p> </li> <li> <p>Notice how the MAP estimate (0.667) is pulled toward 0.5 compared to the MLE (0.7). The prior acts as regularisation. In ML, L2 regularisation (weight decay) is exactly equivalent to MAP estimation with a Gaussian prior on the weights.</p> </li> <li> <p>Full Bayesian inference goes further than MAP. Instead of finding a single best \\(\\theta\\), it maintains the entire posterior distribution \\(P(\\theta | D)\\). This gives you not just a point estimate but a measure of uncertainty.</p> </li> <li> <p>For the biased coin with Beta(2,2) prior and 7 heads, 3 tails, the full posterior is Beta(9, 5). The mean of this distribution is \\(9/14 \\approx 0.643\\), and its spread tells us how confident we are. With more data, the posterior narrows.</p> </li> <li> <p>The three approaches form a spectrum:</p> <ul> <li>MLE: no prior, just data. Fast, but can overfit with little data.</li> <li>MAP: point estimate with prior regularisation. Adds robustness.</li> <li>Full Bayesian: entire posterior distribution. Most informative, but often computationally expensive.</li> </ul> </li> <li> <p>Markov chains model sequences where the next state depends only on the current state, not on the history. This \"memorylessness\" is called the Markov property:</p> </li> </ul> \\[P(X_{t+1} | X_t, X_{t-1}, \\ldots, X_1) = P(X_{t+1} | X_t)\\] <ul> <li> <p>Think of weather. Tomorrow's weather depends on today's weather, but not on last week's (a simplification, but surprisingly useful).</p> </li> <li> <p>A Markov chain has a finite set of states and a transition matrix \\(T\\) where entry \\(T_{ij}\\) gives the probability of moving from state \\(i\\) to state \\(j\\). Each row sums to 1.</p> </li> </ul> <p></p> <ul> <li>For the weather example above, the transition matrix is:</li> </ul> \\[ T = \\begin{pmatrix} 0.3 &amp; 0.4 &amp; 0.3 \\\\ 0.2 &amp; 0.5 &amp; 0.3 \\\\ 0.4 &amp; 0.3 &amp; 0.3 \\end{pmatrix} \\] <ul> <li> <p>If today is rainy (state vector \\(\\mathbf{s}_0 = [1, 0, 0]\\)), the probability distribution over tomorrow's weather is \\(\\mathbf{s}_1 = \\mathbf{s}_0 T = [0.3, 0.4, 0.3]\\). Two days from now: \\(\\mathbf{s}_2 = \\mathbf{s}_0 T^2\\). This uses matrix multiplication from Chapter 1.</p> </li> <li> <p>Many Markov chains converge to a stationary distribution \\(\\pi\\) such that \\(\\pi T = \\pi\\). No matter where you start, after enough steps the chain settles into \\(\\pi\\). This property is the foundation of MCMC (Markov Chain Monte Carlo), a widely used sampling technique in Bayesian ML.</p> </li> <li> <p>Hidden Markov Models (HMMs) extend Markov chains by adding a layer of indirection. The true states are hidden (unobserved), and at each time step the hidden state emits an observable signal.</p> </li> </ul> <p></p> <ul> <li> <p>An HMM has three components:</p> <ul> <li>Transition probabilities \\(P(z_t | z_{t-1})\\): how hidden states evolve (the Markov chain)</li> <li>Emission probabilities \\(P(x_t | z_t)\\): what each hidden state produces as observable output</li> <li>Initial distribution \\(P(z_1)\\): the starting hidden state probabilities</li> </ul> </li> <li> <p>Umbrella example: suppose you cannot see the weather directly but you can observe whether your friend carries an umbrella. The hidden states are {Rainy, Sunny} and the observation is {Umbrella, No umbrella}.</p> </li> <li> <p>Transition probabilities: \\(P(\\text{Rainy}|\\text{Rainy}) = 0.7\\), \\(P(\\text{Sunny}|\\text{Rainy}) = 0.3\\), \\(P(\\text{Rainy}|\\text{Sunny}) = 0.4\\), \\(P(\\text{Sunny}|\\text{Sunny}) = 0.6\\).</p> </li> <li> <p>Emission probabilities: \\(P(\\text{Umbrella}|\\text{Rainy}) = 0.9\\), \\(P(\\text{No umbrella}|\\text{Rainy}) = 0.1\\), \\(P(\\text{Umbrella}|\\text{Sunny}) = 0.2\\), \\(P(\\text{No umbrella}|\\text{Sunny}) = 0.8\\).</p> </li> <li> <p>The key questions for HMMs are:</p> <ul> <li>Decoding: given observations, what is the most likely sequence of hidden states? Solved by the Viterbi algorithm.</li> <li>Evaluation: what is the probability of an observation sequence? Solved by the Forward algorithm.</li> <li>Learning: given observations, what are the best model parameters? Solved by the Baum-Welch algorithm (an instance of Expectation-Maximisation).</li> </ul> </li> <li> <p>Viterbi walkthrough: suppose you observe [Umbrella, Umbrella, No umbrella] and want to find the most likely weather sequence.</p> </li> <li> <p>Start with initial probabilities. Assume \\(P(R) = 0.5\\), \\(P(S) = 0.5\\).</p> </li> <li> <p>Day 1 (observe Umbrella):</p> <ul> <li>\\(V_1(R) = P(R) \\cdot P(U|R) = 0.5 \\times 0.9 = 0.45\\)</li> <li>\\(V_1(S) = P(S) \\cdot P(U|S) = 0.5 \\times 0.2 = 0.10\\)</li> </ul> </li> <li> <p>Day 2 (observe Umbrella):</p> <ul> <li>\\(V_2(R) = \\max(V_1(R) \\cdot P(R|R), V_1(S) \\cdot P(R|S)) \\cdot P(U|R)\\)</li> <li>\\(= \\max(0.45 \\times 0.7, 0.10 \\times 0.4) \\times 0.9 = \\max(0.315, 0.04) \\times 0.9 = 0.2835\\)</li> <li>\\(V_2(S) = \\max(V_1(R) \\cdot P(S|R), V_1(S) \\cdot P(S|S)) \\cdot P(U|S)\\)</li> <li>\\(= \\max(0.45 \\times 0.3, 0.10 \\times 0.6) \\times 0.2 = \\max(0.135, 0.06) \\times 0.2 = 0.027\\)</li> </ul> </li> <li> <p>Day 3 (observe No umbrella):</p> <ul> <li>\\(V_3(R) = \\max(0.2835 \\times 0.7, 0.027 \\times 0.4) \\times 0.1 = 0.1985 \\times 0.1 = 0.01985\\)</li> <li>\\(V_3(S) = \\max(0.2835 \\times 0.3, 0.027 \\times 0.6) \\times 0.8 = 0.08505 \\times 0.8 = 0.06804\\)</li> </ul> </li> <li> <p>Day 3's maximum is at Sunny. Backtracking: Day 3 = Sunny (from R), Day 2 = Rainy (from R), Day 1 = Rainy. Most likely sequence: Rainy, Rainy, Sunny.</p> </li> <li> <p>The Forward-Backward algorithm computes the probability of being in each hidden state at each time step, given the entire observation sequence. The forward pass computes \\(P(z_t, x_{1:t})\\) and the backward pass computes \\(P(x_{t+1:T} | z_t)\\). Multiplying these gives the smoothed state probabilities.</p> </li> <li> <p>The Baum-Welch algorithm learns HMM parameters from data when the hidden states are unobserved. It is an Expectation-Maximisation (EM) algorithm: the E-step uses forward-backward to estimate which hidden states generated the observations, and the M-step updates the transition and emission probabilities.</p> </li> <li> <p>HMMs were historically dominant in speech recognition (hidden phoneme states emit acoustic signals) and bioinformatics (hidden gene states emit DNA base pairs). While deep learning has largely superseded HMMs in these fields, the ideas of hidden states, emissions, and sequential inference remain central to sequence models.</p> </li> <li> <p>Conditional Random Fields (CRFs) improve on HMMs by removing the independence assumption on emissions. In an HMM, the observation at time \\(t\\) depends only on the hidden state at time \\(t\\). CRFs allow the label at position \\(t\\) to depend on the entire input sequence.</p> </li> <li> <p>A linear-chain CRF models the conditional probability of a label sequence \\(\\mathbf{y}\\) given an input sequence \\(\\mathbf{x}\\):</p> </li> </ul> \\[P(\\mathbf{y} | \\mathbf{x}) = \\frac{1}{Z(\\mathbf{x})} \\exp\\!\\left(\\sum_t \\left[\\sum_k \\lambda_k f_k(y_t, y_{t-1}, \\mathbf{x}, t)\\right]\\right)\\] <ul> <li> <p>Here \\(f_k\\) are feature functions (which can look at any part of the input), \\(\\lambda_k\\) are learned weights, and \\(Z(\\mathbf{x})\\) is a normalising constant.</p> </li> <li> <p>CRFs are discriminative models (they model \\(P(\\mathbf{y}|\\mathbf{x})\\) directly) while HMMs are generative (they model \\(P(\\mathbf{x}, \\mathbf{y})\\)). This distinction is the same as logistic regression (discriminative) vs Naive Bayes (generative).</p> </li> <li> <p>In modern NLP, CRF layers are often added on top of neural networks (BiLSTM-CRF, BERT-CRF) for tasks like named entity recognition and part-of-speech tagging, where capturing label dependencies is important.</p> </li> </ul>"},{"location":"chapter%2005%3A%20probability/04.%20bayesian/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement MLE and MAP for a coin toss experiment. Observe how the MAP estimate changes with different priors and different amounts of data. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Data: observed coin flips\nheads, tails = 7, 3\n\n# MLE\np_mle = heads / (heads + tails)\nprint(f\"MLE: {p_mle:.4f}\")\n\n# MAP with Beta prior\nfor alpha, beta in [(1,1), (2,2), (5,5), (10,10)]:\n    p_map = (alpha + heads - 1) / (alpha + beta + heads + tails - 2)\n    print(f\"MAP (Beta({alpha},{beta})): {p_map:.4f}\")\n\n# Visualise posterior for Beta(2,2) prior\ntheta = jnp.linspace(0.01, 0.99, 200)\n# Posterior is Beta(alpha+heads, beta+tails)\na_post, b_post = 2 + heads, 2 + tails\nposterior = theta**(a_post-1) * (1-theta)**(b_post-1)\nposterior = posterior / jnp.trapezoid(posterior, theta)\n\nplt.figure(figsize=(8, 4))\nplt.plot(theta, posterior, color=\"#e74c3c\", linewidth=2, label=f\"Posterior Beta({a_post},{b_post})\")\nplt.axvline(p_mle, color=\"#3498db\", linestyle=\"--\", label=f\"MLE = {p_mle:.2f}\")\nplt.axvline((a_post-1)/(a_post+b_post-2), color=\"#e74c3c\", linestyle=\"--\", label=f\"MAP = {(a_post-1)/(a_post+b_post-2):.3f}\")\nplt.xlabel(\"\u03b8 (coin bias)\")\nplt.ylabel(\"Density\")\nplt.title(\"Posterior distribution after 7H, 3T with Beta(2,2) prior\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n</code></pre></p> </li> <li> <p>Build a Markov chain for the weather model and simulate it. Compute the stationary distribution both by simulation and by solving \\(\\pi T = \\pi\\). <pre><code>import jax\nimport jax.numpy as jnp\n\n# Transition matrix: R, S, C\nT = jnp.array([\n    [0.3, 0.4, 0.3],\n    [0.2, 0.5, 0.3],\n    [0.4, 0.3, 0.3]\n])\nstates = [\"Rainy\", \"Sunny\", \"Cloudy\"]\n\n# Simulate 100,000 steps\nkey = jax.random.PRNGKey(42)\nn_steps = 100_000\nstate = 0  # start rainy\ncounts = jnp.zeros(3)\n\nfor i in range(n_steps):\n    key, subkey = jax.random.split(key)\n    state = jax.random.choice(subkey, 3, p=T[state])\n    counts = counts.at[state].add(1)\n\nsim_stationary = counts / n_steps\nprint(\"Simulated stationary distribution:\")\nfor s, p in zip(states, sim_stationary):\n    print(f\"  {s}: {p:.4f}\")\n\n# Analytical: find left eigenvector with eigenvalue 1\neigenvalues, eigenvectors = jnp.linalg.eig(T.T)\nidx = jnp.argmin(jnp.abs(eigenvalues - 1.0))\npi = jnp.real(eigenvectors[:, idx])\npi = pi / pi.sum()\nprint(\"\\nAnalytical stationary distribution:\")\nfor s, p in zip(states, pi):\n    print(f\"  {s}: {p:.4f}\")\n</code></pre></p> </li> <li> <p>Implement the Viterbi algorithm for the umbrella HMM and decode a sequence of observations. <pre><code>import jax.numpy as jnp\n\n# HMM parameters\nstates = [\"Rainy\", \"Sunny\"]\nobs_names = [\"Umbrella\", \"No umbrella\"]\n\ntrans = jnp.array([[0.7, 0.3],   # R-&gt;R, R-&gt;S\n                    [0.4, 0.6]])  # S-&gt;R, S-&gt;S\n\nemit = jnp.array([[0.9, 0.1],    # R-&gt;U, R-&gt;noU\n                   [0.2, 0.8]])   # S-&gt;U, S-&gt;noU\n\ninit = jnp.array([0.5, 0.5])\n\n# Observations: U=0, noU=1\nobservations = [0, 0, 1]  # Umbrella, Umbrella, No umbrella\n\ndef viterbi(obs, init, trans, emit):\n    n_states = len(init)\n    T = len(obs)\n    V = jnp.zeros((T, n_states))\n    path = jnp.zeros((T, n_states), dtype=int)\n\n    # Initialisation\n    V = V.at[0].set(init * emit[:, obs[0]])\n\n    # Recursion\n    for t in range(1, T):\n        for j in range(n_states):\n            probs = V[t-1] * trans[:, j]\n            V = V.at[t, j].set(jnp.max(probs) * emit[j, obs[t]])\n            path = path.at[t, j].set(jnp.argmax(probs))\n\n    # Backtrack\n    best = [int(jnp.argmax(V[-1]))]\n    for t in range(T-1, 0, -1):\n        best.insert(0, int(path[t, best[0]]))\n    return best, V\n\ndecoded, scores = viterbi(observations, init, trans, emit)\nprint(\"Observations:\", [obs_names[o] for o in observations])\nprint(\"Decoded:     \", [states[s] for s in decoded])\n</code></pre></p> </li> <li> <p>Visualise how the posterior evolves as you observe more coin flips. Start with a Beta(1,1) prior (uniform) and update after each flip. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ntheta = jnp.linspace(0.01, 0.99, 300)\nkey = jax.random.PRNGKey(7)\n\n# True bias = 0.65\nflips = jax.random.bernoulli(key, p=0.65, shape=(50,))\n\nplt.figure(figsize=(10, 5))\na, b = 1, 1  # Beta(1,1) = uniform\n\nfor n_obs in [0, 1, 5, 10, 25, 50]:\n    h = int(flips[:n_obs].sum())\n    t = n_obs - h\n    a_post = a + h\n    b_post = b + t\n    y = theta**(a_post-1) * (1-theta)**(b_post-1)\n    y = y / jnp.trapezoid(y, theta)\n    plt.plot(theta, y, linewidth=2, label=f\"n={n_obs} (h={h})\")\n\nplt.axvline(0.65, color=\"black\", linestyle=\":\", alpha=0.5, label=\"true p=0.65\")\nplt.xlabel(\"\u03b8\")\nplt.ylabel(\"Density\")\nplt.title(\"Bayesian updating: posterior narrows with more data\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2005%3A%20probability/05.%20information%20theory/","title":"Information Theory","text":"<p>Information theory quantifies information, surprise, and the difference between probability distributions. This file covers entropy, cross-entropy, KL divergence, mutual information, and surprisal -- the concepts behind every classification loss function, VAE objective, and data compression scheme used in ML.</p> <ul> <li> <p>Information theory, founded by Claude Shannon in 1948, gives us a mathematical framework for quantifying information. It answers questions like: how surprised should you be by an event? How much information does a message carry? How different are two probability distributions?</p> </li> <li> <p>These questions sound abstract, but they are the foundation of ML loss functions, data compression, and communication systems. Cross-entropy loss, the most common loss function in classification, comes directly from information theory.</p> </li> <li> <p>Start with the simplest question: how much information does a single event carry?</p> </li> <li> <p>Surprisal (also called self-information) measures how surprising an event is. If something very likely happens, you learn almost nothing. If something rare happens, you learn a lot.</p> </li> <li> <p>If you live in a desert and someone tells you it is sunny, that is not very informative. If they tell you it is snowing, that is extremely informative. Surprisal formalises this intuition:</p> </li> </ul> \\[I(x) = \\log_2 \\frac{1}{p(x)} = -\\log_2 p(x)\\] <ul> <li> <p>The unit is bits when we use \\(\\log_2\\). A fair coin flip has surprisal \\(-\\log_2(0.5) = 1\\) bit. An event with probability \\(1/8\\) has surprisal \\(\\log_2(8) = 3\\) bits.</p> </li> <li> <p>Why logarithm and not just \\(1/p\\)? Three reasons:</p> <ul> <li>A certain event (\\(p = 1\\)) should give zero information: \\(\\log(1) = 0\\) but \\(1/1 = 1\\).</li> <li>Independent events should have additive information: \\(\\log(1/p_1 p_2) = \\log(1/p_1) + \\log(1/p_2)\\).</li> <li>We want a smooth, well-behaved function. \\(1/p\\) explodes; \\(\\log(1/p)\\) grows gently.</li> </ul> </li> <li> <p>Entropy is the expected surprisal, the average amount of information you get per event sampled from a distribution. It measures the uncertainty or \"unpredictability\" of the distribution:</p> </li> </ul> \\[H(X) = E[I(X)] = -\\sum_{x} p(x) \\log_2 p(x)\\] <p></p> <ul> <li> <p>A fair coin has entropy \\(H = -0.5\\log_2(0.5) - 0.5\\log_2(0.5) = 1\\) bit. Maximum uncertainty.</p> </li> <li> <p>A biased coin with \\(p = 0.9\\) has entropy \\(H = -0.9\\log_2(0.9) - 0.1\\log_2(0.1) \\approx 0.469\\) bits. Less uncertain, so less entropy.</p> </li> <li> <p>A deterministic event (\\(p = 1\\)) has entropy \\(H = 0\\). No uncertainty at all.</p> </li> <li> <p>Entropy is maximised when all outcomes are equally likely. For \\(n\\) equally likely outcomes, \\(H = \\log_2 n\\). A fair die has entropy \\(\\log_2 6 \\approx 2.585\\) bits.</p> </li> <li> <p>The practical meaning of entropy is compression. Shannon's source coding theorem says you cannot compress data below its entropy rate without losing information. An image where every pixel is equally likely (maximum entropy) cannot be compressed. An image that is mostly white (low entropy) compresses well.</p> </li> <li> <p>For a quick sense of scale: a grayscale pixel (256 values) has a maximum entropy of 8 bits. A 1080p grayscale image has at most \\(1920 \\times 1080 \\times 8 \\approx 16.6\\) million bits. Real images have much lower entropy because neighbouring pixels are correlated, which is why JPEG compression works.</p> </li> <li> <p>For continuous random variables, the discrete sum becomes an integral. Differential entropy is:</p> </li> </ul> \\[h(X) = -\\int_{-\\infty}^{\\infty} f(x) \\log f(x)\\, dx\\] <ul> <li> <p>A Gaussian with variance \\(\\sigma^2\\) has differential entropy \\(h = \\frac{1}{2}\\log_2(2\\pi e \\sigma^2)\\). Among all distributions with the same variance, the Gaussian has the maximum entropy. This is one reason the Gaussian is so common in modelling: it makes the fewest assumptions beyond the specified mean and variance.</p> </li> <li> <p>Mutual information measures how much knowing one variable tells you about another. It is the reduction in uncertainty about \\(X\\) when you observe \\(Y\\):</p> </li> </ul> \\[I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\\] <ul> <li>Equivalently:</li> </ul> \\[I(X; Y) = \\sum_{x,y} p(x,y) \\log_2 \\frac{p(x,y)}{p(x) p(y)}\\] <ul> <li> <p>If \\(X\\) and \\(Y\\) are independent, \\(p(x,y) = p(x)p(y)\\) and mutual information is zero. The more dependent they are, the higher the mutual information.</p> </li> <li> <p>In ML, mutual information is used in feature selection (pick features with high MI with the target), in information bottleneck methods, and in evaluating clustering quality.</p> </li> <li> <p>Cross-entropy measures the average number of bits needed to encode events from distribution \\(p\\) using a code optimised for distribution \\(q\\):</p> </li> </ul> \\[H(p, q) = -\\sum_{x} p(x) \\log_2 q(x)\\] <ul> <li> <p>If \\(q\\) matches \\(p\\) perfectly, cross-entropy equals entropy: \\(H(p, p) = H(p)\\). If \\(q\\) is a bad approximation, cross-entropy is higher. The \"extra\" bits come from the mismatch.</p> </li> <li> <p>This is exactly why cross-entropy is the standard loss function for classification in ML. The true labels define \\(p\\) (a one-hot distribution), and the model's predicted probabilities define \\(q\\). Minimising cross-entropy pushes \\(q\\) toward \\(p\\):</p> </li> </ul> \\[\\mathcal{L} = -\\sum_{c} y_c \\log \\hat{y}_c\\] <ul> <li> <p>For a single sample with true class \\(c\\), this simplifies to \\(\\mathcal{L} = -\\log \\hat{y}_c\\). The loss is the surprisal of the true class under the model's predictions. If the model assigns high probability to the correct class, the loss is low.</p> </li> <li> <p>KL divergence (Kullback-Leibler divergence, also called relative entropy) measures how much one distribution differs from another:</p> </li> </ul> \\[D_{\\text{KL}}(p \\| q) = \\sum_{x} p(x) \\log \\frac{p(x)}{q(x)} = H(p, q) - H(p)\\] <ul> <li>KL divergence is the \"extra cost\" of using distribution \\(q\\) instead of the true distribution \\(p\\). It is always non-negative (\\(D_{\\text{KL}} \\ge 0\\)) and equals zero only when \\(p = q\\).</li> </ul> <p></p> <ul> <li> <p>KL divergence is not symmetric: \\(D_{\\text{KL}}(p \\| q) \\ne D_{\\text{KL}}(q \\| p)\\). This asymmetry matters. \\(D_{\\text{KL}}(p \\| q)\\) penalises \\(q\\) for placing low probability where \\(p\\) has high probability (because \\(\\log(p/q)\\) blows up). \\(D_{\\text{KL}}(q \\| p)\\) penalises the reverse.</p> </li> <li> <p>This asymmetry leads to two styles of approximation:</p> <ul> <li>Minimising \\(D_{\\text{KL}}(p \\| q)\\) produces moment-matching behaviour: \\(q\\) covers all modes of \\(p\\) but may be too spread out.</li> <li>Minimising \\(D_{\\text{KL}}(q \\| p)\\) produces mode-seeking behaviour: \\(q\\) concentrates on one mode of \\(p\\) but may miss others. This is what variational inference uses.</li> </ul> </li> <li> <p>Since \\(H(p)\\) is constant with respect to the model, minimising cross-entropy \\(H(p, q)\\) is equivalent to minimising \\(D_{\\text{KL}}(p \\| q)\\). This is why we can use cross-entropy loss and know that we are also minimising the KL divergence between the true and predicted distributions.</p> </li> <li> <p>KL divergence plays a central role in Bayesian updating. The posterior \\(P(\\theta | D)\\) is the distribution closest to the prior \\(P(\\theta)\\) (in KL divergence terms) that is consistent with the observed data. Each new observation updates the posterior, reducing uncertainty about \\(\\theta\\).</p> </li> <li> <p>In variational autoencoders (VAEs), the loss function has two terms: a reconstruction loss (cross-entropy) and a KL divergence term that regularises the latent space to stay close to a standard normal distribution.</p> </li> <li> <p>To tie everything together: entropy tells you the intrinsic uncertainty in a distribution, cross-entropy tells you how well your model approximates reality, and KL divergence tells you the gap between the two. These three quantities form the backbone of modern ML optimisation.</p> </li> </ul>"},{"location":"chapter%2005%3A%20probability/05.%20information%20theory/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Compute the entropy of various distributions and verify that the uniform distribution has maximum entropy for a given number of outcomes. <pre><code>import jax.numpy as jnp\n\ndef entropy(p):\n    \"\"\"Compute entropy in bits. Filter out zero-probability events.\"\"\"\n    p = p[p &gt; 0]\n    return -jnp.sum(p * jnp.log2(p))\n\n# Fair die\nfair = jnp.ones(6) / 6\nprint(f\"Fair die entropy:   {entropy(fair):.4f} bits (max = log2(6) = {jnp.log2(6.):.4f})\")\n\n# Loaded die\nloaded = jnp.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.5])\nprint(f\"Loaded die entropy: {entropy(loaded):.4f} bits\")\n\n# Deterministic\ndet = jnp.array([0.0, 0.0, 0.0, 0.0, 0.0, 1.0])\nprint(f\"Deterministic:      {entropy(det):.4f} bits\")\n\n# Fair coin\ncoin = jnp.array([0.5, 0.5])\nprint(f\"Fair coin entropy:  {entropy(coin):.4f} bits\")\n</code></pre></p> </li> <li> <p>Compute cross-entropy and KL divergence between a true distribution and several approximations. Verify that \\(D_{\\text{KL}}(p \\| q) = H(p, q) - H(p)\\). <pre><code>import jax.numpy as jnp\n\ndef cross_entropy(p, q):\n    return -jnp.sum(p * jnp.log2(jnp.clip(q, 1e-10, 1.0)))\n\ndef kl_divergence(p, q):\n    mask = p &gt; 0\n    return jnp.sum(jnp.where(mask, p * jnp.log2(p / jnp.clip(q, 1e-10, 1.0)), 0.0))\n\ndef entropy(p):\n    p = p[p &gt; 0]\n    return -jnp.sum(p * jnp.log2(p))\n\np = jnp.array([0.4, 0.3, 0.2, 0.1])  # true distribution\n\nfor name, q in [(\"perfect match\", p),\n                (\"slight mismatch\", jnp.array([0.35, 0.30, 0.25, 0.10])),\n                (\"big mismatch\", jnp.array([0.1, 0.1, 0.1, 0.7]))]:\n    h_p = entropy(p)\n    h_pq = cross_entropy(p, q)\n    kl = kl_divergence(p, q)\n    print(f\"{name:20s}: H(p)={h_p:.4f}, H(p,q)={h_pq:.4f}, \"\n          f\"KL={kl:.4f}, H(p,q)-H(p)={h_pq-h_p:.4f}\")\n</code></pre></p> </li> <li> <p>Show that KL divergence is not symmetric by computing \\(D_{\\text{KL}}(p \\| q)\\) and \\(D_{\\text{KL}}(q \\| p)\\) for two different distributions. <pre><code>import jax.numpy as jnp\n\ndef kl_div(p, q):\n    mask = p &gt; 0\n    return float(jnp.sum(jnp.where(mask, p * jnp.log2(p / jnp.clip(q, 1e-10, 1.0)), 0.0)))\n\np = jnp.array([0.9, 0.1])\nq = jnp.array([0.5, 0.5])\n\nprint(f\"D_KL(p || q) = {kl_div(p, q):.4f}\")\nprint(f\"D_KL(q || p) = {kl_div(q, p):.4f}\")\nprint(f\"Not the same! KL divergence is asymmetric.\")\n</code></pre></p> </li> <li> <p>Simulate cross-entropy loss during training. Create a \"true\" one-hot label and show how the loss decreases as the model's predicted probabilities improve. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# True label: class 2 out of 4\ntrue_label = jnp.array([0, 0, 1, 0])\n\n# Simulate improving predictions\nsteps = []\nlosses = []\nfor confidence in jnp.linspace(0.25, 0.99, 50):\n    # Model becomes more confident in class 2\n    remaining = (1 - confidence) / 3\n    pred = jnp.array([remaining, remaining, confidence, remaining])\n    loss = -jnp.sum(true_label * jnp.log(jnp.clip(pred, 1e-10, 1.0)))\n    steps.append(float(confidence))\n    losses.append(float(loss))\n\nplt.figure(figsize=(8, 4))\nplt.plot(steps, losses, color=\"#e74c3c\", linewidth=2)\nplt.xlabel(\"Model confidence in true class\")\nplt.ylabel(\"Cross-entropy loss\")\nplt.title(\"Cross-entropy loss decreases as predictions improve\")\nplt.grid(alpha=0.3)\nplt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2006%3A%20machine%20learning/01.%20classical%20machine%20learning/","title":"Classical Machine Learning","text":"<p>Classical ML algorithms learn patterns from data without explicit programming, using closed-form solutions or heuristic search rather than gradient descent. This file covers Naive Bayes, k-NN, decision trees, random forests, SVMs, k-means clustering, and PCA -- the workhorses of tabular data and baselines for every ML project.</p> <ul> <li> <p>Machine learning is the study of algorithms that improve their performance on some task by learning from data, rather than being explicitly programmed with rules. Instead of writing \"if income &gt; 50k and age &lt; 30 then approve loan,\" you hand the algorithm thousands of past loan decisions and let it figure out the pattern.</p> </li> <li> <p>There are three broad paradigms. Supervised learning uses labelled data, meaning each input comes with a known correct output. The algorithm learns a mapping from inputs to outputs. Unsupervised learning works with unlabelled data and tries to discover hidden structure, like clusters or compressed representations. Reinforcement learning learns through trial and error, receiving rewards or penalties for actions taken in an environment (covered in file 04).</p> </li> <li> <p>Within supervised learning, classification predicts discrete categories (spam or not spam, cat or dog) while regression predicts continuous values (house price, temperature tomorrow). The boundary is not always sharp: logistic regression is named \"regression\" but performs classification.</p> </li> <li> <p>A key distinction in probabilistic models is generative vs discriminative. A generative model learns the joint distribution \\(P(x, y)\\), which means it understands how the data itself is generated. It can produce new samples. A discriminative model learns \\(P(y \\mid x)\\) directly, focusing only on the boundary between classes. Naive Bayes is generative; logistic regression (file 02) is discriminative. Generative models are more flexible but harder to train well; discriminative models often give better classification accuracy when you have enough data.</p> </li> <li> <p>Naive Bayes is one of the simplest and most effective classifiers. It applies Bayes' theorem (from chapter 05) directly:</p> </li> </ul> \\[P(C_k \\mid x) = \\frac{P(x \\mid C_k) \\, P(C_k)}{P(x)}\\] <ul> <li> <p>The \"naive\" part is a strong independence assumption: it treats every feature as independent given the class. If you are classifying emails as spam, Naive Bayes assumes the presence of the word \"free\" tells you nothing about the presence of the word \"winner,\" once you know the email is spam. This is almost never true in reality, but the classifier works surprisingly well anyway.</p> </li> <li> <p>Since \\(P(x)\\) is the same for all classes, classification simplifies to picking the class that maximises the numerator:</p> </li> </ul> \\[\\hat{y} = \\arg\\max_{k} \\; P(C_k) \\prod_{i=1}^{n} P(x_i \\mid C_k)\\] <ul> <li> <p>The prior \\(P(C_k)\\) is just the fraction of training examples in each class. The likelihoods \\(P(x_i \\mid C_k)\\) depend on what kind of features you have, which gives rise to three common variants.</p> </li> <li> <p>Multinomial Naive Bayes is designed for count data, like word frequencies in documents. Each feature \\(x_i\\) represents how many times word \\(i\\) appears, and the likelihood follows a multinomial distribution. This is the standard choice for text classification, sentiment analysis, and spam filtering.</p> </li> <li> <p>Gaussian Naive Bayes assumes each feature follows a normal distribution within each class. You estimate the mean \\(\\mu_{ik}\\) and variance \\(\\sigma_{ik}^2\\) of feature \\(i\\) for class \\(k\\) from the training data, then compute:</p> </li> </ul> \\[P(x_i \\mid C_k) = \\frac{1}{\\sqrt{2\\pi\\sigma_{ik}^2}} \\exp\\!\\left(-\\frac{(x_i - \\mu_{ik})^2}{2\\sigma_{ik}^2}\\right)\\] <ul> <li>This is the natural choice when your features are continuous measurements, like height, weight, or sensor readings.</li> </ul> <p></p> <ul> <li> <p>Bernoulli Naive Bayes models binary features: each feature is either present (1) or absent (0). Instead of counting how many times a word appears, you only track whether it appears at all. This works well for short texts or binary feature vectors.</p> </li> <li> <p>A practical problem arises when a feature value never appears with a certain class in training data. The likelihood becomes zero, and because everything is multiplied together, the entire posterior collapses to zero. Laplace smoothing fixes this by adding a small count (usually 1) to every feature-class combination:</p> </li> </ul> \\[P(x_i \\mid C_k) = \\frac{\\text{count}(x_i, C_k) + \\alpha}{\\text{count}(C_k) + \\alpha \\cdot V}\\] <ul> <li> <p>Here \\(\\alpha\\) is the smoothing parameter (typically 1) and \\(V\\) is the number of possible values for that feature. This ensures no probability is ever exactly zero.</p> </li> <li> <p>Decision trees take a completely different approach. Instead of computing probabilities, they partition the feature space through a sequence of yes/no questions. Think of the game Twenty Questions: at each step, you ask the question that narrows down the possibilities the most.</p> </li> <li> <p>A tree starts at the root with all training examples. At each internal node, it picks a feature and a threshold to split on (e.g., \"is age &lt; 30?\"). Examples flow left or right based on the answer. This continues recursively until the leaves, which hold predictions: the majority class for classification, or the mean value for regression.</p> </li> </ul> <p></p> <ul> <li> <p>The critical question is: which feature should you split on? You want splits that produce the \"purest\" child nodes, where most examples belong to the same class. Two common measures of impurity are Gini impurity and entropy.</p> </li> <li> <p>Gini impurity measures the probability that a randomly chosen sample would be misclassified if labelled according to the distribution in that node:</p> </li> </ul> \\[\\text{Gini}(S) = 1 - \\sum_{k=1}^{K} p_k^2\\] <ul> <li> <p>If a node is perfectly pure (all one class), Gini is 0. If classes are equally balanced (say 50/50 for two classes), Gini reaches its maximum of 0.5.</p> </li> <li> <p>Entropy (from chapter 05's information theory section) measures the average surprise:</p> </li> </ul> \\[H(S) = -\\sum_{k=1}^{K} p_k \\log_2 p_k\\] <ul> <li> <p>A pure node has entropy 0. A perfectly balanced binary node has entropy 1 bit. In practice, Gini and entropy give very similar trees; Gini is slightly faster to compute since it avoids the logarithm.</p> </li> <li> <p>Information gain is the reduction in impurity achieved by a split. For a split that divides set \\(S\\) into subsets \\(S_L\\) and \\(S_R\\):</p> </li> </ul> \\[\\text{IG}(S, \\text{split}) = H(S) - \\frac{|S_L|}{|S|} H(S_L) - \\frac{|S_R|}{|S|} H(S_R)\\] <ul> <li> <p>The algorithm greedily picks the split with the highest information gain at each node. This is a locally optimal strategy, not globally optimal, but it works well in practice.</p> </li> <li> <p>Regression trees work the same way, but leaves predict a continuous value (the mean of the examples that reach that leaf) and the splitting criterion uses variance reduction instead of Gini or entropy.</p> </li> <li> <p>Left unchecked, a decision tree will keep splitting until every leaf is pure, essentially memorising the training data. This is severe overfitting. Pruning combats this. Pre-pruning sets limits before growing the tree: maximum depth, minimum samples per leaf, or minimum information gain to make a split. Post-pruning grows the full tree first, then removes branches that do not improve performance on a validation set.</p> </li> <li> <p>A single decision tree is easy to interpret but tends to be unstable: small changes in the data can produce a very different tree. Ensemble methods combine many models to get better predictions than any single model could achieve.</p> </li> <li> <p>The core idea is the \"wisdom of crowds.\" If you ask 100 mediocre classifiers and take a majority vote, the ensemble can be excellent, as long as the individual classifiers make somewhat independent errors.</p> </li> <li> <p>Bagging (bootstrap aggregating) trains multiple models on different random subsets of the data, sampled with replacement (bootstrap samples). Each model sees roughly 63% of the original data. At prediction time, you average the outputs (regression) or take a majority vote (classification). Because each model sees different data, they make different mistakes, and averaging cancels out much of the variance.</p> </li> <li> <p>Random Forests are bagging applied to decision trees with one extra twist: at each split, the tree only considers a random subset of features (typically \\(\\sqrt{d}\\) features out of \\(d\\) total). This further decorrelates the trees, making the ensemble even more powerful. Random forests are one of the most reliable off-the-shelf classifiers in all of machine learning.</p> </li> </ul> <p></p> <ul> <li> <p>Boosting takes the opposite philosophy. Instead of training models independently, it trains them sequentially, with each new model focusing on the examples that previous models got wrong.</p> </li> <li> <p>AdaBoost (Adaptive Boosting) maintains a weight for each training example. Initially all weights are equal. After training a weak learner (often a very shallow decision tree, called a \"stump\"), examples that were misclassified get higher weights, so the next learner pays more attention to them. The final prediction is a weighted vote of all learners, where better-performing learners get more say:</p> </li> </ul> \\[H(x) = \\text{sign}\\!\\left(\\sum_{t=1}^{T} \\alpha_t \\, h_t(x)\\right)\\] <ul> <li>The weight \\(\\alpha_t\\) for learner \\(t\\) depends on its error rate \\(\\epsilon_t\\):</li> </ul> \\[\\alpha_t = \\frac{1}{2} \\ln\\!\\left(\\frac{1 - \\epsilon_t}{\\epsilon_t}\\right)\\] <ul> <li> <p>A learner with low error gets a large positive weight; one performing at chance (\\(\\epsilon = 0.5\\)) gets zero weight.</p> </li> <li> <p>Gradient Boosting generalises this idea. Instead of reweighting examples, each new model is trained to predict the residual errors (negative gradient of the loss function) of the combined ensemble so far. For squared error loss, the residuals are literally the differences between predictions and targets. Gradient boosting with decision trees (GBDT) is behind many winning solutions in structured data competitions (XGBoost, LightGBM, CatBoost are popular implementations).</p> </li> <li> <p>The key contrast: bagging reduces variance (averaging out noise) while boosting reduces bias (correcting systematic errors). Bagging works best when individual models overfit; boosting works best when they underfit.</p> </li> <li> <p>Shifting to unsupervised learning, K-Means clustering is the simplest and most widely used clustering algorithm. Given \\(n\\) data points and a target number of clusters \\(K\\), it assigns each point to one of \\(K\\) groups by minimising the total distance from each point to its cluster centre.</p> </li> <li> <p>The algorithm alternates two steps. First, assign each point to the nearest centroid. Second, update each centroid to the mean of all points assigned to it. Repeat until assignments stop changing. This is guaranteed to converge because the total within-cluster distance decreases (or stays the same) at every step.</p> </li> </ul> <p></p> <ul> <li>Formally, K-Means minimises the within-cluster sum of squares, called inertia:</li> </ul> \\[J = \\sum_{k=1}^{K} \\sum_{x \\in C_k} \\|x - \\mu_k\\|^2\\] <ul> <li> <p>where \\(\\mu_k\\) is the centroid of cluster \\(C_k\\).</p> </li> <li> <p>K-Means is sensitive to initialisation. Bad starting centroids can lead to poor local minima. The K-Means++ initialisation strategy picks the first centroid randomly, then chooses each subsequent centroid with probability proportional to its squared distance from the nearest existing centroid. This spreads out the initial centres and almost always gives better results.</p> </li> <li> <p>How do you choose \\(K\\)? Two common tools. The elbow method plots inertia vs \\(K\\) and looks for the \"elbow\" where adding more clusters stops helping much. The silhouette score measures how similar a point is to its own cluster compared to the nearest other cluster, ranging from -1 (wrong cluster) to +1 (well clustered). Average silhouette score across all points gives an overall measure of cluster quality.</p> </li> <li> <p>K-Means has limitations: it assumes spherical clusters of roughly equal size, and it makes \"hard\" assignments (each point belongs to exactly one cluster). Gaussian Mixture Models (GMMs) relax both restrictions.</p> </li> <li> <p>A GMM models the data as a mixture of \\(K\\) Gaussian distributions, each with its own mean \\(\\mu_k\\), covariance \\(\\Sigma_k\\), and mixing weight \\(\\pi_k\\) (where the weights sum to 1):</p> </li> </ul> \\[P(x) = \\sum_{k=1}^{K} \\pi_k \\, \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)\\] <ul> <li> <p>Instead of hard assignments, each point gets a soft assignment: the probability (called the \"responsibility\") that it belongs to each cluster. A point near the boundary between two Gaussians might be 60% cluster A and 40% cluster B.</p> </li> <li> <p>GMMs are fitted using the Expectation-Maximisation (EM) algorithm, which alternates two steps, much like K-Means. The E-step computes responsibilities: for each point, what is the probability it came from each Gaussian? The M-step updates parameters: given the responsibilities, what are the best means, covariances, and mixing weights? EM is guaranteed to increase the data likelihood at each iteration and converges to a local maximum.</p> </li> <li> <p>K-Means is actually a special case of EM with GMMs: it corresponds to spherical Gaussians with equal covariance and hard (0/1) responsibilities.</p> </li> <li> <p>Support Vector Machines (SVMs) approach classification from a geometric perspective. Given two linearly separable classes, there are infinitely many hyperplanes that separate them. SVM finds the one with the maximum margin, the largest possible gap between the hyperplane and the nearest data points from each class.</p> </li> <li> <p>The nearest points, the ones sitting right on the edge of the margin, are called support vectors. They are the only points that matter for defining the boundary; you could remove all other training points and get the same hyperplane.</p> </li> </ul> <p></p> <ul> <li>For a linear classifier \\(f(x) = w \\cdot x + b\\), finding the maximum margin amounts to solving:</li> </ul> \\[\\min_{w, b} \\; \\frac{1}{2}\\|w\\|^2 \\quad \\text{subject to} \\quad y_i(w \\cdot x_i + b) \\geq 1 \\; \\text{for all } i\\] <ul> <li> <p>This is a convex quadratic program, so it has a unique global solution (no local minima to worry about).</p> </li> <li> <p>Real data is rarely perfectly separable. Soft-margin SVM allows some points to violate the margin by introducing slack variables \\(\\xi_i \\geq 0\\):</p> </li> </ul> \\[\\min_{w, b, \\xi} \\; \\frac{1}{2}\\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i \\quad \\text{subject to} \\quad y_i(w \\cdot x_i + b) \\geq 1 - \\xi_i\\] <ul> <li> <p>The hyperparameter \\(C\\) controls the tradeoff: large \\(C\\) penalises misclassifications heavily (tighter fit, risk of overfitting), small \\(C\\) allows more violations (wider margin, more regularised).</p> </li> <li> <p>The most powerful feature of SVMs is the kernel trick. Many datasets that are not linearly separable in the original feature space become separable when mapped to a higher-dimensional space. The kernel trick lets you compute dot products in that high-dimensional space without ever explicitly computing the transformation.</p> </li> <li> <p>A kernel function \\(K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j)\\) replaces every dot product in the SVM optimisation. The most popular kernel is the Radial Basis Function (RBF) kernel:</p> </li> </ul> \\[K(x_i, x_j) = \\exp\\!\\left(-\\gamma \\|x_i - x_j\\|^2\\right)\\] <ul> <li> <p>The RBF kernel implicitly maps data to an infinite-dimensional space. The parameter \\(\\gamma\\) controls how far the influence of a single training point reaches: large \\(\\gamma\\) means each point only influences its immediate neighbourhood (risk of overfitting), small \\(\\gamma\\) gives smoother boundaries.</p> </li> <li> <p>Other common kernels include the polynomial kernel \\(K(x_i, x_j) = (x_i \\cdot x_j + c)^d\\) and the linear kernel \\(K(x_i, x_j) = x_i \\cdot x_j\\) (which is just the standard SVM without any transformation).</p> </li> <li> <p>In practice, SVMs with RBF kernels were the dominant classifier before deep learning took over. They still work well on small-to-medium datasets, especially when the number of features is large relative to the number of samples.</p> </li> <li> <p>The SVM's connection to chapter 02 (matrices) runs deep. The optimisation is typically solved in its dual form, where the solution depends only on dot products between training examples, which is exactly what makes the kernel trick possible. The entire algorithm operates in the language of inner products and linear algebra.</p> </li> <li> <p>To summarise the classical ML toolkit:</p> </li> </ul> Algorithm Type Key Strength Key Weakness Naive Bayes Supervised (generative) Fast, works with little data Independence assumption Decision Tree Supervised Interpretable Overfits easily Random Forest Supervised (ensemble) Robust, few hyperparameters Less interpretable Gradient Boosting Supervised (ensemble) State-of-the-art on tabular data Slower, more tuning K-Means Unsupervised (clustering) Simple, scalable Assumes spherical clusters GMM Unsupervised (clustering) Soft assignments, flexible shapes Sensitive to initialisation SVM Supervised Effective in high dimensions Slow on large datasets"},{"location":"chapter%2006%3A%20machine%20learning/01.%20classical%20machine%20learning/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement Gaussian Naive Bayes from scratch. Train on synthetic 2D data with two classes and visualise the decision boundary. Compare with scikit-learn's implementation. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Generate synthetic data\nX, y = make_classification(n_samples=300, n_features=2, n_redundant=0,\n                           n_informative=2, n_clusters_per_class=1, random_state=42)\nX, y = jnp.array(X), jnp.array(y)\n\n# Fit Gaussian Naive Bayes from scratch\nclasses = jnp.unique(y)\nparams = {}\nfor c in classes:\n    c = int(c)\n    mask = y == c\n    X_c = X[mask]\n    params[c] = {\n        'mean': jnp.mean(X_c, axis=0),\n        'var': jnp.var(X_c, axis=0),\n        'prior': jnp.sum(mask) / len(y)\n    }\n\ndef gaussian_log_likelihood(x, mean, var):\n    return -0.5 * jnp.sum(jnp.log(2 * jnp.pi * var) + (x - mean)**2 / var)\n\ndef predict(X):\n    preds = []\n    for x in X:\n        log_posts = []\n        for c in [0, 1]:\n            log_post = jnp.log(params[c]['prior']) + gaussian_log_likelihood(\n                x, params[c]['mean'], params[c]['var'])\n            log_posts.append(log_post)\n        preds.append(jnp.argmax(jnp.array(log_posts)))\n    return jnp.array(preds)\n\n# Decision boundary visualisation\nxx, yy = jnp.meshgrid(jnp.linspace(X[:,0].min()-1, X[:,0].max()+1, 200),\n                       jnp.linspace(X[:,1].min()-1, X[:,1].max()+1, 200))\ngrid = jnp.column_stack([xx.ravel(), yy.ravel()])\nzz = predict(grid).reshape(xx.shape)\n\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, zz, alpha=0.3, cmap='coolwarm')\nplt.scatter(X[y==0, 0], X[y==0, 1], c='#3498db', label='Class 0', edgecolors='k', s=20)\nplt.scatter(X[y==1, 0], X[y==1, 1], c='#e74c3c', label='Class 1', edgecolors='k', s=20)\nplt.title(\"Gaussian Naive Bayes Decision Boundary\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n\naccuracy = jnp.mean(predict(X) == y)\nprint(f\"Training accuracy: {accuracy:.2%}\")\n</code></pre></p> </li> <li> <p>Build a decision tree that splits using Gini impurity. Implement the splitting logic for a single node and show how information gain selects the best feature and threshold. <pre><code>import jax.numpy as jnp\n\ndef gini_impurity(y):\n    \"\"\"Gini impurity of a label array.\"\"\"\n    classes, counts = jnp.unique(y, return_counts=True)\n    probs = counts / len(y)\n    return 1.0 - jnp.sum(probs ** 2)\n\ndef information_gain(y, left_mask):\n    \"\"\"IG from splitting y into left/right by boolean mask.\"\"\"\n    parent_gini = gini_impurity(y)\n    left_y, right_y = y[left_mask], y[~left_mask]\n    n = len(y)\n    if len(left_y) == 0 or len(right_y) == 0:\n        return 0.0\n    child_gini = (len(left_y)/n) * gini_impurity(left_y) + \\\n                 (len(right_y)/n) * gini_impurity(right_y)\n    return float(parent_gini - child_gini)\n\ndef best_split(X, y):\n    \"\"\"Find the feature and threshold that maximise information gain.\"\"\"\n    best_ig, best_feat, best_thresh = -1, None, None\n    for feat in range(X.shape[1]):\n        thresholds = jnp.unique(X[:, feat])\n        for thresh in thresholds:\n            mask = X[:, feat] &lt;= float(thresh)\n            ig = information_gain(y, mask)\n            if ig &gt; best_ig:\n                best_ig, best_feat, best_thresh = ig, feat, float(thresh)\n    return best_feat, best_thresh, best_ig\n\n# Example: synthetic data\nfrom sklearn.datasets import make_classification\nX, y = make_classification(n_samples=100, n_features=4, n_redundant=0, random_state=0)\nX, y = jnp.array(X), jnp.array(y)\n\nfeat, thresh, ig = best_split(X, y)\nprint(f\"Best split: feature {feat}, threshold {thresh:.3f}, info gain {ig:.4f}\")\nprint(f\"Parent Gini: {gini_impurity(y):.4f}\")\nmask = X[:, feat] &lt;= thresh\nprint(f\"Left Gini:   {gini_impurity(y[mask]):.4f} ({int(jnp.sum(mask))} samples)\")\nprint(f\"Right Gini:  {gini_impurity(y[~mask]):.4f} ({int(jnp.sum(~mask))} samples)\")\n</code></pre></p> </li> <li> <p>Implement K-Means from scratch with K-Means++ initialisation. Cluster a synthetic dataset and visualise the clusters at each iteration. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic clusters\nX, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=42)\nX = jnp.array(X)\n\ndef kmeans_plus_plus_init(X, K, key):\n    \"\"\"K-Means++ initialisation.\"\"\"\n    n = X.shape[0]\n    idx = jax.random.randint(key, (), 0, n)\n    centroids = [X[idx]]\n    for _ in range(1, K):\n        dists = jnp.min(jnp.stack([jnp.sum((X - c)**2, axis=1) for c in centroids]), axis=0)\n        probs = dists / jnp.sum(dists)\n        key, subkey = jax.random.split(key)\n        idx = jax.random.choice(subkey, n, p=probs)\n        centroids.append(X[idx])\n    return jnp.stack(centroids)\n\ndef kmeans(X, K, max_iters=20, key=jax.random.PRNGKey(0)):\n    centroids = kmeans_plus_plus_init(X, K, key)\n    history = [centroids]\n    for _ in range(max_iters):\n        # Assign step\n        dists = jnp.stack([jnp.sum((X - c)**2, axis=1) for c in centroids])\n        labels = jnp.argmin(dists, axis=0)\n        # Update step\n        new_centroids = jnp.stack([\n            jnp.mean(X[labels == k], axis=0) for k in range(K)\n        ])\n        history.append(new_centroids)\n        if jnp.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return labels, centroids, history\n\nK = 4\nlabels, centroids, history = kmeans(X, K)\n\n# Plot final result\ncolors = ['#3498db', '#e74c3c', '#27ae60', '#9b59b6']\nplt.figure(figsize=(8, 6))\nfor k in range(K):\n    mask = labels == k\n    plt.scatter(X[mask, 0], X[mask, 1], c=colors[k], s=20, alpha=0.6)\n    plt.scatter(centroids[k, 0], centroids[k, 1], c=colors[k], marker='X',\n                s=200, edgecolors='k', linewidths=1.5)\nplt.title(f\"K-Means Clustering (K={K}, {len(history)-1} iterations)\")\nplt.grid(alpha=0.3)\nplt.show()\n\n# Compute inertia\ninertia = sum(jnp.sum((X[labels == k] - centroids[k])**2) for k in range(K))\nprint(f\"Final inertia: {inertia:.2f}\")\n</code></pre></p> </li> <li> <p>Demonstrate the kernel trick. Show that an RBF kernel computes dot products in a high-dimensional space by comparing the kernel matrix with explicit feature mapping for a polynomial kernel. <pre><code>import jax.numpy as jnp\n\n# Simple 2D data\nX = jnp.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n\n# Polynomial kernel: K(x,y) = (x\u00b7y + 1)^2\ndef poly_kernel(X, degree=2, c=1.0):\n    return (X @ X.T + c) ** degree\n\n# Explicit degree-2 feature map for 2D: (1, sqrt(2)*x1, sqrt(2)*x2, x1^2, x2^2, sqrt(2)*x1*x2)\ndef poly_features(X):\n    x1, x2 = X[:, 0], X[:, 1]\n    return jnp.column_stack([\n        jnp.ones(len(X)),\n        jnp.sqrt(2) * x1,\n        jnp.sqrt(2) * x2,\n        x1 ** 2,\n        x2 ** 2,\n        jnp.sqrt(2) * x1 * x2\n    ])\n\nK_trick = poly_kernel(X)\nphi = poly_features(X)\nK_explicit = phi @ phi.T\n\nprint(\"Kernel trick (polynomial degree 2):\")\nprint(K_trick)\nprint(\"\\nExplicit feature map dot products:\")\nprint(K_explicit)\nprint(f\"\\nMatrices match: {jnp.allclose(K_trick, K_explicit)}\")\n\n# RBF kernel: no finite explicit map exists\ndef rbf_kernel(X, gamma=0.5):\n    sq_dists = jnp.sum(X**2, axis=1, keepdims=True) + \\\n               jnp.sum(X**2, axis=1) - 2 * X @ X.T\n    return jnp.exp(-gamma * sq_dists)\n\nK_rbf = rbf_kernel(X)\nprint(\"\\nRBF kernel matrix:\")\nprint(K_rbf)\nprint(\"Diagonal is always 1 (a point is identical to itself)\")\nprint(\"Off-diagonal entries decay with distance\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2006%3A%20machine%20learning/02.%20gradient%20machine%20learning/","title":"Gradient Machine Learning","text":"<p>Gradient-based learning optimises model parameters by iteratively following the slope of a loss surface. This file covers linear regression, logistic regression, softmax classification, gradient descent variants, regularisation (L1/L2), and the bias-variance tradeoff -- the bridge between classical statistics and deep learning.</p> <ul> <li> <p>The classical methods in file 01 use clever heuristics or closed-form solutions. This file covers algorithms that learn by following gradients, taking small steps downhill on a loss surface until they find good parameters. Gradient-based learning is the engine behind everything from linear regression to the largest neural networks.</p> </li> <li> <p>Linear regression is the simplest gradient-based model, and it also has a closed-form solution, which makes it a perfect starting point. The model is a line (or hyperplane in higher dimensions):</p> </li> </ul> \\[\\hat{y} = w \\cdot x + b = \\sum_{i=1}^{d} w_i x_i + b\\] <ul> <li> <p>In matrix notation (from chapter 02), if we stack all training inputs as rows of a matrix \\(X\\) and absorb the bias into \\(w\\) by appending a column of ones, this becomes \\(\\hat{y} = Xw\\).</p> </li> <li> <p>The goal is to minimise the mean squared error (MSE), the average squared difference between predictions and actual values:</p> </li> </ul> \\[\\mathcal{L}(w) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\|y - Xw\\|^2\\] <ul> <li>Why squared error? It has a probabilistic justification: if you assume the targets are generated as \\(y = Xw + \\epsilon\\) where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\), then maximising the Gaussian likelihood of the data (chapter 05) is equivalent to minimising MSE. Squared error also penalises large mistakes more than small ones, which is often desirable.</li> </ul> <p></p> <ul> <li>Because MSE is a quadratic function of \\(w\\), it has a unique global minimum that we can find analytically. Taking the derivative, setting it to zero, and solving gives the normal equation:</li> </ul> \\[w^{*} = (X^T X)^{-1} X^T y\\] <ul> <li> <p>This directly uses the matrix inverse from chapter 02. The expression \\(X^T X\\) is a \\(d \\times d\\) matrix (where \\(d\\) is the number of features), and \\(X^T y\\) is a \\(d\\)-dimensional vector. The normal equation gives the exact optimal weights in one shot.</p> </li> <li> <p>When does the normal equation fail? When \\(X^T X\\) is singular (not invertible), which happens if features are linearly dependent or if you have more features than samples (\\(d &gt; n\\)). In these cases you need regularisation (covered later) or gradient descent.</p> </li> <li> <p>Logistic regression adapts the linear model for binary classification. Instead of predicting a continuous value, we want a probability between 0 and 1. The sigmoid function squashes any real number into this range:</p> </li> </ul> \\[\\sigma(z) = \\frac{1}{1 + e^{-z}}\\] <ul> <li>The model computes \\(z = w \\cdot x + b\\) (a linear score, just like linear regression) and then passes it through the sigmoid: \\(\\hat{y} = \\sigma(w \\cdot x + b)\\). The output \\(\\hat{y}\\) is interpreted as \\(P(y = 1 \\mid x)\\).</li> </ul> <p></p> <ul> <li> <p>The sigmoid has nice properties: \\(\\sigma(0) = 0.5\\), \\(\\sigma(z) \\to 1\\) as \\(z \\to \\infty\\), \\(\\sigma(z) \\to 0\\) as \\(z \\to -\\infty\\), and its derivative has the elegant form \\(\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\\).</p> </li> <li> <p>The loss function for logistic regression is binary cross-entropy (BCE), which comes directly from the Bernoulli likelihood (chapter 05):</p> </li> </ul> \\[\\mathcal{L} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\\] <ul> <li> <p>When the true label is 1, only the first term is active and it penalises low predictions. When the true label is 0, only the second term is active and it penalises high predictions. The logarithm makes the penalty extremely steep for confident wrong predictions: predicting 0.01 when the true label is 1 costs much more than predicting 0.4.</p> </li> <li> <p>Unlike MSE for linear regression, there is no closed-form solution for the BCE-minimising weights. We need an iterative approach: gradient descent.</p> </li> <li> <p>The intuition behind gradient descent is simple: imagine you are standing on a hilly landscape (the loss surface) in fog. You cannot see the global minimum, but you can feel the slope under your feet. You take a step downhill, feel the slope again, and repeat. Eventually you reach a valley.</p> </li> </ul> \\[w \\leftarrow w - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w}\\] <ul> <li>The learning rate \\(\\eta\\) controls your step size. Too large and you overshoot valleys, bouncing around without converging. Too small and you inch along painfully slowly, possibly getting stuck in a local minimum.</li> </ul> <p></p> <ul> <li> <p>The gradient \\(\\frac{\\partial \\mathcal{L}}{\\partial w}\\) is a vector pointing in the direction of steepest ascent. We subtract it because we want to go downhill. This is the chain rule from chapter 03 applied to the loss function.</p> </li> <li> <p>Batch gradient descent computes the gradient using the entire training set at every step. This gives an exact gradient but is expensive when \\(n\\) is large.</p> </li> <li> <p>Stochastic gradient descent (SGD) uses a single random example per step. The gradient is noisy (it estimates the true gradient from one sample) but each step is extremely fast. The noise can actually help escape shallow local minima.</p> </li> <li> <p>Mini-batch gradient descent splits the difference: use a batch of \\(B\\) examples (typically 32, 64, or 256) per step. This balances computational efficiency (vectorised operations on the batch) with gradient quality. Almost all deep learning uses mini-batch SGD.</p> </li> <li> <p>Backpropagation is how we actually compute gradients in models with many parameters, like neural networks. It is the chain rule from chapter 03 applied systematically through a computational graph.</p> </li> <li> <p>Any model can be represented as a directed acyclic graph of operations: inputs flow in, get multiplied by weights, added together, passed through nonlinear functions, and eventually produce a loss value. The forward pass computes the output (and loss) by flowing data through this graph from input to output.</p> </li> <li> <p>The backward pass (backpropagation) flows gradients in reverse. Starting from the loss, you compute how the loss changes with respect to each intermediate value, using the chain rule at every node. If \\(L\\) depends on \\(z\\) which depends on \\(w\\), then:</p> </li> </ul> \\[\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}\\] <ul> <li> <p>Each node only needs to know its own local derivative and the gradient flowing in from above. This makes backpropagation modular and efficient: the cost is roughly twice the forward pass (one pass forward, one backward).</p> </li> <li> <p>Vanilla SGD has a problem: it oscillates in directions with steep curvature while making slow progress in flat directions. Optimisers improve on this by adapting the step based on gradient history.</p> </li> <li> <p>SGD with momentum keeps a running average of past gradients (an exponential moving average, from chapter 04). This smooths out oscillations and accelerates progress along consistent directions:</p> </li> </ul> \\[v_t = \\beta v_{t-1} + (1 - \\beta) \\nabla \\mathcal{L}$$ $$w \\leftarrow w - \\eta \\, v_t\\] <ul> <li> <p>Think of a ball rolling downhill: momentum lets it build up speed in a consistent direction and dampens the side-to-side jitter. The typical value is \\(\\beta = 0.9\\).</p> </li> <li> <p>Nesterov Accelerated Gradient (NAG) is a small but clever tweak: instead of computing the gradient at the current position, compute it at the \"look-ahead\" position \\(w - \\eta \\beta v_{t-1}\\). This corrective step reduces overshooting:</p> </li> </ul> \\[v_t = \\beta \\, v_{t-1} + \\nabla \\mathcal{L}(w - \\eta \\beta \\, v_{t-1})$$ $$w \\leftarrow w - \\eta \\, v_t\\] <ul> <li>Adagrad adapts the learning rate per parameter. Parameters that receive large gradients get smaller learning rates, and vice versa. It accumulates the squared gradients:</li> </ul> \\[G_t = G_{t-1} + g_t^2, \\quad w \\leftarrow w - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} g_t\\] <ul> <li> <p>The problem: \\(G_t\\) only grows, so the effective learning rate monotonically decreases and eventually becomes too small to learn anything.</p> </li> <li> <p>RMSprop fixes this by using an exponential moving average of squared gradients instead of a sum, so recent gradients matter more than ancient ones:</p> </li> </ul> \\[s_t = \\beta \\, s_{t-1} + (1 - \\beta) g_t^2, \\quad w \\leftarrow w - \\frac{\\eta}{\\sqrt{s_t + \\epsilon}} g_t\\] <ul> <li>Adam (Adaptive Moment Estimation) combines momentum and RMSprop. It maintains both a first-moment estimate (mean of gradients, like momentum) and a second-moment estimate (mean of squared gradients, like RMSprop):</li> </ul> \\[m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t$$ $$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\\] <ul> <li>Since \\(m_t\\) and \\(v_t\\) are initialised at zero, they are biased toward zero in early steps. Bias correction fixes this:</li> </ul> \\[\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\\] \\[w \\leftarrow w - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\\] <p></p> <ul> <li> <p>Default hyperparameters (\\(\\beta_1 = 0.9\\), \\(\\beta_2 = 0.999\\), \\(\\epsilon = 10^{-8}\\)) work well across a wide range of problems, which is why Adam is the default optimiser in most deep learning work.</p> </li> <li> <p>AdamW decouples weight decay from the gradient update. Standard L2 regularisation and weight decay are equivalent for SGD but not for Adam. AdamW applies weight decay directly to the parameters rather than adding \\(\\lambda w\\) to the gradient. This gives better generalisation and is now the standard in transformer training:</p> </li> </ul> \\[w \\leftarrow w - \\eta \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\, w \\right)\\] <ul> <li>LION (EvoLved Sign Momentum) is a newer optimiser discovered through program search. It uses only the sign of the momentum update (not the magnitude), which makes each update uniform in scale. LION uses less memory than Adam (no second-moment buffer) and can match or beat Adam on many tasks:</li> </ul> \\[w \\leftarrow w - \\eta \\cdot \\text{sign}(\\beta_1 \\, m_{t-1} + (1 - \\beta_1) \\, g_t)$$ $$m_t = \\beta_2 \\, m_{t-1} + (1 - \\beta_2) \\, g_t\\] <ul> <li>Muon (Momentum + Orthogonalisation) applies Nesterov momentum and then orthogonalises the update matrix using Newton-Schulz iterations, which approximate the polar decomposition. The resulting update direction lies on the Stiefel manifold, every update has roughly equal magnitude across all singular directions, preventing any single direction from dominating. This removes the need for adaptive second-moment estimates (no \\(v_t\\) buffer like Adam), reducing memory. Muon has shown strong results on transformer training, often matching AdamW quality at faster convergence, particularly for the attention and MLP weight matrices. Embedding and output layers are typically still handled by AdamW.</li> </ul> \\[G_t = \\text{NesterovMomentum}(\\nabla \\mathcal{L})$$ $$U_t = \\text{NewtonSchulz}(G_t) \\approx G_t (G_t^T G_t)^{-1/2}$$ $$W \\leftarrow W - \\eta \\, U_t\\] <ul> <li>The Newton-Schulz iteration computes the orthogonal factor by repeating \\(X_{k+1} = \\frac{1}{2} X_k (3I - X_k^T X_k)\\) for a few steps (typically 5-10). This avoids the cost of a full SVD while giving a good approximation.</li> </ul> <p></p> <p></p> <ul> <li> <p>Beyond MSE and BCE, several other loss functions are commonly used.</p> </li> <li> <p>Mean Absolute Error (MAE), or L1 loss, takes the average of absolute differences: \\(\\frac{1}{n}\\sum|y_i - \\hat{y}_i|\\). It is more robust to outliers than MSE because it does not square large errors.</p> </li> <li> <p>Huber loss combines the best of both: it behaves like MSE for small errors (smooth, easy to optimise) and like MAE for large errors (robust to outliers). It has a threshold \\(\\delta\\) that controls the transition.</p> </li> <li> <p>Categorical cross-entropy (CCE) generalises BCE to multiple classes. If \\(\\hat{y}_k\\) is the predicted probability for class \\(k\\) and the true class is \\(c\\):</p> </li> </ul> \\[\\mathcal{L} = -\\log(\\hat{y}_c)\\] <ul> <li> <p>This is just the negative log-probability of the correct class. Minimising cross-entropy is equivalent to maximising the likelihood, which connects back to the information theory in chapter 05: cross-entropy measures how many extra bits you need when using your predicted distribution instead of the true distribution.</p> </li> <li> <p>Hinge loss is used by SVMs: \\(\\mathcal{L} = \\max(0, 1 - y \\cdot f(x))\\). It only penalises predictions that are on the wrong side of the margin or within the margin. Once a point is correctly classified with sufficient confidence, the loss is zero.</p> </li> <li> <p>Regularisation prevents overfitting by adding a penalty for complex models. The regularised loss is:</p> </li> </ul> \\[\\mathcal{L}_{\\text{reg}} = \\mathcal{L}_{\\text{data}} + \\lambda \\, R(w)\\] <ul> <li> <p>L2 regularisation (Ridge, weight decay) penalises the sum of squared weights: \\(R(w) = \\|w\\|^2 = \\sum w_i^2\\). It discourages any single weight from becoming too large, effectively shrinking all weights toward zero but rarely making them exactly zero.</p> </li> <li> <p>L1 regularisation (Lasso) penalises the sum of absolute weights: \\(R(w) = \\|w\\|_1 = \\sum |w_i|\\). It encourages sparsity, driving many weights to exactly zero, which performs automatic feature selection.</p> </li> <li> <p>Elastic Net combines both: \\(R(w) = \\alpha \\|w\\|_1 + (1 - \\alpha) \\|w\\|^2\\), blending sparsity and shrinkage.</p> </li> <li> <p>There is a beautiful Bayesian interpretation (from chapter 05). L2 regularisation is equivalent to placing a Gaussian prior on the weights and finding the MAP estimate. L1 regularisation corresponds to a Laplace prior. The regularisation strength \\(\\lambda\\) controls how much you trust the prior relative to the data.</p> </li> <li> <p>Evaluation metrics tell you whether your model is actually working. For regression, MSE and MAE are standard. For classification, things are more nuanced.</p> </li> <li> <p>A confusion matrix is a table of four counts for binary classification:</p> </li> <li>True Positive (TP): predicted positive, actually positive</li> <li>False Positive (FP): predicted positive, actually negative</li> <li>True Negative (TN): predicted negative, actually negative</li> <li> <p>False Negative (FN): predicted negative, actually positive</p> </li> <li> <p>Accuracy = \\(\\frac{TP + TN}{TP + TN + FP + FN}\\) can be misleading when classes are imbalanced. If 99% of emails are not spam, a model that always predicts \"not spam\" has 99% accuracy but is useless.</p> </li> <li> <p>Precision = \\(\\frac{TP}{TP + FP}\\) answers: of all predicted positives, how many are actually positive? High precision means few false alarms.</p> </li> <li> <p>Recall (sensitivity) = \\(\\frac{TP}{TP + FN}\\) answers: of all actual positives, how many did you catch? High recall means few missed cases.</p> </li> <li> <p>F1 score = \\(\\frac{2 \\cdot \\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\\) is the harmonic mean of precision and recall, balancing both.</p> </li> <li> <p>The ROC curve plots the true positive rate (recall) against the false positive rate (\\(\\frac{FP}{FP + TN}\\)) as you vary the classification threshold from 0 to 1. A perfect classifier hugs the top-left corner. The AUC (area under the ROC curve) summarises performance in a single number: 1.0 is perfect, 0.5 is random guessing.</p> </li> <li> <p>Cross-validation provides a more reliable estimate of generalisation performance. In \\(k\\)-fold cross-validation, you split the data into \\(k\\) folds, train on \\(k-1\\) of them, test on the remaining fold, and rotate. The average test performance across all \\(k\\) folds is your estimate. This uses all data for both training and testing (just never at the same time), which is especially valuable when data is scarce.</p> </li> <li> <p>The bias-variance tradeoff (from chapter 04) is the fundamental tension in ML. A model's expected error decomposes into:</p> </li> </ul> \\[\\text{Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}\\] <ul> <li> <p>Bias is systematic error from wrong assumptions (e.g., fitting a line to curved data). Variance is sensitivity to training data fluctuations (e.g., a degree-20 polynomial fitting noise). Simple models have high bias and low variance; complex models have low bias and high variance. The sweet spot minimises total error.</p> </li> <li> <p>Learning rate scheduling adjusts \\(\\eta\\) during training. Common strategies:</p> </li> <li>Step decay: multiply \\(\\eta\\) by a factor (e.g., 0.1) every \\(N\\) epochs</li> <li>Cosine annealing: smoothly decrease \\(\\eta\\) following a cosine curve from the initial value to near zero</li> <li>Warmup: start with a very small \\(\\eta\\) and linearly increase it for the first few thousand steps, then decay. This prevents large initial gradients from destabilising training</li> <li> <p>1cycle: one cosine cycle up then down, which can give faster convergence</p> </li> <li> <p>Hyperparameter tuning is the process of finding good values for learning rate, batch size, regularisation strength, and other settings that are not learned by gradient descent. Common approaches:</p> </li> <li>Grid search: try every combination on a predefined grid (exhaustive but expensive)</li> <li>Random search: sample combinations randomly, which is often more efficient because not all hyperparameters matter equally</li> <li>Bayesian optimisation: build a model of the objective function and intelligently choose the next hyperparameters to try</li> <li> <p>ASHA (Asynchronous Successive Halving Algorithm): runs many trials in parallel with small budgets, then promotes the most promising ones to larger budgets while killing the rest early. It combines the efficiency of early stopping with massive parallelism \u2014 instead of running 100 full training runs, start all 100 cheaply, keep the top quarter at each rung, and only a handful run to completion. This is the backbone of modern large-scale tuning frameworks like Ray Tune.</p> </li> <li> <p>Schedule-free learning eliminates the need for a learning rate schedule altogether. Instead of decaying \\(\\eta\\) on a fixed curve, it maintains two sequences: a slow-moving average of iterates \\(z_t\\) (which converges to the optimum) and a fast exploratory iterate \\(y_t\\) (where gradients are evaluated). The final output is the averaged sequence, which provably matches the convergence rate of the best schedule in hindsight. This removes the schedule as a hyperparameter entirely \u2014 you only set the base learning rate and the optimizer handles the rest. Schedule-free variants of both SGD and Adam have been shown to match or exceed their tuned-schedule counterparts.</p> </li> </ul>"},{"location":"chapter%2006%3A%20machine%20learning/02.%20gradient%20machine%20learning/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement linear regression with both the normal equation and gradient descent. Compare the solutions and plot the convergence of the GD loss over iterations. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Generate synthetic data: y = 3x + 2 + noise\nkey = jax.random.PRNGKey(42)\nn = 100\nX = jax.random.uniform(key, (n, 1), minval=0, maxval=10)\ny = 3 * X[:, 0] + 2 + jax.random.normal(key, (n,)) * 1.5\n\n# Add bias column\nX_b = jnp.column_stack([X, jnp.ones(n)])\n\n# Normal equation\nw_exact = jnp.linalg.solve(X_b.T @ X_b, X_b.T @ y)\nprint(f\"Normal equation: w={w_exact[0]:.4f}, b={w_exact[1]:.4f}\")\n\n# Gradient descent\nw_gd = jnp.zeros(2)\nlr = 0.005\nlosses = []\nfor step in range(500):\n    pred = X_b @ w_gd\n    error = pred - y\n    loss = jnp.mean(error ** 2)\n    losses.append(float(loss))\n    grad = (2 / n) * X_b.T @ error\n    w_gd = w_gd - lr * grad\n\nprint(f\"Gradient descent: w={w_gd[0]:.4f}, b={w_gd[1]:.4f}\")\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\naxes[0].scatter(X[:, 0], y, s=15, alpha=0.5, color='#3498db')\naxes[0].plot([0, 10], [w_exact[1], w_exact[0]*10 + w_exact[1]], color='#e74c3c', linewidth=2)\naxes[0].set_title(\"Linear Regression Fit\")\naxes[0].set_xlabel(\"x\"); axes[0].set_ylabel(\"y\")\n\naxes[1].plot(losses, color='#27ae60', linewidth=1.5)\naxes[1].set_title(\"GD Loss Convergence\")\naxes[1].set_xlabel(\"Step\"); axes[1].set_ylabel(\"MSE\")\naxes[1].set_yscale('log')\nplt.tight_layout()\nplt.show()\n</code></pre></p> </li> <li> <p>Implement logistic regression from scratch with gradient descent. Train on a 2D dataset and visualise the learned decision boundary. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\n\n# Generate data\nX, y = make_moons(n_samples=300, noise=0.2, random_state=42)\nX, y = jnp.array(X), jnp.array(y, dtype=jnp.float32)\n\ndef sigmoid(z):\n    return 1 / (1 + jnp.exp(-z))\n\n# Add bias column\nX_b = jnp.column_stack([X, jnp.ones(len(X))])\nw = jnp.zeros(3)\nlr = 0.5\nlosses = []\n\nfor step in range(2000):\n    z = X_b @ w\n    pred = sigmoid(z)\n    # BCE loss\n    loss = -jnp.mean(y * jnp.log(pred + 1e-8) + (1 - y) * jnp.log(1 - pred + 1e-8))\n    losses.append(float(loss))\n    # Gradient\n    grad = X_b.T @ (pred - y) / len(y)\n    w = w - lr * grad\n\n# Decision boundary\nxx, yy = jnp.meshgrid(jnp.linspace(-2, 3, 200), jnp.linspace(-1.5, 2, 200))\ngrid = jnp.column_stack([xx.ravel(), yy.ravel(), jnp.ones(xx.size)])\nzz = sigmoid(grid @ w).reshape(xx.shape)\n\nplt.figure(figsize=(8, 6))\nplt.contourf(xx, yy, zz, levels=[0, 0.5, 1], alpha=0.3, colors=['#e74c3c', '#3498db'])\nplt.contour(xx, yy, zz, levels=[0.5], colors='#9b59b6', linewidths=2)\nplt.scatter(X[y==0, 0], X[y==0, 1], c='#e74c3c', s=15, label='Class 0')\nplt.scatter(X[y==1, 0], X[y==1, 1], c='#3498db', s=15, label='Class 1')\nplt.title(\"Logistic Regression Decision Boundary\")\nplt.legend()\nplt.grid(alpha=0.3)\nplt.show()\n</code></pre></p> </li> <li> <p>Compare optimiser trajectories on a 2D quadratic surface. Run SGD, SGD+Momentum, and Adam from the same starting point and plot their paths. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Elongated quadratic: L(w1, w2) = 0.5*w1^2 + 10*w2^2\ndef loss_fn(w):\n    return 0.5 * w[0]**2 + 10 * w[1]**2\n\ngrad_fn = jax.grad(loss_fn)\n\ndef run_sgd(w0, lr=0.05, steps=80):\n    w = w0.copy()\n    path = [w.copy()]\n    for _ in range(steps):\n        g = grad_fn(w)\n        w = w - lr * g\n        path.append(w.copy())\n    return jnp.stack(path)\n\ndef run_momentum(w0, lr=0.05, beta=0.9, steps=80):\n    w, v = w0.copy(), jnp.zeros(2)\n    path = [w.copy()]\n    for _ in range(steps):\n        g = grad_fn(w)\n        v = beta * v + (1 - beta) * g\n        w = w - lr * v\n        path.append(w.copy())\n    return jnp.stack(path)\n\ndef run_adam(w0, lr=0.05, b1=0.9, b2=0.999, eps=1e-8, steps=80):\n    w, m, v = w0.copy(), jnp.zeros(2), jnp.zeros(2)\n    path = [w.copy()]\n    for t in range(1, steps + 1):\n        g = grad_fn(w)\n        m = b1 * m + (1 - b1) * g\n        v = b2 * v + (1 - b2) * g**2\n        m_hat = m / (1 - b1**t)\n        v_hat = v / (1 - b2**t)\n        w = w - lr * m_hat / (jnp.sqrt(v_hat) + eps)\n        path.append(w.copy())\n    return jnp.stack(path)\n\nw0 = jnp.array([8.0, 3.0])\nsgd_path = run_sgd(w0)\nmom_path = run_momentum(w0)\nadam_path = run_adam(w0)\n\n# Plot\nfig, ax = plt.subplots(figsize=(8, 6))\nw1 = jnp.linspace(-10, 10, 100)\nw2 = jnp.linspace(-4, 4, 100)\nW1, W2 = jnp.meshgrid(w1, w2)\nL = 0.5 * W1**2 + 10 * W2**2\nax.contour(W1, W2, L, levels=20, cmap='Greys', alpha=0.4)\nax.plot(sgd_path[:,0], sgd_path[:,1], 'o-', color='#3498db', markersize=2, linewidth=1, label='SGD')\nax.plot(mom_path[:,0], mom_path[:,1], 'o-', color='#27ae60', markersize=2, linewidth=1, label='Momentum')\nax.plot(adam_path[:,0], adam_path[:,1], 'o-', color='#e74c3c', markersize=2, linewidth=1, label='Adam')\nax.plot(0, 0, 'k*', markersize=15, label='Minimum')\nax.set_xlabel('w\u2081'); ax.set_ylabel('w\u2082')\nax.set_title(\"Optimizer Trajectories on Elongated Quadratic\")\nax.legend()\nplt.grid(alpha=0.3)\nplt.show()\n</code></pre></p> </li> <li> <p>Show the effect of L1 vs L2 regularisation on weight sparsity. Train linear regression with both penalties and compare the resulting weight vectors. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Synthetic data: only first 3 of 20 features are relevant\nkey = jax.random.PRNGKey(0)\nn, d = 200, 20\nw_true = jnp.zeros(d).at[:3].set(jnp.array([3.0, -2.0, 1.5]))\nX = jax.random.normal(key, (n, d))\ny = X @ w_true + 0.5 * jax.random.normal(key, (n,))\n\ndef train_ridge(X, y, lam=1.0, lr=0.01, steps=2000):\n    \"\"\"L2 regularised linear regression via GD.\"\"\"\n    w = jnp.zeros(X.shape[1])\n    for _ in range(steps):\n        pred = X @ w\n        grad = (2/len(y)) * X.T @ (pred - y) + 2 * lam * w\n        w = w - lr * grad\n    return w\n\ndef train_lasso(X, y, lam=1.0, lr=0.01, steps=2000):\n    \"\"\"L1 regularised linear regression via proximal GD.\"\"\"\n    w = jnp.zeros(X.shape[1])\n    for _ in range(steps):\n        pred = X @ w\n        grad = (2/len(y)) * X.T @ (pred - y)\n        w = w - lr * grad\n        # Soft thresholding (proximal operator for L1)\n        w = jnp.sign(w) * jnp.maximum(jnp.abs(w) - lr * lam, 0)\n    return w\n\nw_l2 = train_ridge(X, y, lam=0.1)\nw_l1 = train_lasso(X, y, lam=0.1)\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\naxes[0].bar(range(d), w_true, color='#333', alpha=0.7)\naxes[0].set_title(\"True Weights\"); axes[0].set_xlabel(\"Feature\")\naxes[1].bar(range(d), w_l2, color='#3498db', alpha=0.7)\naxes[1].set_title(\"L2 (Ridge): shrinks all\"); axes[1].set_xlabel(\"Feature\")\naxes[2].bar(range(d), w_l1, color='#e74c3c', alpha=0.7)\naxes[2].set_title(\"L1 (Lasso): zeros out irrelevant\"); axes[2].set_xlabel(\"Feature\")\nplt.tight_layout()\nplt.show()\n\nprint(f\"L2 non-zero weights: {int(jnp.sum(jnp.abs(w_l2) &gt; 0.01))}/{d}\")\nprint(f\"L1 non-zero weights: {int(jnp.sum(jnp.abs(w_l1) &gt; 0.01))}/{d}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2006%3A%20machine%20learning/03.%20deep%20learning/","title":"Deep Learning","text":"<p>Deep learning stacks nonlinear layers to build hierarchical representations that transform raw inputs into useful features automatically. This file covers MLPs, activation functions, backpropagation, CNNs, RNNs, LSTMs, attention, transformers, GANs, VAEs, diffusion models, and normalisation techniques -- the architectures powering modern AI.</p> <ul> <li> <p>What makes a network \"deep\"? A shallow network has one hidden layer; a deep network has many. Depth lets the network build hierarchical representations, with early layers learning simple features (edges, tones) and later layers composing them into complex concepts (faces, sentences). This compositionality is what gives deep learning its power.</p> </li> <li> <p>The simplest deep network is the multi-layer perceptron (MLP), also called a fully connected or dense network. Each layer computes:</p> </li> </ul> \\[h = \\sigma(Wx + b)\\] <ul> <li> <p>Here \\(W\\) is a weight matrix (chapter 02), \\(b\\) is a bias vector, and \\(\\sigma\\) is a nonlinear activation function. The output of one layer becomes the input to the next. Without the nonlinearity, stacking layers would be pointless: \\(W_2(W_1 x) = (W_2 W_1)x\\), which is just another linear transformation. This is exactly the matrix multiplication collapse from chapter 02.</p> </li> <li> <p>Activation functions introduce the nonlinearity that makes depth meaningful.</p> </li> <li> <p>ReLU (Rectified Linear Unit): \\(\\text{ReLU}(x) = \\max(0, x)\\). It is the most widely used activation. It is fast to compute, does not saturate for positive inputs, and produces sparse activations (many neurons output exactly zero). The downside: neurons with negative input always output zero, and if they get stuck there permanently, they \"die\" and stop learning.</p> </li> <li> <p>Sigmoid: \\(\\sigma(x) = \\frac{1}{1+e^{-x}}\\), squashing inputs to \\((0, 1)\\). Useful for output layers in binary classification, but problematic in hidden layers because gradients vanish when the input is far from zero (the curve is nearly flat).</p> </li> <li> <p>Tanh: \\(\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\\), squashing to \\((-1, 1)\\). Zero-centred (unlike sigmoid), which helps gradient flow, but still suffers from vanishing gradients at extremes.</p> </li> <li> <p>GELU (Gaussian Error Linear Unit): \\(\\text{GELU}(x) = x \\cdot \\Phi(x)\\), where \\(\\Phi\\) is the standard normal CDF. It is a smooth approximation to ReLU that allows small negative values through. GELU is the default in GPT and BERT.</p> </li> <li> <p>Swish: \\(\\text{Swish}(x) = x \\cdot \\sigma(x)\\), another smooth gate. Similar to GELU in practice.</p> </li> </ul> <p></p> <ul> <li> <p>A dense layer with \\(d_{\\text{in}}\\) inputs and \\(d_{\\text{out}}\\) outputs has \\(d_{\\text{in}} \\times d_{\\text{out}} + d_{\\text{out}}\\) parameters (weights plus biases). The matrix multiply \\(Wx\\) is just matrix-vector multiplication from chapter 02. In a batch setting, the input is a matrix \\(X\\) of shape \\((B, d_{\\text{in}})\\) and the output is \\(XW^T + b\\) of shape \\((B, d_{\\text{out}})\\).</p> </li> <li> <p>The universal approximation theorem states that a single hidden layer with enough neurons can approximate any continuous function on a compact domain to arbitrary accuracy. This sounds like depth should not matter, but the catch is \"enough neurons.\" In practice, deep networks can represent the same functions with exponentially fewer parameters than shallow ones. Depth gives you efficiency, not just expressiveness.</p> </li> <li> <p>As networks get deeper, two gradient pathologies emerge. Vanishing gradients: when gradients pass through many layers (via the chain rule, chapter 03), they get multiplied by many factors. If these factors are consistently less than 1 (as happens with sigmoid and tanh saturating), the gradient shrinks exponentially toward zero. Early layers barely learn. Exploding gradients: if factors are consistently greater than 1, gradients grow exponentially, causing numerical overflow and unstable training.</p> </li> <li> <p>Solutions to vanishing/exploding gradients:</p> </li> <li>Use ReLU or GELU activations (gradient is 1 for positive inputs, no saturation)</li> <li>Careful weight initialisation</li> <li>Normalisation layers</li> <li>Residual connections (skip connections)</li> <li> <p>Gradient clipping (for exploding gradients): cap the gradient norm at a maximum value</p> </li> <li> <p>Weight initialisation matters because it determines the scale of activations and gradients at the start of training. If weights are too large, activations explode; too small, they vanish.</p> </li> <li> <p>Xavier (Glorot) initialisation sets weights from a distribution with variance \\(\\frac{2}{d_{\\text{in}} + d_{\\text{out}}}\\). This keeps the variance of activations roughly constant across layers, assuming linear or tanh activations.</p> </li> <li> <p>He (Kaiming) initialisation uses variance \\(\\frac{2}{d_{\\text{in}}}\\), which is calibrated for ReLU activations (since ReLU zeros out half the activations, you need double the variance to compensate).</p> </li> <li> <p>Normalisation layers stabilise training by ensuring that the inputs to each layer have consistent statistics (roughly zero mean, unit variance).</p> </li> <li> <p>Batch Normalisation (BatchNorm) normalises across the batch dimension: for each channel/feature, compute the mean and variance across all samples in the mini-batch, then normalise. It adds learnable scale (\\(\\gamma\\)) and shift (\\(\\beta\\)) parameters so the network can undo the normalisation if needed:</p> </li> </ul> \\[\\hat{x} = \\frac{x - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}, \\quad y = \\gamma \\hat{x} + \\beta\\] <ul> <li> <p>BatchNorm has a problem: it depends on the batch size. With very small batches, the statistics are noisy. At inference time, you use running averages instead of batch statistics, which creates a train/test discrepancy.</p> </li> <li> <p>Layer Normalisation (LayerNorm) normalises across the feature dimension for each individual sample. It does not depend on other samples in the batch, making it the standard choice for transformers and recurrent networks.</p> </li> <li> <p>Instance Normalisation normalises across spatial dimensions for each sample and each channel independently. It is popular in style transfer.</p> </li> <li> <p>Group Normalisation splits channels into groups and normalises within each group. It is a compromise between LayerNorm and InstanceNorm.</p> </li> </ul> <p></p> <ul> <li> <p>Dropout is a regularisation technique that randomly zeroes out a fraction \\(p\\) of neurons during training. This forces the network to not rely on any single neuron, encouraging redundant representations. At test time, all neurons are active. Inverted dropout scales activations by \\(\\frac{1}{1-p}\\) during training so that no scaling is needed at test time. This is the standard implementation.</p> </li> <li> <p>Convolutional Neural Networks (CNNs) exploit spatial structure. Instead of connecting every input to every output (as in dense layers), a convolutional layer slides a small filter (kernel) across the input, computing a dot product at each position. The same filter weights are shared across all positions, which drastically reduces parameters and builds in translation invariance.</p> </li> <li> <p>The convolution operation for a 2D input with filter \\(K\\) of size \\(k \\times k\\):</p> </li> </ul> \\[(\\text{input} * K)[i,j] = \\sum_{m=0}^{k-1} \\sum_{n=0}^{k-1} \\text{input}[i+m, j+n] \\cdot K[m, n]\\] <p></p> <ul> <li> <p>The output size depends on three hyperparameters. Stride controls how many pixels the filter moves between positions (stride 2 halves the spatial dimensions). Padding adds zeros around the input border (\"same\" padding preserves spatial size, \"valid\" padding does not). The output size formula: \\(\\text{out} = \\lfloor (\\text{in} - k + 2p) / s \\rfloor + 1\\).</p> </li> <li> <p>Pooling layers downsample feature maps. Max pooling takes the maximum value in each window; average pooling takes the mean. Pooling reduces spatial dimensions while keeping the most important information.</p> </li> <li> <p>Dilated convolutions insert gaps between filter elements, increasing the receptive field without increasing parameters. A dilation rate of 2 means the 3x3 filter covers a 5x5 area.</p> </li> <li> <p>1x1 convolutions are convolutions with a 1x1 filter. They do not look at spatial neighbours; instead, they mix information across channels. Think of them as applying a dense layer at every spatial position. They are used to change the number of channels cheaply.</p> </li> <li> <p>Skip connections (residual connections) let the input bypass one or more layers: \\(\\text{output} = F(x) + x\\). The layer only needs to learn the residual \\(F(x) = \\text{output} - x\\), which is easier when the optimal transformation is close to identity. ResNets (Residual Networks) stacked over 100 layers using this trick, solving the degradation problem where deeper networks performed worse than shallower ones.</p> </li> <li> <p>CNNs build a feature hierarchy. Early layers detect edges and textures. Middle layers combine these into parts (eyes, wheels). Late layers recognise whole objects. Each layer's receptive field (the region of the input it can \"see\") grows with depth.</p> </li> <li> <p>Embeddings map discrete tokens (words, characters, item IDs) to dense vectors. An embedding layer is just a lookup table: a matrix \\(E\\) of shape (vocabulary size, embedding dimension). Looking up token \\(i\\) means selecting row \\(i\\) of \\(E\\). This is equivalent to multiplying by a one-hot vector, which is just a special case of matrix-vector multiplication (chapter 02). Embeddings are learned during training, so similar tokens end up with similar vectors.</p> </li> <li> <p>Tokenisation is the process of converting raw text into a sequence of tokens. Word-level tokenisation splits on spaces but cannot handle unseen words. Subword tokenisation (BPE, WordPiece, SentencePiece) breaks text into frequent subword units, balancing vocabulary size and coverage. The word \"unhappiness\" might become [\"un\", \"happiness\"] or [\"un\", \"happ\", \"iness\"].</p> </li> <li> <p>Recurrent Neural Networks (RNNs) process sequences one element at a time, maintaining a hidden state that carries information forward:</p> </li> </ul> \\[h_t = \\tanh(W_h h_{t-1} + W_x x_t + b)\\] <ul> <li> <p>The hidden state \\(h_t\\) is a compressed summary of everything the network has seen up to time \\(t\\). The same weights \\(W_h\\) and \\(W_x\\) are shared across all time steps (weight sharing, like CNNs share spatial weights).</p> </li> <li> <p>Vanilla RNNs struggle with long sequences because of vanishing gradients: the gradient signal from step \\(t\\) to step \\(t - k\\) passes through \\(k\\) multiplications by \\(W_h\\), and it shrinks (or explodes) exponentially.</p> </li> <li> <p>LSTM (Long Short-Term Memory) solves this by introducing a separate cell state \\(c_t\\) that flows through time with minimal interference. Three gates control what information enters, leaves, and persists:</p> </li> <li> <p>The forget gate decides what to erase from the cell state: \\(f_t = \\sigma(W_f [h_{t-1}, x_t] + b_f)\\)</p> </li> <li>The input gate decides what new information to write: \\(i_t = \\sigma(W_i [h_{t-1}, x_t] + b_i)\\), with candidate values \\(\\tilde{c}_t = \\tanh(W_c [h_{t-1}, x_t] + b_c)\\)</li> <li>The cell state updates: \\(c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t\\)</li> <li>The output gate decides what to expose: \\(o_t = \\sigma(W_o [h_{t-1}, x_t] + b_o)\\), and \\(h_t = o_t \\odot \\tanh(c_t)\\)</li> </ul> <p></p> <ul> <li> <p>The cell state acts like a conveyor belt: information can flow unchanged across many time steps (the forget gate stays close to 1), which solves the vanishing gradient problem for long-range dependencies.</p> </li> <li> <p>GRU (Gated Recurrent Unit) simplifies the LSTM by merging the cell state and hidden state into one, and using two gates instead of three: an update gate (combines forget and input) and a reset gate. GRUs have fewer parameters and often perform comparably to LSTMs.</p> </li> <li> <p>The fundamental limitation of RNNs (including LSTMs) is sequential processing: you must process token 1 before token 2 before token 3. This prevents parallelisation and creates an information bottleneck, as all context must squeeze through the fixed-size hidden state.</p> </li> <li> <p>Attention solves both problems. Instead of compressing the entire input into a fixed vector, attention lets the model look back at all input positions and decide which ones are relevant for the current output.</p> </li> <li> <p>The modern formulation uses queries, keys, and values (Q, K, V). Think of it like a library search: you have a query (what you are looking for), keys (labels on each book), and values (the actual book contents). You compare your query against all keys to figure out which values to retrieve.</p> </li> <li> <p>Scaled dot-product attention:</p> </li> </ul> \\[\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\\] <ul> <li> <p>\\(QK^T\\) computes the similarity between every query and every key. This is a matrix multiply (chapter 02), and the entries are dot products, which measure cosine similarity (chapter 01). Dividing by \\(\\sqrt{d_k}\\) prevents the dot products from becoming too large (which would make the softmax saturate and produce near-one-hot distributions with vanishing gradients). The softmax converts similarities to a probability distribution. Multiplying by \\(V\\) produces a weighted combination of values.</p> </li> <li> <p>Multi-head attention runs \\(h\\) parallel attention operations, each with different learned projections of Q, K, and V. This lets the model attend to information from different representation subspaces simultaneously. One head might attend to syntactic relationships while another attends to semantic ones. The outputs are concatenated and projected:</p> </li> </ul> \\[\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W^O\\] <ul> <li>The Transformer architecture (Vaswani et al., 2017) is built entirely from attention and feed-forward layers, with no recurrence. The encoder block repeats: multi-head self-attention, add and layer-norm, feed-forward network, add and layer-norm. The decoder block adds a masked self-attention (preventing the model from seeing future tokens) and a cross-attention layer that attends to the encoder output.</li> </ul> <p></p> <ul> <li>Positional encoding is necessary because attention is permutation-equivariant, meaning it treats the input as a set, not a sequence. Without position information, \"the cat sat on the mat\" and \"the mat sat on the cat\" would be identical. The original Transformer uses sinusoidal positional encodings:</li> </ul> \\[PE_{(pos, 2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d}}\\right), \\quad PE_{(pos, 2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d}}\\right)\\] <ul> <li> <p>Each position gets a unique vector that the model can use to distinguish positions. Modern models often use learned positional embeddings or relative positional encodings (RoPE, ALiBi) instead.</p> </li> <li> <p>Transformers process all tokens in parallel (the self-attention matrix \\(QK^T\\) is computed in one matrix multiply), which makes them much faster to train than RNNs on modern hardware. The tradeoff is that self-attention is \\(O(n^2)\\) in sequence length (every token attends to every other), while RNNs are \\(O(n)\\). This is why long-context models require special attention variants (sparse attention, linear attention, flash attention).</p> </li> <li> <p>Vision Transformers (ViT) apply the Transformer to images by splitting the image into fixed-size patches (e.g., 16x16), flattening each patch into a vector, and treating the patches as a sequence of tokens. A learnable [CLS] token is prepended, and its final representation is used for classification. Despite having no convolutional inductive biases, ViTs match or surpass CNNs when trained on enough data.</p> </li> <li> <p>MLP-Mixer is an even simpler architecture that replaces both attention and convolution with MLPs. It alternates between \"token-mixing\" MLPs (applied across spatial positions) and \"channel-mixing\" MLPs (applied across features). It performs competitively, suggesting that the key insight of modern architectures is not attention itself, but rather efficient mixing of information across tokens and features.</p> </li> <li> <p>Autoencoders learn compressed representations by training a network to reconstruct its own input. The encoder maps the input to a lower-dimensional bottleneck (the latent code), and the decoder maps it back:</p> </li> </ul> \\[z = f_{\\text{enc}}(x), \\quad \\hat{x} = f_{\\text{dec}}(z), \\quad \\mathcal{L} = \\|x - \\hat{x}\\|^2\\] <ul> <li> <p>The bottleneck forces the network to learn the most important features. Autoencoders are used for dimensionality reduction, denoising (train on noisy input, reconstruct clean output), and anomaly detection (high reconstruction error signals an unusual input).</p> </li> <li> <p>Variational Autoencoders (VAEs) add a probabilistic twist. Instead of encoding to a single point \\(z\\), the encoder outputs the parameters of a distribution (mean \\(\\mu\\) and variance \\(\\sigma^2\\) of a Gaussian). The latent code is sampled from this distribution: \\(z = \\mu + \\sigma \\odot \\epsilon\\), where \\(\\epsilon \\sim \\mathcal{N}(0, I)\\). This reparameterisation trick makes the sampling differentiable so gradients can flow through.</p> </li> <li> <p>The VAE loss has two terms:</p> </li> </ul> \\[\\mathcal{L} = \\underbrace{\\|x - \\hat{x}\\|^2}_{\\text{reconstruction}} + \\underbrace{D_{\\text{KL}}(q(z|x) \\| p(z))}_{\\text{regularisation}}\\] <ul> <li>The KL divergence term (from chapter 05) pushes the learned posterior \\(q(z|x)\\) toward the prior \\(p(z) = \\mathcal{N}(0, I)\\), ensuring the latent space is smooth and well-structured. You can then sample from the prior and decode to generate new data. This is what makes VAEs generative models.</li> </ul>"},{"location":"chapter%2006%3A%20machine%20learning/03.%20deep%20learning/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Build a simple MLP from scratch in JAX. Train it on a 2D classification problem (e.g., concentric circles) and visualise the decision boundary. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_circles\n\n# Data\nX, y = make_circles(n_samples=500, noise=0.1, factor=0.5, random_state=42)\nX, y = jnp.array(X), jnp.array(y, dtype=jnp.float32)\n\n# Initialise a 2-layer MLP: 2 -&gt; 16 -&gt; 16 -&gt; 1\ndef init_params(key):\n    k1, k2, k3 = jax.random.split(key, 3)\n    return {\n        'W1': jax.random.normal(k1, (2, 16)) * 0.5,\n        'b1': jnp.zeros(16),\n        'W2': jax.random.normal(k2, (16, 16)) * 0.5,\n        'b2': jnp.zeros(16),\n        'W3': jax.random.normal(k3, (16, 1)) * 0.5,\n        'b3': jnp.zeros(1),\n    }\n\ndef forward(params, x):\n    h = jnp.maximum(0, x @ params['W1'] + params['b1'])  # ReLU\n    h = jnp.maximum(0, h @ params['W2'] + params['b2'])   # ReLU\n    logit = (h @ params['W3'] + params['b3']).squeeze()\n    return jax.nn.sigmoid(logit)\n\ndef loss_fn(params, X, y):\n    pred = forward(params, X)\n    return -jnp.mean(y * jnp.log(pred + 1e-7) + (1 - y) * jnp.log(1 - pred + 1e-7))\n\ngrad_fn = jax.jit(jax.grad(loss_fn))\nparams = init_params(jax.random.PRNGKey(0))\nlr = 0.1\n\nfor step in range(2000):\n    grads = grad_fn(params, X, y)\n    params = {k: params[k] - lr * grads[k] for k in params}\n\n# Plot decision boundary\nxx, yy = jnp.meshgrid(jnp.linspace(-2, 2, 200), jnp.linspace(-2, 2, 200))\ngrid = jnp.column_stack([xx.ravel(), yy.ravel()])\nzz = forward(params, grid).reshape(xx.shape)\n\nplt.figure(figsize=(7, 6))\nplt.contourf(xx, yy, zz, levels=[0, 0.5, 1], alpha=0.3, colors=['#e74c3c', '#3498db'])\nplt.scatter(X[y==0,0], X[y==0,1], c='#e74c3c', s=10, label='Class 0')\nplt.scatter(X[y==1,0], X[y==1,1], c='#3498db', s=10, label='Class 1')\nplt.title(\"MLP Decision Boundary on Concentric Circles\")\nplt.legend(); plt.grid(alpha=0.3); plt.show()\n\nacc = jnp.mean((forward(params, X) &gt; 0.5) == y)\nprint(f\"Accuracy: {acc:.2%}\")\n</code></pre></p> </li> <li> <p>Implement 1D convolution from scratch. Apply a simple edge-detection filter to a signal and compare with the built-in <code>jnp.convolve</code>. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef conv1d(signal, kernel):\n    \"\"\"1D convolution (valid mode) from scratch.\"\"\"\n    n, k = len(signal), len(kernel)\n    output = jnp.zeros(n - k + 1)\n    for i in range(n - k + 1):\n        output = output.at[i].set(jnp.sum(signal[i:i+k] * kernel))\n    return output\n\n# Create a signal with a step function\nt = jnp.linspace(0, 4, 200)\nsignal = jnp.where(t &lt; 1, 0.0, jnp.where(t &lt; 2, 1.0, jnp.where(t &lt; 3, 0.5, 1.5)))\n\n# Edge detection kernel\nedge_kernel = jnp.array([-1.0, 0.0, 1.0])\n\n# Our implementation vs built-in\nour_output = conv1d(signal, edge_kernel)\njnp_output = jnp.convolve(signal, edge_kernel, mode='valid')\n\nfig, axes = plt.subplots(3, 1, figsize=(10, 6), sharex=True)\naxes[0].plot(t, signal, color='#3498db', linewidth=1.5)\naxes[0].set_title(\"Original Signal\"); axes[0].set_ylabel(\"Value\")\n\naxes[1].plot(t[:len(our_output)], our_output, color='#e74c3c', linewidth=1.5)\naxes[1].set_title(\"After Edge Detection (our conv1d)\"); axes[1].set_ylabel(\"Value\")\n\naxes[2].plot(t[:len(jnp_output)], jnp_output, color='#27ae60', linewidth=1.5, linestyle='--')\naxes[2].set_title(\"After Edge Detection (jnp.convolve)\"); axes[2].set_ylabel(\"Value\")\naxes[2].set_xlabel(\"t\")\n\nplt.tight_layout(); plt.show()\nprint(f\"Outputs match: {jnp.allclose(our_output, jnp_output)}\")\n</code></pre></p> </li> <li> <p>Implement scaled dot-product attention from scratch. Compute attention weights for a small example and visualise the attention matrix as a heatmap. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef scaled_dot_product_attention(Q, K, V):\n    \"\"\"Scaled dot-product attention.\"\"\"\n    d_k = Q.shape[-1]\n    scores = Q @ K.T / jnp.sqrt(d_k)\n    weights = jax.nn.softmax(scores, axis=-1)\n    output = weights @ V\n    return output, weights\n\n# Example: 4 tokens, embedding dim 8\nkey = jax.random.PRNGKey(42)\nk1, k2, k3 = jax.random.split(key, 3)\nseq_len, d_model = 4, 8\n\nQ = jax.random.normal(k1, (seq_len, d_model))\nK = jax.random.normal(k2, (seq_len, d_model))\nV = jax.random.normal(k3, (seq_len, d_model))\n\noutput, weights = scaled_dot_product_attention(Q, K, V)\n\nprint(f\"Q shape: {Q.shape}\")\nprint(f\"Attention weights shape: {weights.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"\\nAttention weights (rows sum to 1):\")\nprint(weights)\nprint(f\"Row sums: {weights.sum(axis=-1)}\")\n\n# Visualise attention\nfig, ax = plt.subplots(figsize=(5, 4))\nim = ax.imshow(weights, cmap='Blues', vmin=0, vmax=1)\nax.set_xlabel(\"Key position\"); ax.set_ylabel(\"Query position\")\nax.set_title(\"Attention Weights\")\ntokens = ['tok 0', 'tok 1', 'tok 2', 'tok 3']\nax.set_xticks(range(4)); ax.set_xticklabels(tokens)\nax.set_yticks(range(4)); ax.set_yticklabels(tokens)\nfor i in range(4):\n    for j in range(4):\n        ax.text(j, i, f\"{weights[i,j]:.2f}\", ha='center', va='center', fontsize=10)\nplt.colorbar(im); plt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Build a simple autoencoder that compresses 2D data through a 1D bottleneck and reconstructs it. Visualise the latent space and reconstructions. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\n\n# Data\nX, _ = make_moons(n_samples=500, noise=0.05, random_state=42)\nX = jnp.array(X)\n\n# Autoencoder: 2 -&gt; 8 -&gt; 1 -&gt; 8 -&gt; 2\ndef init_ae(key):\n    k1, k2, k3, k4 = jax.random.split(key, 4)\n    return {\n        'enc_W1': jax.random.normal(k1, (2, 8)) * 0.5, 'enc_b1': jnp.zeros(8),\n        'enc_W2': jax.random.normal(k2, (8, 1)) * 0.5, 'enc_b2': jnp.zeros(1),\n        'dec_W1': jax.random.normal(k3, (1, 8)) * 0.5, 'dec_b1': jnp.zeros(8),\n        'dec_W2': jax.random.normal(k4, (8, 2)) * 0.5, 'dec_b2': jnp.zeros(2),\n    }\n\ndef encode(p, x):\n    h = jnp.tanh(x @ p['enc_W1'] + p['enc_b1'])\n    return h @ p['enc_W2'] + p['enc_b2']\n\ndef decode(p, z):\n    h = jnp.tanh(z @ p['dec_W1'] + p['dec_b1'])\n    return h @ p['dec_W2'] + p['dec_b2']\n\ndef ae_loss(p, X):\n    z = encode(p, X)\n    X_hat = decode(p, z)\n    return jnp.mean((X - X_hat) ** 2)\n\ngrad_fn = jax.jit(jax.grad(ae_loss))\nparams = init_ae(jax.random.PRNGKey(0))\nlr = 0.01\n\nfor step in range(3000):\n    grads = grad_fn(params, X)\n    params = {k: params[k] - lr * grads[k] for k in params}\n\nz = encode(params, X)\nX_hat = decode(params, z)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\naxes[0].scatter(X[:,0], X[:,1], c=z.squeeze(), cmap='viridis', s=10)\naxes[0].set_title(\"Original Data (coloured by latent code)\")\naxes[1].scatter(X_hat[:,0], X_hat[:,1], c=z.squeeze(), cmap='viridis', s=10)\naxes[1].set_title(\"Reconstruction from 1D bottleneck\")\nfor ax in axes:\n    ax.set_aspect('equal'); ax.grid(alpha=0.3)\nplt.tight_layout(); plt.show()\n\nprint(f\"Reconstruction MSE: {ae_loss(params, X):.4f}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2006%3A%20machine%20learning/04.%20reinforcement%20learning/","title":"Reinforcement Learning","text":"<p>Reinforcement learning trains agents to make sequential decisions by maximising cumulative reward through trial and error. This file covers MDPs, value functions, Bellman equations, Q-learning, policy gradients, actor-critic methods, PPO, and RLHF -- the framework behind game-playing agents and language model alignment.</p> <ul> <li> <p>Supervised learning needs labelled data. Unsupervised learning finds patterns in unlabelled data. Reinforcement learning (RL) is different from both: an agent learns by interacting with an environment, taking actions, and receiving rewards. There are no correct labels; the agent must discover good behaviour through trial and error.</p> </li> <li> <p>Think of teaching a dog a new trick. You do not show it a dataset of correct behaviours. Instead, it tries things, you give treats for good actions, and over time it figures out what you want. RL formalises this process.</p> </li> <li> <p>The RL setup has five core components. The agent is the learner and decision-maker. The environment is everything outside the agent that it interacts with. At each time step, the agent observes a state \\(s_t\\), chooses an action \\(a_t\\), receives a reward \\(r_t\\), and transitions to a new state \\(s_{t+1}\\). The agent's goal is to maximise the total reward it collects over time.</p> </li> </ul> <p></p> <ul> <li> <p>A policy \\(\\pi\\) is the agent's strategy: a mapping from states to actions. A deterministic policy gives one action per state: \\(a = \\pi(s)\\). A stochastic policy gives a probability distribution over actions: \\(\\pi(a \\mid s)\\). The goal of RL is to find the optimal policy, the one that maximises expected cumulative reward.</p> </li> <li> <p>The mathematical framework for RL is the Markov Decision Process (MDP), defined by a tuple \\((S, A, P, R, \\gamma)\\): a set of states \\(S\\), a set of actions \\(A\\), transition probabilities \\(P(s' \\mid s, a)\\), a reward function \\(R(s, a)\\), and a discount factor \\(\\gamma\\).</p> </li> <li> <p>The Markov property (from chapter 05) says the future depends only on the current state, not on the history of how you got there: \\(P(s_{t+1} \\mid s_t, a_t, s_{t-1}, \\ldots) = P(s_{t+1} \\mid s_t, a_t)\\). This means the state contains all the information needed to make a decision.</p> </li> <li> <p>The discount factor \\(\\gamma \\in [0, 1)\\) determines how much the agent cares about future rewards versus immediate ones. The discounted return from time \\(t\\) is:</p> </li> </ul> \\[G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}\\] <ul> <li> <p>With \\(\\gamma = 0\\), the agent is completely myopic, caring only about the next reward. With \\(\\gamma\\) close to 1, the agent is far-sighted. The discount factor also ensures the sum converges (if rewards are bounded), which is important for mathematical well-definedness.</p> </li> <li> <p>Value functions estimate how good it is to be in a state (or to take an action in a state). The state-value function \\(V^\\pi(s)\\) is the expected return starting from state \\(s\\) and following policy \\(\\pi\\):</p> </li> </ul> \\[V^\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t \\mid s_t = s \\right]\\] <ul> <li>The action-value function \\(Q^\\pi(s, a)\\) is the expected return starting from state \\(s\\), taking action \\(a\\), and then following \\(\\pi\\):</li> </ul> \\[Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ G_t \\mid s_t = s, a_t = a \\right]\\] <ul> <li> <p>The relationship: \\(V^\\pi(s) = \\sum_a \\pi(a \\mid s) \\, Q^\\pi(s, a)\\). The state value is the average of action values, weighted by the policy.</p> </li> <li> <p>The Bellman equation expresses a recursive relationship: the value of a state equals the immediate reward plus the discounted value of the next state. For the state-value function:</p> </li> </ul> \\[V^\\pi(s) = \\sum_a \\pi(a \\mid s) \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a) + \\gamma \\, V^\\pi(s') \\right]\\] <ul> <li>For the optimal value function \\(V^{*}(s)\\), the agent always picks the best action:</li> </ul> \\[V^{*}(s) = \\max_a \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a) + \\gamma \\, V^{*}(s') \\right]\\] <ul> <li>Similarly, the Bellman optimality equation for \\(Q^{*}\\):</li> </ul> \\[Q^{*}(s, a) = \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a) + \\gamma \\max_{a'} Q^{*}(s', a') \\right]\\] <ul> <li> <p>Once you have \\(Q^{*}\\), the optimal policy is trivial: always pick the action with the highest Q-value: \\(\\pi^{*}(s) = \\arg\\max_a Q^{*}(s, a)\\).</p> </li> <li> <p>Dynamic programming methods solve MDPs when you know the transition probabilities and rewards (the full model). Policy evaluation computes \\(V^\\pi\\) for a given policy by iteratively applying the Bellman equation until convergence. Policy improvement takes the value function and constructs a better policy by acting greedily: \\(\\pi'(s) = \\arg\\max_a \\sum_{s'} P(s' \\mid s, a)[R(s,a) + \\gamma V^\\pi(s')]\\).</p> </li> <li> <p>Policy iteration alternates between evaluation and improvement until the policy stops changing. It is guaranteed to converge to the optimal policy.</p> </li> <li> <p>Value iteration combines both steps into one: it repeatedly applies the Bellman optimality equation until \\(V^{*}\\) converges, then extracts the policy.</p> </li> </ul> \\[V(s) \\leftarrow \\max_a \\sum_{s'} P(s' \\mid s, a) \\left[ R(s, a) + \\gamma \\, V(s') \\right]\\] <ul> <li> <p>Dynamic programming requires knowing \\(P(s' \\mid s, a)\\), which is often impractical. In most real problems, the agent does not know the environment's dynamics; it can only interact with it. This is where model-free methods come in.</p> </li> <li> <p>Temporal Difference (TD) learning learns from experience without knowing the model. The key idea is bootstrapping: instead of waiting until the end of an episode to compute the actual return \\(G_t\\), you estimate it using the current value function:</p> </li> </ul> \\[V(s_t) \\leftarrow V(s_t) + \\alpha \\left[ r_t + \\gamma \\, V(s_{t+1}) - V(s_t) \\right]\\] <ul> <li>The term in brackets is the TD error: the difference between the TD target (\\(r_t + \\gamma V(s_{t+1})\\)) and the current estimate \\(V(s_t)\\). If the TD error is positive, the state was better than expected, so we increase its value. If negative, we decrease it.</li> </ul> <p></p> <ul> <li> <p>TD learning updates after every single step (not after complete episodes), which makes it much more efficient than Monte Carlo methods. It also works in continuing (non-episodic) environments.</p> </li> <li> <p>SARSA (State-Action-Reward-State-Action) is TD learning applied to Q-values. The agent takes action \\(a\\) in state \\(s\\), observes reward \\(r\\) and next state \\(s'\\), then chooses next action \\(a'\\) according to its policy:</p> </li> </ul> \\[Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\, Q(s', a') - Q(s, a) \\right]\\] <ul> <li> <p>SARSA is on-policy: it updates using the action the agent actually takes, which includes exploration. This makes SARSA more conservative; it learns a policy that accounts for its own exploration noise.</p> </li> <li> <p>Q-learning is the most famous RL algorithm. It is like SARSA, but instead of using the action the agent actually takes, it uses the best possible action:</p> </li> </ul> \\[Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\\] <ul> <li> <p>Q-learning is off-policy: it learns the optimal Q-values regardless of the policy being followed. The agent can explore randomly while still learning the optimal action values. This makes Q-learning more aggressive and often faster to converge, but it can overestimate values.</p> </li> <li> <p>Exploration vs exploitation is the fundamental dilemma: should the agent exploit what it already knows (choose the action with the highest estimated value) or explore unknown actions (which might turn out to be better)?</p> </li> <li> <p>The simplest strategy is epsilon-greedy: with probability \\(\\epsilon\\), take a random action (explore); with probability \\(1 - \\epsilon\\), take the greedy action (exploit). A common schedule starts with high \\(\\epsilon\\) (lots of exploration) and decays it over time.</p> </li> <li> <p>Tabular methods (storing a value for each state-action pair in a table) work for small, discrete state spaces. For large or continuous state spaces, you need function approximation. Deep Q-Networks (DQN) use a neural network to approximate \\(Q(s, a; \\theta)\\), where \\(\\theta\\) are the network weights.</p> </li> <li> <p>DQN introduced two critical stabilisation techniques. Experience replay: instead of learning from consecutive transitions (which are highly correlated), store transitions in a replay buffer and sample random mini-batches for training. This breaks correlations and reuses data efficiently.</p> </li> <li> <p>Target network: use a separate, slowly-updated copy of the network to compute TD targets. Without this, the target moves every time you update the network, creating a \"chasing your own tail\" instability. The target network is updated periodically (hard update every \\(N\\) steps) or continuously (soft update: \\(\\theta^{-} \\leftarrow \\tau\\theta + (1-\\tau)\\theta^{-}\\)).</p> </li> <li> <p>The DQN loss is just MSE between predicted Q-values and TD targets:</p> </li> </ul> \\[\\mathcal{L}(\\theta) = \\mathbb{E} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta^{-}) - Q(s, a; \\theta) \\right)^2 \\right]\\] <ul> <li> <p>All the methods so far learn value functions and derive policies from them. Policy gradient methods take a different approach: they directly parameterise the policy \\(\\pi(a \\mid s; \\theta)\\) and optimise it by gradient ascent on expected return.</p> </li> <li> <p>The policy gradient theorem gives the gradient of expected return with respect to policy parameters:</p> </li> </ul> \\[\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi \\left[ \\nabla_\\theta \\log \\pi(a \\mid s; \\theta) \\cdot G_t \\right]\\] <ul> <li> <p>This says: increase the probability of actions that led to high returns, decrease the probability of actions that led to low returns. The log-probability gradient gives the direction to change the policy, and \\(G_t\\) scales how much to change it.</p> </li> <li> <p>REINFORCE is the simplest policy gradient algorithm. Run an episode, compute returns \\(G_t\\) for each step, and update:</p> </li> </ul> \\[\\theta \\leftarrow \\theta + \\alpha \\, \\nabla_\\theta \\log \\pi(a_t \\mid s_t; \\theta) \\cdot G_t\\] <ul> <li>REINFORCE has high variance because \\(G_t\\) is a noisy, single-sample estimate of the expected return. A common fix is to subtract a baseline (typically the average return or a learned value function) to reduce variance without introducing bias:</li> </ul> \\[\\theta \\leftarrow \\theta + \\alpha \\, \\nabla_\\theta \\log \\pi(a_t \\mid s_t; \\theta) \\cdot (G_t - b)\\] <ul> <li>Actor-Critic methods use two networks. The actor is the policy \\(\\pi(a \\mid s; \\theta)\\). The critic is a value function \\(V(s; \\phi)\\) that serves as the baseline. The advantage \\(A_t = r_t + \\gamma V(s_{t+1}) - V(s_t)\\) replaces \\(G_t - b\\):</li> </ul> \\[\\theta \\leftarrow \\theta + \\alpha \\, \\nabla_\\theta \\log \\pi(a_t \\mid s_t; \\theta) \\cdot A_t\\] <ul> <li>The critic is updated by minimising TD error, just like value-based methods. The actor is updated using the policy gradient, with the critic's advantage estimate reducing variance. This is the best of both worlds.</li> </ul> <p></p> <ul> <li> <p>PPO (Proximal Policy Optimization) is the most widely used policy gradient algorithm in practice. It addresses a key problem: if a policy update is too large, performance can collapse catastrophically.</p> </li> <li> <p>PPO uses a clipped surrogate objective. Let \\(r_t(\\theta) = \\frac{\\pi(a_t | s_t; \\theta)}{\\pi(a_t | s_t; \\theta_{\\text{old}})}\\) be the probability ratio between new and old policies. The loss is:</p> </li> </ul> \\[\\mathcal{L}^{\\text{CLIP}}(\\theta) = \\mathbb{E} \\left[ \\min\\!\\left( r_t(\\theta) A_t, \\; \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right]\\] <ul> <li> <p>The clipping (typically \\(\\epsilon = 0.2\\)) prevents the ratio from moving too far from 1, which keeps updates small and stable. If the advantage is positive (action was good), the ratio is capped at \\(1 + \\epsilon\\). If negative (action was bad), the ratio is capped at \\(1 - \\epsilon\\). This is simpler and more stable than earlier trust-region methods (TRPO).</p> </li> <li> <p>PPO is what was used to train ChatGPT-style models via RLHF (Reinforcement Learning from Human Feedback). In RLHF, a reward model is trained on human preference data (which of two outputs do humans prefer?), and then PPO optimises the language model's policy to maximise this learned reward.</p> </li> <li> <p>DPO (Direct Preference Optimization) simplifies RLHF by eliminating the reward model entirely. Instead of training a reward model and then running RL, DPO derives a closed-form loss that directly optimises the policy from preference data:</p> </li> </ul> \\[\\mathcal{L}_{\\text{DPO}}(\\theta) = -\\mathbb{E} \\left[ \\log \\sigma\\!\\left( \\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\text{ref}}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\text{ref}}(y_l \\mid x)} \\right) \\right]\\] <ul> <li> <p>Here \\(y_w\\) is the preferred (winning) response and \\(y_l\\) is the dispreferred (losing) response. DPO increases the relative probability of preferred outputs and is much simpler to implement than PPO-based RLHF.</p> </li> <li> <p>Two important distinctions in RL algorithms. On-policy vs off-policy: on-policy methods (SARSA, PPO) learn from data generated by the current policy; off-policy methods (Q-learning, DQN) can learn from data generated by any policy. Off-policy methods are more sample-efficient (they reuse old data) but can be less stable.</p> </li> <li> <p>Model-based vs model-free: model-free methods (everything discussed so far) learn values or policies directly from experience. Model-based methods learn a model of the environment (\\(P(s' \\mid s, a)\\) and \\(R(s, a)\\)) and use it for planning (imagining future trajectories without actually taking actions). Model-based methods are more sample-efficient but add the complexity of learning an accurate model.</p> </li> <li> <p>To summarise the RL landscape:</p> </li> </ul> Method Type Key Idea Strength Value Iteration DP, model-based Bellman optimality Exact solution (small MDPs) SARSA TD, on-policy Learn Q on-policy Conservative, safe Q-Learning TD, off-policy Learn Q*, greedy target Simple, effective DQN Deep, off-policy Neural Q + replay + target net Scales to high-dim states REINFORCE Policy gradient Gradient of log-prob * return Simple policy optimisation Actor-Critic PG + value Actor + critic for low variance Practical and flexible PPO PG, clipped Trust-region-like stability Industry standard DPO Direct preference Skip reward model Simpler RLHF"},{"location":"chapter%2006%3A%20machine%20learning/04.%20reinforcement%20learning/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement value iteration for a simple gridworld. Compute the optimal value function and extract the optimal policy. Visualise both as a heatmap and arrow plot. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# 4x4 gridworld: goal at (3,3), reward -1 per step, 0 at goal\ngrid_size = 4\ngamma = 0.99\ngoal = (3, 3)\n\n# Actions: up, down, left, right\nactions = [(-1, 0), (1, 0), (0, -1), (0, 1)]\naction_names = ['up', 'down', 'left', 'right']\naction_arrows = ['\\u2191', '\\u2193', '\\u2190', '\\u2192']\n\ndef step(s, a):\n    \"\"\"Deterministic transition.\"\"\"\n    ns = (max(0, min(grid_size-1, s[0]+a[0])),\n          max(0, min(grid_size-1, s[1]+a[1])))\n    return ns\n\n# Value iteration\nV = jnp.zeros((grid_size, grid_size))\nfor iteration in range(100):\n    V_new = jnp.array(V)\n    for i in range(grid_size):\n        for j in range(grid_size):\n            if (i, j) == goal:\n                continue\n            values = []\n            for a in actions:\n                ns = step((i, j), a)\n                values.append(-1 + gamma * float(V[ns[0], ns[1]]))\n            V_new = V_new.at[i, j].set(max(values))\n    if jnp.max(jnp.abs(V_new - V)) &lt; 1e-6:\n        print(f\"Converged in {iteration+1} iterations\")\n        break\n    V = V_new\n\n# Extract policy\npolicy = [['' for _ in range(grid_size)] for _ in range(grid_size)]\nfor i in range(grid_size):\n    for j in range(grid_size):\n        if (i, j) == goal:\n            policy[i][j] = 'G'\n            continue\n        best_a = max(range(4), key=lambda a: -1 + gamma * float(V[step((i,j), actions[a])[0], step((i,j), actions[a])[1]]))\n        policy[i][j] = action_arrows[best_a]\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\nim = axes[0].imshow(V, cmap='YlOrRd_r')\naxes[0].set_title(\"Optimal Value Function\")\nfor i in range(grid_size):\n    for j in range(grid_size):\n        axes[0].text(j, i, f\"{V[i,j]:.1f}\", ha='center', va='center', fontsize=10)\nplt.colorbar(im, ax=axes[0])\n\naxes[1].imshow(jnp.ones((grid_size, grid_size)), cmap='Greys', vmin=0, vmax=2)\naxes[1].set_title(\"Optimal Policy\")\nfor i in range(grid_size):\n    for j in range(grid_size):\n        axes[1].text(j, i, policy[i][j], ha='center', va='center', fontsize=18)\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Implement tabular Q-learning on a simple gridworld. Train the agent, plot the learning curve, and show the learned Q-values. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ngrid_size = 5\ngoal = (4, 4)\nactions = [(-1,0), (1,0), (0,-1), (0,1)]\n\n# Q-table\nQ = {}\nfor i in range(grid_size):\n    for j in range(grid_size):\n        Q[(i,j)] = [0.0] * 4\n\nalpha = 0.1\ngamma = 0.95\nepsilon = 1.0\nepsilon_decay = 0.995\nmin_epsilon = 0.01\n\ndef step(s, a_idx):\n    a = actions[a_idx]\n    ns = (max(0, min(grid_size-1, s[0]+a[0])),\n          max(0, min(grid_size-1, s[1]+a[1])))\n    r = 0.0 if ns == goal else -1.0\n    done = ns == goal\n    return ns, r, done\n\nkey = jax.random.PRNGKey(42)\nrewards_per_episode = []\n\nfor ep in range(500):\n    s = (0, 0)\n    total_reward = 0\n    for _ in range(100):\n        key, subkey = jax.random.split(key)\n        if float(jax.random.uniform(subkey)) &lt; epsilon:\n            key, subkey = jax.random.split(key)\n            a = int(jax.random.randint(subkey, (), 0, 4))\n        else:\n            a = max(range(4), key=lambda i: Q[s][i])\n\n        ns, r, done = step(s, a)\n        total_reward += r\n        # Q-learning update\n        Q[s][a] += alpha * (r + gamma * max(Q[ns]) - Q[s][a])\n        s = ns\n        if done:\n            break\n    rewards_per_episode.append(total_reward)\n    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n\nplt.figure(figsize=(8, 4))\n# Smooth the curve\nwindow = 20\nsmoothed = [sum(rewards_per_episode[max(0,i-window):i+1])/min(i+1, window)\n            for i in range(len(rewards_per_episode))]\nplt.plot(smoothed, color='#3498db', linewidth=1.5)\nplt.xlabel(\"Episode\"); plt.ylabel(\"Total Reward (smoothed)\")\nplt.title(\"Q-Learning on Gridworld\")\nplt.grid(alpha=0.3); plt.show()\n\n# Show learned policy\narrow = ['\\u2191', '\\u2193', '\\u2190', '\\u2192']\nprint(\"Learned policy:\")\nfor i in range(grid_size):\n    row = \"\"\n    for j in range(grid_size):\n        if (i,j) == goal:\n            row += \" G \"\n        else:\n            row += f\" {arrow[max(range(4), key=lambda a: Q[(i,j)][a])]} \"\n    print(row)\n</code></pre></p> </li> <li> <p>Implement REINFORCE on a multi-armed bandit problem. Show how the policy evolves over training to favour the best arm. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# 5-armed bandit with different expected rewards\ntrue_rewards = jnp.array([0.2, 0.5, 0.8, 0.3, 0.1])\nn_arms = len(true_rewards)\n\n# Policy: softmax over logits\nlogits = jnp.zeros(n_arms)\nlr = 0.1\nkey = jax.random.PRNGKey(42)\n\npolicy_history = []\nreward_history = []\n\nfor step in range(2000):\n    probs = jax.nn.softmax(logits)\n    policy_history.append(probs)\n\n    # Sample action\n    key, subkey = jax.random.split(key)\n    action = jax.random.choice(subkey, n_arms, p=probs)\n\n    # Get reward (Bernoulli)\n    key, subkey = jax.random.split(key)\n    reward = float(jax.random.uniform(subkey) &lt; true_rewards[action])\n    reward_history.append(reward)\n\n    # REINFORCE update\n    # grad log pi(a) = e_a - probs (for softmax parameterisation)\n    grad_log_pi = -probs.at[action].add(1.0)  # one-hot(a) - probs\n    logits = logits + lr * reward * grad_log_pi\n\npolicy_history = jnp.stack(policy_history)\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\ncolors = ['#3498db', '#e74c3c', '#27ae60', '#9b59b6', '#f39c12']\nfor i in range(n_arms):\n    axes[0].plot(policy_history[:, i], color=colors[i],\n                 label=f'Arm {i} (true={true_rewards[i]:.1f})', linewidth=1.5)\naxes[0].set_xlabel(\"Step\"); axes[0].set_ylabel(\"P(arm)\")\naxes[0].set_title(\"Policy Evolution (REINFORCE)\")\naxes[0].legend(fontsize=8); axes[0].grid(alpha=0.3)\n\n# Smoothed reward\nwindow = 50\nsmoothed = [sum(reward_history[max(0,i-window):i+1])/min(i+1,window)\n            for i in range(len(reward_history))]\naxes[1].plot(smoothed, color='#27ae60', linewidth=1.5)\naxes[1].axhline(y=0.8, color='#e74c3c', linestyle='--', alpha=0.5, label='Best arm')\naxes[1].set_xlabel(\"Step\"); axes[1].set_ylabel(\"Avg Reward\")\naxes[1].set_title(\"Reward Over Time\"); axes[1].legend()\naxes[1].grid(alpha=0.3)\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2006%3A%20machine%20learning/05.%20distributed%20deep%20learning/","title":"Distributed Deep Learning","text":"<p>Distributed training spreads computation across multiple GPUs and machines to train models that are too large or too slow for a single device. This file covers mixed precision, data parallelism, model parallelism, pipeline parallelism, ZeRO, FSDP, tensor parallelism, and communication primitives like all-reduce -- essential for training LLMs at scale.</p> <ul> <li> <p>Training a large neural network on a single GPU eventually hits a wall. The model might not fit in memory, or training might take months. Distributed training spreads the work across multiple devices (GPUs, TPUs, or entire machines) to train faster and train bigger models. This file covers the techniques that make that possible.</p> </li> <li> <p>To understand why distribution matters, start with the computational cost of training. A single forward pass through a dense layer with \\(d_{\\text{in}}\\) inputs and \\(d_{\\text{out}}\\) outputs on a batch of \\(B\\) examples requires roughly \\(2 \\cdot B \\cdot d_{\\text{in}} \\cdot d_{\\text{out}}\\) FLOPs (floating-point operations): one multiply and one add for each element of the output matrix. The backward pass costs roughly twice the forward pass (computing gradients with respect to both the inputs and the weights), so one training step on a dense layer is about \\(6 \\cdot B \\cdot d_{\\text{in}} \\cdot d_{\\text{out}}\\) FLOPs.</p> </li> <li> <p>For a transformer layer with hidden dimension \\(d\\), the self-attention block involves four projections (Q, K, V, and output) each costing \\(O(B \\cdot n \\cdot d^2)\\) FLOPs (where \\(n\\) is the sequence length), plus the attention matrix computation at \\(O(B \\cdot n^2 \\cdot d)\\). The feed-forward block has two dense layers, typically expanding to \\(4d\\) and back: \\(O(B \\cdot n \\cdot 8d^2)\\). Total per layer: roughly \\(O(B \\cdot n \\cdot 12d^2 + B \\cdot n^2 \\cdot d)\\). Multiply by the number of layers and you see why training GPT-scale models requires thousands of GPU-hours.</p> </li> <li> <p>The memory wall is often the tighter constraint. During training, GPU memory must hold four things simultaneously:</p> </li> </ul> <p></p> <ul> <li>Parameters: the model weights. A 7-billion parameter model in FP32 (4 bytes per parameter) needs 28 GB just for weights.</li> <li>Gradients: same size as the parameters. Another 28 GB.</li> <li>Optimizer states: Adam maintains two additional buffers (first and second moment estimates), each the size of the parameters. These are kept in FP32 for numerical stability, even when the model uses lower precision. For our 7B model, that is \\(2 \\times 28 = 56\\) GB.</li> <li> <p>Activations: intermediate values saved during the forward pass for use in the backward pass. The size depends on batch size, sequence length, and model width. This is often the largest component and grows linearly with batch size.</p> </li> <li> <p>For our 7B model with FP32 Adam: 28 (params) + 28 (grads) + 56 (optimizer) = 112 GB, before we even count activations. A single 80 GB A100 GPU cannot hold this. This is why distributed strategies are essential.</p> </li> <li> <p>Mixed precision training is the first line of defence. Instead of storing everything in FP32 (32-bit floating point), you train using FP16 or BF16 (16-bit) for the forward and backward passes, while keeping a master copy of weights in FP32 for the optimizer update.</p> </li> <li> <p>FP16 has high precision (10-bit mantissa) but a limited range, which can cause overflow/underflow. Loss scaling (multiplying the loss by a large factor before the backward pass, then dividing gradients by the same factor) mitigates this.</p> </li> <li> <p>BF16 (brain float) has the same exponent range as FP32 (8-bit exponent) but less precision (7-bit mantissa). It almost never overflows and rarely needs loss scaling, making it simpler to use. BF16 is the default for modern transformer training.</p> </li> <li> <p>Mixed precision roughly halves the memory for activations and gradients (the dominant costs during forward/backward passes), while keeping optimizer states in FP32 for numerical stability.</p> </li> <li> <p>Data parallelism is the simplest distributed strategy. You replicate the entire model on \\(N\\) GPUs, split each mini-batch into \\(N\\) equal chunks, and send one chunk to each GPU. Each GPU runs the forward and backward pass on its chunk independently. Then the gradients are averaged across all GPUs (using an all-reduce operation), and each GPU updates its local copy of the model.</p> </li> <li> <p>From the model's perspective, this is equivalent to training with a mini-batch that is \\(N\\) times larger. If each GPU processes a batch of size \\(B\\), the effective batch size is \\(N \\cdot B\\).</p> </li> </ul> <p></p> <ul> <li> <p>The gradient averaging can be done synchronously or asynchronously. Synchronous SGD waits for all GPUs to finish before averaging, ensuring mathematical equivalence to single-GPU training with a larger batch. The downside is that the slowest GPU (the \"straggler\") holds everyone up.</p> </li> <li> <p>Asynchronous SGD lets each GPU update a shared parameter server independently, without waiting. This eliminates the straggler problem but introduces \"stale gradients\": a GPU might compute gradients based on slightly outdated parameters. Stale gradients add noise and can slow convergence. In practice, synchronous SGD with efficient communication is preferred.</p> </li> <li> <p>Gradient accumulation is a software trick for simulating larger batch sizes on limited hardware. Instead of doing one update per mini-batch, you run several forward/backward passes and accumulate the gradients, then do one update. This gives the same result as a larger batch without needing more GPU memory for activations (only one mini-batch of activations is in memory at a time).</p> </li> <li> <p>When the model itself is too large to fit on a single GPU, you need model parallelism. There are two main flavours.</p> </li> <li> <p>Tensor parallelism splits individual layers across GPUs. A large matrix multiply \\(Y = XW\\) can be split column-wise: partition \\(W\\) into \\([W_1, W_2]\\) across two GPUs, compute \\(Y_1 = XW_1\\) and \\(Y_2 = XW_2\\) in parallel, then concatenate. This works for attention projections and feed-forward layers. It requires fast communication between GPUs (typically NVLink within a node) because partial results must be combined at every layer.</p> </li> <li> <p>Pipeline parallelism assigns different layers to different GPUs. GPU 0 runs layers 1-4, GPU 1 runs layers 5-8, and so on. Data flows through the pipeline like an assembly line. The naive approach has a \"pipeline bubble\": while GPU 0 processes the forward pass for micro-batch 1, GPUs 1-3 sit idle. Micro-batching mitigates this by splitting the mini-batch into smaller micro-batches that flow through the pipeline in sequence, keeping all GPUs busy most of the time.</p> </li> <li> <p>Hybrid parallelism combines data, tensor, and pipeline parallelism. A typical large-model setup might use tensor parallelism within a node (8 GPUs connected by fast NVLink), pipeline parallelism across nodes, and data parallelism across groups of nodes. This is how models like GPT-4 and Llama are trained.</p> </li> <li> <p>The efficiency of distributed training depends heavily on communication. The key operation is all-reduce: given a value on each of \\(N\\) GPUs, compute the sum (or average) and distribute the result to all GPUs.</p> </li> <li> <p>A naive all-reduce sends all data to one GPU, sums it, and broadcasts back. This is \\(O(N)\\) in communication and creates a bottleneck at the root.</p> </li> <li> <p>Ring all-reduce is much more efficient. Arrange the \\(N\\) GPUs in a ring. Each GPU splits its data into \\(N\\) chunks. In \\(N - 1\\) steps, each GPU sends one chunk to its neighbour and receives a chunk from its other neighbour, accumulating partial sums. After another \\(N - 1\\) steps, the full sum is propagated to all GPUs. Total data transferred per GPU: \\(2(N-1)/N\\) times the data size, which approaches \\(2\\times\\) as \\(N\\) grows. Crucially, this does not increase with \\(N\\), making it bandwidth-optimal.</p> </li> </ul> <p></p> <ul> <li> <p>Parameter servers are an alternative architecture where dedicated server nodes hold the model parameters. Workers compute gradients and send them to the server, which updates parameters and sends them back. This is simpler but can create communication bottlenecks at the server.</p> </li> <li> <p>NCCL (NVIDIA Collective Communications Library) is the standard library for GPU-to-GPU communication. It provides optimised implementations of all-reduce, all-gather, broadcast, and other collective operations, automatically choosing the best algorithm for the network topology.</p> </li> <li> <p>Scaling laws describe how model performance improves with compute, data, and model size. The original Kaplan et al. (2020) scaling laws found that loss decreases as a power law with each:</p> </li> </ul> \\[L(N) \\propto N^{-\\alpha_N}, \\quad L(D) \\propto D^{-\\alpha_D}, \\quad L(C) \\propto C^{-\\alpha_C}\\] <ul> <li> <p>where \\(N\\) is the number of parameters, \\(D\\) is the dataset size, and \\(C\\) is the compute budget.</p> </li> <li> <p>The Chinchilla scaling laws (Hoffmann et al., 2022) showed that most models were undertrained: for a given compute budget, you should train a smaller model on more data than previously thought. The optimal ratio is roughly 20 tokens per parameter. A 7B model should see about 140B tokens, not the 300B tokens that Llama 1 used with a 65B model. This finding shifted the field toward \"compute-optimal\" training.</p> </li> <li> <p>Mixture of Experts (MoE) is an architecture that scales model capacity without proportionally scaling compute. Instead of one feed-forward network per transformer layer, you have \\(N\\) \"expert\" networks (each a standard FFN). A gating network (router) examines each token and sends it to the top-\\(K\\) experts (typically \\(K = 1\\) or \\(K = 2\\)).</p> </li> </ul> <p></p> <ul> <li> <p>The total parameter count is much larger (because you have \\(N\\) experts), but the FLOPs per token stay roughly constant (because only \\(K\\) experts activate per token). For example, Mixtral 8x7B has 47B total parameters but only uses about 13B per forward pass, giving the performance of a much larger model at the cost of a smaller one.</p> </li> <li> <p>MoE introduces challenges. Load balancing: if the router sends most tokens to the same expert, the others are wasted. An auxiliary loss encourages uniform routing. Communication: different experts may live on different GPUs, so routing tokens requires all-to-all communication, which is expensive.</p> </li> <li> <p>Fault tolerance is critical when training runs last weeks or months on thousands of GPUs. If a single GPU fails, you do not want to lose all progress. Checkpointing periodically saves model weights, optimizer states, and the training state (learning rate, step count, data position) to disk. If a failure occurs, you restart from the last checkpoint.</p> </li> <li> <p>Gradient checkpointing (also called activation recomputation) is a memory optimisation, not a fault-tolerance mechanism. During the forward pass, instead of saving all activations for the backward pass, you only save activations at certain checkpoints. During the backward pass, you recompute the missing activations from the checkpoints. This trades compute for memory: it increases the forward-pass cost by roughly 33% but can reduce activation memory by a factor of \\(\\sqrt{L}\\) (where \\(L\\) is the number of layers).</p> </li> <li> <p>Putting it all together, training a frontier model combines all of these techniques: BF16 mixed precision, data parallelism across thousands of GPUs with ring all-reduce, tensor parallelism within nodes, pipeline parallelism across nodes, gradient checkpointing to reduce memory, MoE for parameter efficiency, and regular checkpointing for fault tolerance. The systems engineering is as challenging as the algorithm design.</p> </li> <li> <p>To summarise the distributed training toolkit:</p> </li> </ul> Technique What It Does Tradeoff Mixed precision (BF16) Halves memory for activations/grads Slight numerical differences Data parallelism Scales batch size across GPUs Communication overhead for gradient sync Tensor parallelism Splits layers across GPUs Requires fast interconnect Pipeline parallelism Splits model stages across GPUs Pipeline bubble (wasted compute) Gradient accumulation Simulates large batches Slower (multiple forward/backward passes) Gradient checkpointing Reduces activation memory ~33% more compute Ring all-reduce Efficient gradient averaging Bandwidth-limited for large models MoE More capacity, same FLOPs Load balancing, routing complexity Scaling laws Guides compute allocation Empirical, may not hold at all scales"},{"location":"chapter%2006%3A%20machine%20learning/05.%20distributed%20deep%20learning/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Compute the FLOPs and memory requirements for a transformer layer. Given hidden dimension \\(d\\), sequence length \\(n\\), batch size \\(B\\), and number of layers, estimate the total training cost. <pre><code>import jax.numpy as jnp\n\ndef transformer_layer_flops(d, n, B):\n    \"\"\"Approximate FLOPs for one transformer layer forward pass.\"\"\"\n    # QKV projections: 3 * (B * n * d * d) * 2 (multiply-add)\n    qkv_flops = 3 * 2 * B * n * d * d\n    # Attention: (B * n * n * d) * 2 for QK^T, (B * n * n * d) * 2 for attn*V\n    attn_flops = 2 * 2 * B * n * n * d\n    # Output projection: (B * n * d * d) * 2\n    out_flops = 2 * B * n * d * d\n    # FFN: two layers, d-&gt;4d and 4d-&gt;d: 2 * (B * n * d * 4d) * 2\n    ffn_flops = 2 * 2 * B * n * d * 4 * d\n    return qkv_flops + attn_flops + out_flops + ffn_flops\n\ndef transformer_layer_memory(d, n, B, dtype_bytes=2):\n    \"\"\"Approximate activation memory (bytes) for one layer.\"\"\"\n    # QKV: 3 * B * n * d\n    qkv_mem = 3 * B * n * d * dtype_bytes\n    # Attention weights: B * heads * n * n (approx B * n * n * sizeof)\n    attn_mem = B * n * n * dtype_bytes\n    # FFN intermediate: B * n * 4d\n    ffn_mem = B * n * 4 * d * dtype_bytes\n    return qkv_mem + attn_mem + ffn_mem\n\n# Example: GPT-2 scale\nd, n, B, L = 1024, 1024, 8, 24\nfwd_flops = transformer_layer_flops(d, n, B)\ntotal_flops = 3 * L * fwd_flops  # 3x for forward + backward\nact_mem = L * transformer_layer_memory(d, n, B)\nparam_count = L * (12 * d * d + 13 * d)  # approximate\n\nprint(f\"Model: d={d}, n={n}, B={B}, L={L}\")\nprint(f\"Parameters: {param_count / 1e6:.0f}M\")\nprint(f\"FLOPs per step: {total_flops / 1e12:.2f} TFLOPs\")\nprint(f\"Activation memory: {act_mem / 1e9:.2f} GB (BF16)\")\nprint(f\"Parameter memory (FP32): {param_count * 4 / 1e9:.2f} GB\")\nprint(f\"Adam optimizer memory: {param_count * 8 / 1e9:.2f} GB\")\nprint(f\"Total training memory: {(param_count * 16 + act_mem) / 1e9:.2f} GB\")\n</code></pre></p> </li> <li> <p>Simulate data-parallel training. Split a dataset across multiple \"virtual GPUs,\" compute gradients independently, average them, and verify the result matches single-GPU training. <pre><code>import jax\nimport jax.numpy as jnp\n\n# Simple linear model: y = wx + b\nkey = jax.random.PRNGKey(0)\nX = jax.random.normal(key, (64, 4))\nw_true = jnp.array([1.0, -2.0, 3.0, 0.5])\ny = X @ w_true + 0.1 * jax.random.normal(key, (64,))\n\ndef loss_fn(w, X, y):\n    return jnp.mean((X @ w - y) ** 2)\n\ngrad_fn = jax.grad(loss_fn)\n\n# Single GPU: full batch gradient\nw = jnp.zeros(4)\ngrad_single = grad_fn(w, X, y)\n\n# Data parallel: split across 4 \"GPUs\"\nn_gpus = 4\nchunk_size = len(X) // n_gpus\ngrads = []\nfor i in range(n_gpus):\n    X_chunk = X[i*chunk_size:(i+1)*chunk_size]\n    y_chunk = y[i*chunk_size:(i+1)*chunk_size]\n    grads.append(grad_fn(w, X_chunk, y_chunk))\n\n# All-reduce: average gradients\ngrad_parallel = jnp.mean(jnp.stack(grads), axis=0)\n\nprint(\"Single-GPU gradient:\", grad_single)\nprint(\"Data-parallel gradient (avg):\", grad_parallel)\nprint(f\"Match: {jnp.allclose(grad_single, grad_parallel, atol=1e-5)}\")\n\n# Train both and compare\nw_single, w_parallel = jnp.zeros(4), jnp.zeros(4)\nlr = 0.1\nfor step in range(100):\n    w_single = w_single - lr * grad_fn(w_single, X, y)\n\n    grads = [grad_fn(w_parallel, X[i*chunk_size:(i+1)*chunk_size],\n                     y[i*chunk_size:(i+1)*chunk_size]) for i in range(n_gpus)]\n    avg_grad = jnp.mean(jnp.stack(grads), axis=0)\n    w_parallel = w_parallel - lr * avg_grad\n\nprint(f\"\\nAfter 100 steps:\")\nprint(f\"Single-GPU weights: {w_single}\")\nprint(f\"Data-parallel weights: {w_parallel}\")\nprint(f\"Max difference: {jnp.max(jnp.abs(w_single - w_parallel)):.2e}\")\n</code></pre></p> </li> <li> <p>Implement a simple Mixture of Experts layer. Create a gating network that routes tokens to top-K experts and combine their outputs. <pre><code>import jax\nimport jax.numpy as jnp\n\ndef expert_fn(x, W1, b1, W2, b2):\n    \"\"\"Simple 2-layer FFN expert.\"\"\"\n    h = jnp.maximum(0, x @ W1 + b1)  # ReLU\n    return h @ W2 + b2\n\ndef moe_layer(x, gate_W, experts_params, top_k=2):\n    \"\"\"\n    MoE forward pass.\n    x: (batch, d_model)\n    gate_W: (d_model, n_experts)\n    experts_params: list of (W1, b1, W2, b2) per expert\n    \"\"\"\n    n_experts = len(experts_params)\n\n    # Gating: compute routing scores\n    gate_logits = x @ gate_W  # (batch, n_experts)\n    gate_probs = jax.nn.softmax(gate_logits, axis=-1)\n\n    # Top-K selection\n    top_k_indices = jnp.argsort(-gate_probs, axis=-1)[:, :top_k]\n    top_k_probs = jnp.take_along_axis(gate_probs, top_k_indices, axis=-1)\n    # Renormalise\n    top_k_probs = top_k_probs / jnp.sum(top_k_probs, axis=-1, keepdims=True)\n\n    # Compute expert outputs (simplified: run all experts, mask later)\n    expert_outputs = jnp.stack([\n        expert_fn(x, *experts_params[i]) for i in range(n_experts)\n    ], axis=1)  # (batch, n_experts, d_model)\n\n    # Gather top-K expert outputs and weight them\n    batch_idx = jnp.arange(x.shape[0])[:, None]\n    selected_outputs = expert_outputs[batch_idx, top_k_indices]  # (batch, top_k, d_model)\n    output = jnp.sum(selected_outputs * top_k_probs[:, :, None], axis=1)\n\n    return output, gate_probs\n\n# Setup\nkey = jax.random.PRNGKey(42)\nbatch, d_model, d_ff, n_experts = 8, 16, 32, 4\n\n# Initialise experts\nexperts_params = []\nfor i in range(n_experts):\n    k1, k2, key = jax.random.split(key, 3)[0], jax.random.split(key, 3)[1], jax.random.split(key, 3)[2]\n    experts_params.append((\n        jax.random.normal(k1, (d_model, d_ff)) * 0.1,\n        jnp.zeros(d_ff),\n        jax.random.normal(k2, (d_ff, d_model)) * 0.1,\n        jnp.zeros(d_model),\n    ))\n\nkey, subkey = jax.random.split(key)\ngate_W = jax.random.normal(subkey, (d_model, n_experts)) * 0.1\nx = jax.random.normal(key, (batch, d_model))\n\noutput, gate_probs = moe_layer(x, gate_W, experts_params, top_k=2)\n\nprint(f\"Input shape: {x.shape}\")\nprint(f\"Output shape: {output.shape}\")\nprint(f\"Gate probabilities (first sample): {gate_probs[0]}\")\nprint(f\"Expert usage (avg across batch):\")\nfor i in range(n_experts):\n    usage = jnp.mean(gate_probs[:, i])\n    print(f\"  Expert {i}: {usage:.3f}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2007%3A%20computational%20linguistics/01.%20linguistic%20foundations/","title":"Linguistic Foundations","text":"<p>Linguistics provides the structural vocabulary that NLP systems implicitly learn and exploit. This file covers morphology, syntax, semantics, pragmatics, phonology, constituency and dependency parsing, and the distributional hypothesis -- the human-language science that grounds tokenisation, grammar, and meaning in AI.</p> <ul> <li> <p>Before we can build systems that understand or generate language, we need to understand how language itself works. </p> </li> <li> <p>Linguistics is the scientific study of language, and it provides the conceptual vocabulary that NLP borrows from constantly. </p> </li> <li> <p>Even modern neural models, which learn language from raw data, implicitly rediscover many of the structures that linguists have catalogued for decades.</p> </li> <li> <p>Language has structure at every level: the sounds that make up words, the parts that make up words, the rules that combine words into sentences, the meaning those sentences carry, and the way context shapes interpretation. We will work through each level from the bottom up.</p> </li> <li> <p>Morphology is the study of the internal structure of words. Words are not atomic; they are built from smaller meaningful units called morphemes.</p> </li> <li> <p>The word \"unhappiness\" contains three morphemes: \"un-\" (a prefix meaning \"not\"), \"happy\" (the root), and \"-ness\" (a suffix that turns an adjective into a noun). Each morpheme contributes to the meaning.</p> </li> <li> <p>A root (or stem) is the core morpheme that carries the primary meaning. \"Happy,\" \"run,\" \"compute\" are roots. </p> </li> <li> <p>An affix is a morpheme that attaches to a root to modify it. </p> </li> <li> <p>English has prefixes (before the root: un-, re-, pre-) and suffixes (after: -ing, -ed, -tion). Some languages also have infixes (inserted inside the root) and circumfixes (wrapping around).</p> </li> </ul> <p></p> <ul> <li> <p>There are two kinds of morphological processes. Inflection changes the grammatical properties of a word without changing its core meaning or part of speech: \"run\" becomes \"runs\" (third person), \"running\" (progressive), \"ran\" (past tense). The word is still a verb meaning the same thing.</p> </li> <li> <p>Derivation creates a new word, often changing the part of speech: \"happy\" (adjective) becomes \"happiness\" (noun), \"compute\" (verb) becomes \"computation\" (noun) becomes \"computational\" (adjective). Each derivation shifts meaning and grammatical category.</p> </li> <li> <p>Languages vary enormously in morphological complexity. English is relatively analytic (few morphemes per word, relying on word order). </p> </li> <li> <p>Turkish and Finnish are agglutinative (words can contain many morphemes strung together). Arabic and Hebrew use templatic morphology (roots are consonant skeletons like k-t-b for \"write,\" and vowel patterns are inserted to create different words: kitab \"book,\" kataba \"he wrote,\" maktub \"written\").</p> </li> <li> <p>Morphology matters for NLP because it affects tokenisation. A word-level tokeniser treats \"run,\" \"runs,\" \"running,\" and \"ran\" as four unrelated symbols. </p> </li> <li> <p>A morphologically-aware system recognises they share a root. Subword tokenisation (BPE, WordPiece), which we will cover in file 02, is a statistical approximation to morphological analysis.</p> </li> <li> <p>Syntax is the study of how words combine into phrases and sentences. Every language has rules governing word order and structure; violating them produces gibberish. </p> </li> <li> <p>\"The cat sat on the mat\" is grammatical English; \"Mat the on sat cat the\" is not.</p> </li> <li> <p>There are two main frameworks for describing syntactic structure.</p> </li> <li> <p>Phrase structure grammar (also called constituency grammar) says sentences are built by nesting phrases inside phrases. A sentence (S) consists of a noun phrase (NP) and a verb phrase (VP). </p> </li> <li> <p>A noun phrase might be a determiner (Det) followed by a noun (N). A verb phrase might be a verb (V) followed by a noun phrase. These rules build a tree:</p> </li> </ul> <p></p> <ul> <li> <p>This tree is called a constituency tree (or parse tree). Each internal node is a phrase type, each leaf is a word. The tree captures the hierarchical grouping: \"on the mat\" is a unit (prepositional phrase), \"sat on the mat\" is a unit (verb phrase), and the whole thing is a sentence.</p> </li> <li> <p>A context-free grammar (CFG) formalises these rules. It consists of a set of production rules, each of the form \\(A \\to \\alpha\\), where \\(A\\) is a non-terminal symbol (a phrase type like NP or VP) and \\(\\alpha\\) is a sequence of terminals (words) and non-terminals. For example:</p> </li> </ul> <pre><code>S  \u2192 NP VP\nNP \u2192 Det N\nNP \u2192 Det N PP\nVP \u2192 V NP\nVP \u2192 V PP\nPP \u2192 P NP\nDet \u2192 \"the\" | \"a\"\nN  \u2192 \"cat\" | \"mat\" | \"dog\"\nV  \u2192 \"sat\" | \"chased\"\nP  \u2192 \"on\" | \"under\"\n</code></pre> <ul> <li> <p>Starting from S and repeatedly applying rules, you can generate all sentences the grammar allows. Parsing is the reverse: given a sentence, find the tree (or trees) that produced it. A sentence with multiple valid parse trees is syntactically ambiguous. \"I saw the man with the telescope\" has two parses: I used a telescope to see the man, or I saw a man who had a telescope.</p> </li> <li> <p>Dependency grammar takes a different perspective. Instead of phrase nesting, it describes direct relationships between words. Each word in a sentence depends on exactly one other word (its head), except the root of the sentence. The result is a dependency tree where edges are labelled with grammatical relations (subject, object, modifier, etc.).</p> </li> </ul> <p></p> <ul> <li> <p>In the dependency view, \"sat\" is the root. \"Cat\" depends on \"sat\" as its subject (nsubj). \"On\" depends on \"sat\" as a prepositional modifier. \"Mat\" depends on \"on\" as the object of the preposition. Every word hangs off exactly one head, creating a tree.</p> </li> <li> <p>Dependency grammar has become the dominant framework in modern NLP because dependency trees are easier to produce with statistical parsers and the relations map more directly to semantic roles (who did what to whom).</p> </li> <li> <p>Valency describes how many arguments a verb requires. \"Sleep\" is intransitive (one argument: the sleeper). \"Eat\" is transitive (two: the eater and the eaten). \"Give\" is ditransitive (three: the giver, the thing given, and the receiver). Knowing a verb's valency constrains which parse trees are valid.</p> </li> <li> <p>Semantics is the study of meaning. Syntax tells you how a sentence is structured; semantics tells you what it means.</p> </li> <li> <p>Lexical semantics concerns the meaning of individual words. Words are related to each other in systematic ways:</p> <ul> <li>Synonymy: words with (nearly) the same meaning. \"Big\" and \"large\" are synonyms. True perfect synonymy is rare; there are almost always subtle differences in connotation or usage.</li> <li>Antonymy: words with opposite meanings. \"Hot\" and \"cold,\" \"buy\" and \"sell.\"</li> <li>Hypernymy/hyponymy: \"is-a\" relationships. \"Dog\" is a hyponym of \"animal\" (a dog is a kind of animal). \"Animal\" is a hypernym of \"dog.\" These form taxonomic hierarchies.</li> <li>Meronymy: \"part-of\" relationships. \"Wheel\" is a meronym of \"car.\"</li> <li>Polysemy: a single word with multiple related meanings. \"Bank\" means a financial institution or a river bank. Context disambiguates.</li> </ul> </li> <li> <p>Word sense disambiguation (WSD) is the task of determining which sense of a polysemous word is intended in a given context. In \"I deposited money at the bank,\" the financial sense is correct. In \"We sat by the river bank,\" the geographical sense is. WSD was a central problem in early NLP; modern contextual embeddings (ELMo, BERT) largely solve it by producing different vector representations for different uses of the same word.</p> </li> <li> <p>Compositional semantics asks how the meanings of individual words combine to form the meaning of a phrase or sentence. The principle of compositionality (attributed to Frege) states that the meaning of a complex expression is determined by the meanings of its parts and the rules used to combine them. \"The cat chased the dog\" means something different from \"the dog chased the cat\" because the syntactic structure (who is the subject vs object) interacts with the word meanings.</p> </li> <li> <p>Not all meaning is compositional. Idioms like \"kick the bucket\" (meaning \"to die\") have meanings that cannot be derived from their parts. These are a challenge for any compositional approach.</p> </li> <li> <p>Distributional semantics is the computational approach to meaning that underpins modern NLP. The distributional hypothesis (Firth, 1957) states: \"You shall know a word by the company it keeps.\" Words that appear in similar contexts tend to have similar meanings. This is the theoretical foundation for word embeddings (Word2Vec, GloVe), which we will explore in file 03.</p> </li> <li> <p>Pragmatics studies how context affects meaning. The same sentence can mean different things depending on who says it, when, where, and why.</p> </li> <li> <p>\"Can you pass the salt?\" is syntactically a yes/no question about ability. Pragmatically, it is a request. You would not answer \"Yes, I can\" and then sit still. Understanding this requires knowledge beyond the literal words, specifically, the conventions of speech acts.</p> </li> <li> <p>Speech act theory (Austin, Searle) distinguishes between:</p> <ul> <li>Locutionary act: the literal content (\"Can you pass the salt?\")</li> <li>Illocutionary act: the intended function (a request)</li> <li>Perlocutionary act: the effect on the listener (they pass the salt)</li> </ul> </li> <li> <p>Implicature (Grice) is meaning that is implied but not explicitly stated. If someone asks \"Is John a good cook?\" and you reply \"He's British,\" you have not answered the question literally, but the listener can infer (through cultural stereotypes, fairly or not) that you mean \"no.\" Grice's cooperative principle says speakers generally try to be informative, truthful, relevant, and clear, and listeners interpret utterances assuming these maxims hold.</p> </li> <li> <p>Coreference is a pragmatic phenomenon where different expressions refer to the same entity. In \"Alice went to the store. She bought milk,\" \"she\" refers to Alice. Resolving coreference is essential for understanding multi-sentence text and is a key NLP task.</p> </li> <li> <p>Discourse structure describes how sentences connect to form coherent text. A narrative has a beginning, middle, and end. An argument has claims and evidence. Rhetorical Structure Theory (RST) analyses text as a tree of discourse relations (elaboration, contrast, cause, etc.) between segments.</p> </li> <li> <p>Pragmatics is where NLP gets hardest. Modern language models handle much of syntax and semantics implicitly through training data, but pragmatic reasoning, understanding sarcasm, implicature, and context-dependent meaning, remains a frontier challenge.</p> </li> <li> <p>Phonology studies the sound systems of languages. While this chapter focuses on text, a brief overview bridges to the audio and speech chapter (Chapter 09).</p> </li> <li> <p>A phoneme is the smallest unit of sound that distinguishes meaning. English has about 44 phonemes. The words \"bat\" and \"pat\" differ by one phoneme (/b/ vs /p/), which changes the meaning entirely. This is called a minimal pair.</p> </li> <li> <p>Allophones are different physical realisations of the same phoneme that do not change meaning. The \"p\" in \"pin\" (aspirated, with a puff of air) and the \"p\" in \"spin\" (unaspirated) are allophones of /p/ in English; a native speaker treats them as the same sound.</p> </li> <li> <p>The International Phonetic Alphabet (IPA) provides a standardised notation for phonemes across all languages. The word \"cat\" is transcribed as /k\u00e6t/. IPA is the bridge between written text and speech systems.</p> </li> <li> <p>Prosody covers the rhythm, stress, and intonation of speech. \"I didn't say he stole the money\" has seven different meanings depending on which word is stressed. Prosody carries information that text alone loses, which is why text-to-speech systems must model it carefully.</p> </li> <li> <p>In NLP, phonological knowledge appears in text-to-speech (grapheme-to-phoneme conversion), speech recognition (mapping acoustic signals to phonemes), and even in spelling correction and transliteration.</p> </li> </ul>"},{"location":"chapter%2007%3A%20computational%20linguistics/01.%20linguistic%20foundations/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Build a simple morphological analyser that splits English words into likely morphemes using a list of common prefixes and suffixes. <pre><code>prefixes = ['un', 're', 'pre', 'dis', 'mis', 'over', 'under', 'out', 'non']\nsuffixes = ['ing', 'ed', 'ly', 'ness', 'ment', 'tion', 'able', 'ible', 'er', 'est', 'ful', 'less', 'ous']\n\ndef analyse_morphemes(word):\n    \"\"\"Simple morpheme analysis using known affixes.\"\"\"\n    parts = []\n    remaining = word.lower()\n\n    # Check prefixes\n    for p in sorted(prefixes, key=len, reverse=True):\n        if remaining.startswith(p) and len(remaining) &gt; len(p) + 2:\n            parts.append(f\"[prefix: {p}]\")\n            remaining = remaining[len(p):]\n            break\n\n    # Check suffixes\n    for s in sorted(suffixes, key=len, reverse=True):\n        if remaining.endswith(s) and len(remaining) &gt; len(s) + 2:\n            root = remaining[:-len(s)]\n            parts.append(f\"[root: {root}]\")\n            parts.append(f\"[suffix: {s}]\")\n            remaining = None\n            break\n\n    if remaining is not None:\n        parts.append(f\"[root: {remaining}]\")\n\n    return parts\n\nfor word in ['unhappiness', 'reusable', 'disconnected', 'overreacting', 'kindness']:\n    print(f\"{word:20s} \u2192 {' + '.join(analyse_morphemes(word))}\")\n</code></pre></p> </li> <li> <p>Implement a simple context-free grammar parser using recursive descent. Define a small grammar and parse a sentence into a constituency tree. <pre><code>class CFGParser:\n    \"\"\"Recursive descent parser for a tiny English grammar.\"\"\"\n    def __init__(self, tokens):\n        self.tokens = tokens\n        self.pos = 0\n\n    def peek(self):\n        return self.tokens[self.pos] if self.pos &lt; len(self.tokens) else None\n\n    def consume(self, expected=None):\n        tok = self.peek()\n        if expected and tok != expected:\n            return None\n        self.pos += 1\n        return tok\n\n    def parse_det(self):\n        if self.peek() in ('the', 'a'):\n            return ('Det', self.consume())\n        return None\n\n    def parse_noun(self):\n        if self.peek() in ('cat', 'dog', 'mat', 'man'):\n            return ('N', self.consume())\n        return None\n\n    def parse_verb(self):\n        if self.peek() in ('sat', 'chased', 'saw'):\n            return ('V', self.consume())\n        return None\n\n    def parse_prep(self):\n        if self.peek() in ('on', 'under', 'with'):\n            return ('P', self.consume())\n        return None\n\n    def parse_np(self):\n        save = self.pos\n        det = self.parse_det()\n        noun = self.parse_noun()\n        if det and noun:\n            # Check for optional PP\n            pp = self.parse_pp()\n            if pp:\n                return ('NP', det, noun, pp)\n            return ('NP', det, noun)\n        self.pos = save\n        return None\n\n    def parse_pp(self):\n        save = self.pos\n        prep = self.parse_prep()\n        np = self.parse_np()\n        if prep and np:\n            return ('PP', prep, np)\n        self.pos = save\n        return None\n\n    def parse_vp(self):\n        save = self.pos\n        verb = self.parse_verb()\n        if verb:\n            np = self.parse_np()\n            if np:\n                return ('VP', verb, np)\n            pp = self.parse_pp()\n            if pp:\n                return ('VP', verb, pp)\n        self.pos = save\n        return None\n\n    def parse_sentence(self):\n        np = self.parse_np()\n        vp = self.parse_vp()\n        if np and vp and self.pos == len(self.tokens):\n            return ('S', np, vp)\n        return None\n\ndef print_tree(tree, indent=0):\n    if isinstance(tree, str):\n        print(' ' * indent + tree)\n    elif isinstance(tree, tuple):\n        print(' ' * indent + tree[0])\n        for child in tree[1:]:\n            print_tree(child, indent + 2)\n\nsentences = [\n    \"the cat sat on the mat\",\n    \"a dog chased the cat\",\n]\n\nfor sent in sentences:\n    tokens = sent.split()\n    parser = CFGParser(tokens)\n    tree = parser.parse_sentence()\n    print(f\"\\n'{sent}':\")\n    if tree:\n        print_tree(tree)\n    else:\n        print(\"  (no parse found)\")\n</code></pre></p> </li> <li> <p>Explore lexical relations by building a simple word graph. Given a small vocabulary with synonym, antonym, and hypernym relations, find paths between words. <pre><code>relations = {\n    ('big', 'large'): 'synonym',\n    ('big', 'small'): 'antonym',\n    ('small', 'tiny'): 'synonym',\n    ('dog', 'animal'): 'hypernym',\n    ('cat', 'animal'): 'hypernym',\n    ('puppy', 'dog'): 'hypernym',\n    ('happy', 'glad'): 'synonym',\n    ('happy', 'sad'): 'antonym',\n    ('hot', 'cold'): 'antonym',\n    ('hot', 'warm'): 'synonym',\n}\n\n# Build adjacency list\nfrom collections import defaultdict, deque\n\ngraph = defaultdict(list)\nfor (w1, w2), rel in relations.items():\n    graph[w1].append((w2, rel))\n    graph[w2].append((w1, rel))\n\ndef find_path(start, end):\n    \"\"\"BFS to find a path between two words through the relation graph.\"\"\"\n    queue = deque([(start, [(start, None)])])\n    visited = {start}\n    while queue:\n        node, path = queue.popleft()\n        if node == end:\n            return path\n        for neighbor, rel in graph[node]:\n            if neighbor not in visited:\n                visited.add(neighbor)\n                queue.append((neighbor, path + [(neighbor, rel)]))\n    return None\n\npairs = [('big', 'tiny'), ('puppy', 'cat'), ('happy', 'sad')]\nfor w1, w2 in pairs:\n    path = find_path(w1, w2)\n    if path:\n        steps = \" \u2192 \".join(f\"{w}({r})\" if r else w for w, r in path)\n        print(f\"{w1} \u2192 {w2}: {steps}\")\n    else:\n        print(f\"{w1} \u2192 {w2}: no path found\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2007%3A%20computational%20linguistics/02.%20text%20processing%20and%20classic%20NLP/","title":"Text Processing and Classic NLP","text":"<p>Text processing transforms raw characters into structured representations that models can consume. This file covers tokenisation (word, subword, BPE, WordPiece), text normalisation, edit distance, TF-IDF, n-gram language models, POS tagging, NER, and sentiment analysis -- the classical NLP pipeline that still underpins modern systems.</p> <ul> <li> <p>Raw text is messy. Before any NLP model can work with language, the text must be cleaned, normalised, and converted into a structured representation. This file covers the pipeline from raw characters to features that models can consume, along with the classical NLP algorithms that dominated before deep learning.</p> </li> <li> <p>Text normalisation transforms raw text into a canonical form. The goal is to reduce irrelevant variation so that \"Hello\", \"hello\", \"HELLO\" and \"h\u00e9llo\" are treated appropriately.</p> </li> <li> <p>Case folding converts text to lowercase. This collapses \"The\" and \"the\" into one token. It helps for most tasks, but destroys useful information in some cases: \"US\" (the country) vs \"us\" (the pronoun), or \"Apple\" (the company) vs \"apple\" (the fruit).</p> </li> <li> <p>Unicode normalisation handles the fact that the same character can be encoded multiple ways. The character \"\u00e9\" can be a single code point (U+00E9) or a base \"e\" plus a combining accent (U+0065 + U+0301). NFC normalisation composes them into one code point; NFD decomposes them. Without normalisation, two identical-looking strings may not match.</p> </li> <li> <p>Edit distance measures how different two strings are. The Levenshtein distance counts the minimum number of single-character insertions, deletions, and substitutions needed to transform one string into another. \"kitten\" \u2192 \"sitting\" has edit distance 3 (k\u2192s, e\u2192i, insert g).</p> </li> <li> <p>Edit distance is computed using dynamic programming (we review in alrothim chapter). Define \\(D[i][j]\\) as the distance between the first \\(i\\) characters of string \\(s\\) and the first \\(j\\) characters of string \\(t\\):</p> </li> </ul> \\[ D[i][j] = \\begin{cases} j &amp; \\text{if } i = 0 \\\\ i &amp; \\text{if } j = 0 \\\\ D[i{-}1][j{-}1] &amp; \\text{if } s[i] = t[j] \\\\ 1 + \\min(D[i{-}1][j], \\; D[i][j{-}1], \\; D[i{-}1][j{-}1]) &amp; \\text{otherwise} \\end{cases} \\] <ul> <li> <p>Edit distance powers spelling correction, fuzzy matching, and DNA sequence alignment. In NLP, it is used to handle typos and find similar words.</p> </li> <li> <p>Tokenisation splits text into discrete units (tokens) that a model can process. This is the first and arguably most important preprocessing step. The choice of tokenisation strategy profoundly affects model behaviour.</p> </li> <li> <p>Whitespace tokenisation splits on spaces. Simple but naive: \"New York\" becomes two tokens, \"don't\" is one token (or split into \"don\" and \"'t\" depending on the splitter), and languages like Chinese and Japanese have no spaces between words at all.</p> </li> <li> <p>Rule-based tokenisation uses handcrafted patterns (regular expressions) to handle contractions, punctuation, and special cases. \"I'm\" \u2192 \"I\" + \"'m\", \"U.S.A.\" stays as one token. Every language needs its own rules, which is labour-intensive.</p> </li> <li> <p>Subword tokenisation is the modern solution. Instead of splitting at word boundaries, it learns a vocabulary of frequent subword units from data. This elegantly handles unknown words: if \"unhappiness\" is not in the vocabulary, it might be split into \"un\" + \"happi\" + \"ness\", preserving morphological structure.</p> </li> </ul> <p></p> <ul> <li> <p>Byte-Pair Encoding (BPE) starts with individual characters as the vocabulary. It repeatedly finds the most frequent adjacent pair and merges them into a new token. After enough merges, common words are single tokens and rare words are split into frequent subword pieces.</p> </li> <li> <p>The BPE algorithm:</p> <ol> <li>Initialise the vocabulary with all individual characters in the training corpus</li> <li>Count the frequency of every adjacent token pair</li> <li>Merge the most frequent pair into a new token</li> <li>Repeat steps 2-3 for a desired number of merges (vocabulary size)</li> </ol> </li> <li> <p>For example, starting with \"l o w\" (5 times), \"l o w e r\" (2 times), \"n e w e s t\" (6 times): the most frequent pair might be \"e s\" \u2192 merge into \"es\". Then \"es t\" \u2192 \"est\". Then \"n e w\" \u2192 \"new\". The final vocabulary contains both full words and subword pieces.</p> </li> <li> <p>WordPiece (used by BERT) is similar to BPE but selects merges based on likelihood rather than frequency. It merges the pair that maximises the language model likelihood of the training data. Subword tokens that are not word-initial are prefixed with \"##\" (e.g., \"playing\" \u2192 \"play\" + \"##ing\").</p> </li> <li> <p>Unigram (used by SentencePiece) takes the opposite approach: start with a large vocabulary and iteratively remove tokens whose removal least hurts the training data likelihood. The final vocabulary is the set of subword units that best explain the corpus.</p> </li> <li> <p>SentencePiece is a language-agnostic tokenisation library that treats the input as a raw byte stream (no pre-tokenisation on spaces). This makes it work for any language, including those without spaces. It implements both BPE and Unigram algorithms.</p> </li> <li> <p>The vocabulary size is a key hyperparameter. Typical choices range from 30,000 to 100,000 tokens. Larger vocabularies mean fewer tokens per sequence (more efficient) but a larger embedding table. Smaller vocabularies mean more subword splits and longer sequences.</p> </li> <li> <p>Both techniques reduce words to a base form, but they differ in approach.</p> </li> <li> <p>Stemming chops off suffixes using crude rules. The Porter stemmer reduces \"running\" to \"run\", \"happiness\" to \"happi\", and \"studies\" to \"studi\". It is fast but imprecise: \"university\" and \"universe\" both stem to \"univers\" despite being unrelated.</p> </li> <li> <p>Lemmatisation uses vocabulary and morphological analysis to find the true dictionary form (lemma). \"Running\" \u2192 \"run\", \"better\" \u2192 \"good\", \"mice\" \u2192 \"mouse\". It requires knowing the part of speech: \"saw\" as a verb lemmatises to \"see\", but as a noun it stays \"saw\".</p> </li> <li> <p>Modern subword tokenisation has largely replaced stemming and lemmatisation in neural NLP, but they remain useful in information retrieval and when working with smaller models or limited data.</p> </li> <li> <p>Part-of-speech (POS) tagging assigns a grammatical category to each word: noun, verb, adjective, determiner, etc. This is one of the oldest NLP tasks and is fundamental to syntactic analysis.</p> </li> <li> <p>The Penn Treebank tagset is the most common for English, with 36 tags (NN for singular noun, NNS for plural noun, VB for base verb, VBD for past tense, JJ for adjective, etc.).</p> </li> <li> <p>POS tagging is tricky because many words are ambiguous. \"Book\" can be a noun (\"the book\") or a verb (\"book a flight\"). \"Run\" has dozens of senses across parts of speech. Context is essential.</p> </li> <li> <p>Early taggers used Hidden Markov Models (HMMs) from chapter 05. The hidden states are POS tags, the observations are words. The transition probabilities capture tag sequences (a determiner is likely followed by a noun or adjective), and the emission probabilities capture which words appear with which tags. The Viterbi algorithm finds the most likely tag sequence.</p> </li> <li> <p>The HMM model for POS tagging:</p> </li> </ul> \\[\\hat{t}_{1:n} = \\arg\\max_{t_{1:n}} \\prod_{i=1}^{n} P(w_i \\mid t_i) \\cdot P(t_i \\mid t_{i-1})\\] <ul> <li> <p>Modern POS taggers use neural networks (bidirectional LSTMs or transformers) and achieve over 97% accuracy on English, approaching human performance.</p> </li> <li> <p>Named Entity Recognition (NER) identifies and classifies proper names and other specific entities in text: persons, organisations, locations, dates, monetary amounts, etc.</p> </li> <li> <p>In \"Apple CEO Tim Cook announced the event in Cupertino on Monday,\" a NER system should identify: Apple (ORG), Tim Cook (PER), Cupertino (LOC), Monday (DATE).</p> </li> <li> <p>NER is typically framed as sequence labelling using BIO tagging (also called IOB tagging). Each token gets a tag:</p> <ul> <li>B-TYPE: beginning of an entity of type TYPE</li> <li>I-TYPE: inside (continuation of) an entity of type TYPE</li> <li>O: outside any entity</li> </ul> </li> <li> <p>\"Tim Cook visited New York\" becomes: Tim/B-PER Cook/I-PER visited/O New/B-LOC York/I-LOC. The B tag marks where a new entity starts, which is important when two entities of the same type are adjacent.</p> </li> </ul> <p></p> <ul> <li>Classical NER used Conditional Random Fields (CRFs) from chapter 05, which model the conditional probability of the entire tag sequence given the input. Unlike HMMs, which are generative (\\(P(x, y)\\)), CRFs are discriminative and model \\(P(y \\mid x)\\) directly. A linear-chain CRF defines:</li> </ul> \\[P(y_{1:n} \\mid x_{1:n}) = \\frac{1}{Z(x)} \\exp\\!\\left(\\sum_{i=1}^{n} \\left[\\sum_k \\lambda_k f_k(y_i, x, i) + \\sum_j \\mu_j g_j(y_i, y_{i-1}, x, i)\\right]\\right)\\] <ul> <li> <p>Here \\(f_k\\) are emission features (how likely tag \\(y_i\\) is given the input at position \\(i\\)) and \\(g_j\\) are transition features (how likely tag \\(y_i\\) is given the previous tag \\(y_{i-1}\\)). </p> </li> <li> <p>The partition function \\(Z(x) = \\sum_{y'} \\exp(\\ldots)\\) sums over all possible tag sequences to normalise the distribution. Training maximises the conditional log-likelihood, which requires computing \\(Z(x)\\) efficiently using the forward algorithm (chapter 05). </p> </li> <li> <p>The key advantage over independently classifying each token: the CRF's transition features enforce structural constraints (e.g., I-PER should only follow B-PER or I-PER, never appear after O). </p> </li> <li> <p>Modern NER stacks a CRF on top of a neural encoder (BiLSTM-CRF or BERT-CRF), where the neural network produces the emission scores and the CRF layer learns transition structure.</p> </li> <li> <p>Syntactic parsing converts a sentence into its syntactic structure, either a constituency tree or a dependency tree (both from file 01).</p> </li> <li> <p>The CYK algorithm (Cocke-Younger-Kasami) parses sentences with context-free grammars using dynamic programming. </p> </li> <li> <p>It requires the grammar to be in Chomsky Normal Form (every rule has either two non-terminals or one terminal on the right side). It fills a triangular table bottom-up: cells represent spans of the sentence, and each cell stores the non-terminals that can generate that span.</p> </li> <li> <p>CYK runs in \\(O(n^3 \\cdot |G|)\\) time, where \\(n\\) is the sentence length and \\(|G|\\) is the grammar size. This is exact but slow for large grammars.</p> </li> <li> <p>Shift-reduce parsing processes the sentence left to right, maintaining a stack. At each step, it either shifts (pushes the next word onto the stack) or reduces (pops elements from the stack and replaces them with a phrase). A trained classifier decides the action at each step. This runs in \\(O(n)\\) time, making it much faster than CYK.</p> </li> <li> <p>Dependency parsing is now more common than constituency parsing in practice. Transition-based dependency parsers (like shift-reduce) and graph-based parsers (which score all possible edges and find the maximum spanning tree) are the two main approaches. Neural dependency parsers using BiLSTMs or transformers achieve state-of-the-art results.</p> </li> <li> <p>Before embeddings, NLP represented documents as vectors using simple counting methods.</p> </li> <li> <p>The bag-of-words (BoW) model represents a document as a vector of word counts, ignoring word order entirely. If the vocabulary has \\(V\\) words, each document is a vector in \\(\\mathbb{R}^V\\) (connecting back to vector spaces from chapter 01). The entry for word \\(w\\) is the number of times \\(w\\) appears in the document.</p> </li> </ul> <p></p> <ul> <li> <p>BoW is simple but surprisingly effective for tasks like document classification and spam filtering. Its main weakness is that it treats every word as equally important: \"the\" and \"revolutionary\" get equal weight.</p> </li> <li> <p>TF-IDF (Term Frequency-Inverse Document Frequency) fixes this by weighting words based on how informative they are. Words that appear frequently in one document but rarely across the corpus are likely important for that document.</p> </li> </ul> \\[\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\\] <ul> <li> <p>Term frequency \\(\\text{TF}(t, d)\\) is often the raw count of term \\(t\\) in document \\(d\\) (or its log: \\(1 + \\log(\\text{count})\\)).</p> </li> <li> <p>Inverse document frequency \\(\\text{IDF}(t) = \\log\\frac{N}{|\\{d : t \\in d\\}|}\\), where \\(N\\) is the total number of documents. Words appearing in every document (like \"the\") get IDF close to 0. Rare words get high IDF.</p> </li> <li> <p>TF-IDF vectors can be compared using cosine similarity (from chapter 01) to measure document similarity. This is the foundation of classical information retrieval and search engines.</p> </li> <li> <p>A language model assigns a probability to a sequence of words. It answers: how likely is this sentence? Language models are central to machine translation, speech recognition, spelling correction, and text generation.</p> </li> <li> <p>The probability of a sentence \\(w_1, w_2, \\ldots, w_n\\) is, by the chain rule of probability (chapter 05):</p> </li> </ul> \\[P(w_1, w_2, \\ldots, w_n) = \\prod_{i=1}^{n} P(w_i \\mid w_1, \\ldots, w_{i-1})\\] <ul> <li> <p>This is exact but impractical: you would need to store probabilities for every possible history. The Markov assumption (chapter 05) truncates the history to the last \\(k-1\\) words, giving an n-gram model (where \\(n = k\\)).</p> </li> <li> <p>A bigram model (\\(n = 2\\)) conditions only on the previous word:</p> </li> </ul> \\[P(w_i \\mid w_1, \\ldots, w_{i-1}) \\approx P(w_i \\mid w_{i-1})\\] <ul> <li>A trigram model (\\(n = 3\\)) conditions on the previous two words. N-gram probabilities are estimated by counting in a corpus:</li> </ul> \\[P(w_i \\mid w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i)}{\\text{count}(w_{i-1})}\\] <ul> <li>Perplexity measures how well a language model predicts a test set. It is the inverse probability of the test set, normalised by the number of words:</li> </ul> \\[\\text{PPL} = P(w_1, \\ldots, w_N)^{-1/N} = \\exp\\!\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i \\mid w_{&lt;i})\\right)\\] <ul> <li> <p>Lower perplexity means the model is less \"surprised\" by the test data and therefore better. A model that assigns uniform probability over a 10,000-word vocabulary has perplexity 10,000. A good bigram model might achieve perplexity around 200. Modern neural language models achieve perplexity below 20.</p> </li> <li> <p>Notice that perplexity is the exponentiated cross-entropy (from chapter 05's information theory). Minimising cross-entropy loss during training directly minimises perplexity.</p> </li> <li> <p>Smoothing handles the zero-probability problem: if an n-gram never appeared in training, the model assigns it probability 0, which makes the entire sentence probability 0. Laplace smoothing (add-1) adds a small count to every n-gram:</p> </li> </ul> \\[P_{\\text{Laplace}}(w_i \\mid w_{i-1}) = \\frac{\\text{count}(w_{i-1}, w_i) + 1}{\\text{count}(w_{i-1}) + V}\\] <ul> <li> <p>This is too aggressive for large vocabularies (it steals too much probability from observed n-grams). Kneser-Ney smoothing is the gold standard for n-gram models. It combines two ideas: absolute discounting and a continuation probability for backoff.</p> </li> <li> <p>First, absolute discounting subtracts a fixed discount \\(d\\) (typically \\(d \\approx 0.75\\)) from each observed count, rather than adding pseudocounts. The freed probability mass is redistributed to unseen n-grams. The interpolated form is:</p> </li> </ul> \\[P_{\\text{KN}}(w_i \\mid w_{i-1}) = \\frac{\\max(\\text{count}(w_{i-1}, w_i) - d, \\; 0)}{\\text{count}(w_{i-1})} + \\lambda(w_{i-1}) \\cdot P_{\\text{cont}}(w_i)\\] <ul> <li>where \\(\\lambda(w_{i-1})\\) is a normalising constant that distributes the discounted mass. The key innovation is the continuation probability \\(P_{\\text{cont}}(w_i)\\), which measures how many different contexts \\(w_i\\) appears in, rather than how often it appears overall:</li> </ul> \\[P_{\\text{cont}}(w_i) = \\frac{|\\{w' : \\text{count}(w', w_i) &gt; 0\\}|}{|\\{(w', w'') : \\text{count}(w', w'') &gt; 0\\}|}\\] <ul> <li> <p>The numerator counts how many distinct words precede \\(w_i\\) in the corpus. A word like \"Francisco\" appears in few contexts (almost always after \"San\"), so even if \"San Francisco\" is very frequent, \"Francisco\" gets a low continuation probability and will not be predicted spuriously in other contexts. </p> </li> <li> <p>Conversely, common words like \"the\" appear after many different words and get high continuation probability. This captures the intuition that a word's versatility matters more than its raw frequency for backoff estimation.</p> </li> <li> <p>N-gram models were the state of the art for decades. They are fast, interpretable, and require no training (just counting). But they struggle with long-range dependencies (\"The keys that I left on the table are missing\" requires knowing the subject \"keys\" is plural, which is far from the verb). Neural language models, starting with RNNs and culminating in transformers, address this limitation.</p> </li> </ul>"},{"location":"chapter%2007%3A%20computational%20linguistics/02.%20text%20processing%20and%20classic%20NLP/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement the Levenshtein edit distance using dynamic programming. Test it on word pairs and use it for simple spelling correction. <pre><code>import jax.numpy as jnp\n\ndef edit_distance(s, t):\n    \"\"\"Compute Levenshtein edit distance using DP.\"\"\"\n    m, n = len(s), len(t)\n    D = [[0] * (n + 1) for _ in range(m + 1)]\n\n    for i in range(m + 1):\n        D[i][0] = i\n    for j in range(n + 1):\n        D[0][j] = j\n\n    for i in range(1, m + 1):\n        for j in range(1, n + 1):\n            if s[i-1] == t[j-1]:\n                D[i][j] = D[i-1][j-1]\n            else:\n                D[i][j] = 1 + min(D[i-1][j], D[i][j-1], D[i-1][j-1])\n\n    return D[m][n]\n\n# Test\npairs = [(\"kitten\", \"sitting\"), (\"sunday\", \"saturday\"), (\"hello\", \"hallo\")]\nfor s, t in pairs:\n    print(f\"d('{s}', '{t}') = {edit_distance(s, t)}\")\n\n# Simple spelling correction\ndictionary = [\"the\", \"their\", \"there\", \"then\", \"than\", \"this\", \"that\", \"these\", \"those\"]\nmisspelled = \"thier\"\ncorrections = sorted(dictionary, key=lambda w: edit_distance(misspelled, w))\nprint(f\"\\nClosest to '{misspelled}': {corrections[:3]}\")\n</code></pre></p> </li> <li> <p>Implement BPE tokenisation from scratch. Start with character-level tokens and iteratively merge the most frequent pairs. <pre><code>from collections import Counter\n\ndef get_pairs(corpus):\n    \"\"\"Count adjacent token pairs across all words.\"\"\"\n    pairs = Counter()\n    for word, freq in corpus.items():\n        symbols = word.split()\n        for i in range(len(symbols) - 1):\n            pairs[(symbols[i], symbols[i+1])] += freq\n    return pairs\n\ndef merge_pair(pair, corpus):\n    \"\"\"Merge all occurrences of a pair in the corpus.\"\"\"\n    new_corpus = {}\n    bigram = ' '.join(pair)\n    replacement = ''.join(pair)\n    for word, freq in corpus.items():\n        new_word = word.replace(bigram, replacement)\n        new_corpus[new_word] = freq\n    return new_corpus\n\n# Training corpus with word frequencies\ntext = \"low low low low low lower lower newest newest newest newest newest newest\"\nword_freqs = Counter(text.split())\n# Initialise: split each word into characters with end-of-word marker\ncorpus = {' '.join(word) + ' _': freq for word, freq in word_freqs.items()}\n\nprint(\"Initial corpus:\")\nfor word, freq in corpus.items():\n    print(f\"  {word}: {freq}\")\n\n# Run BPE for 10 merges\nfor i in range(10):\n    pairs = get_pairs(corpus)\n    if not pairs:\n        break\n    best_pair = max(pairs, key=pairs.get)\n    corpus = merge_pair(best_pair, corpus)\n    print(f\"\\nMerge {i+1}: {best_pair} (freq={pairs[best_pair]})\")\n    for word, freq in corpus.items():\n        print(f\"  {word}: {freq}\")\n</code></pre></p> </li> <li> <p>Build a bigram language model and compute perplexity on a test sentence. Experiment with Laplace smoothing. <pre><code>from collections import Counter, defaultdict\nimport math\n\n# Training corpus\ntrain = \"\"\"the cat sat on the mat . the dog chased the cat .\nthe cat ran from the dog . a dog sat on a mat .\"\"\".split()\n\n# Count bigrams and unigrams\nbigrams = Counter(zip(train[:-1], train[1:]))\nunigrams = Counter(train)\nvocab_size = len(set(train))\n\ndef bigram_prob(w2, w1, alpha=0):\n    \"\"\"P(w2 | w1) with optional Laplace smoothing.\"\"\"\n    return (bigrams[(w1, w2)] + alpha) / (unigrams[w1] + alpha * vocab_size)\n\n# Compute perplexity\ntest = \"the cat sat on a mat .\".split()\n\nfor alpha in [0, 1, 0.1]:\n    log_prob = 0\n    for w1, w2 in zip(test[:-1], test[1:]):\n        p = bigram_prob(w2, w1, alpha=alpha)\n        if p &gt; 0:\n            log_prob += math.log(p)\n        else:\n            log_prob += float('-inf')\n\n    ppl = math.exp(-log_prob / (len(test) - 1)) if log_prob &gt; float('-inf') else float('inf')\n    print(f\"Smoothing \u03b1={alpha}: perplexity = {ppl:.2f}\")\n</code></pre></p> </li> <li> <p>Implement TF-IDF from scratch and use cosine similarity to find the most similar document to a query. <pre><code>import jax.numpy as jnp\nimport math\nfrom collections import Counter\n\ndocuments = [\n    \"the cat sat on the mat\",\n    \"the dog chased the cat around the park\",\n    \"a mat was placed on the floor by the door\",\n    \"the quick brown fox jumped over the lazy dog\",\n]\n\n# Build vocabulary\nvocab = sorted(set(word for doc in documents for word in doc.split()))\nword_to_idx = {w: i for i, w in enumerate(vocab)}\nV = len(vocab)\nN = len(documents)\n\n# Compute TF-IDF matrix\ndoc_freq = Counter()\nfor doc in documents:\n    for word in set(doc.split()):\n        doc_freq[word] += 1\n\ntfidf_matrix = jnp.zeros((N, V))\nfor i, doc in enumerate(documents):\n    word_counts = Counter(doc.split())\n    for word, count in word_counts.items():\n        tf = 1 + math.log(count)\n        idf = math.log(N / doc_freq[word])\n        j = word_to_idx[word]\n        tfidf_matrix = tfidf_matrix.at[i, j].set(tf * idf)\n\n# Query\nquery = \"cat on the mat\"\nquery_vec = jnp.zeros(V)\nquery_counts = Counter(query.split())\nfor word, count in query_counts.items():\n    if word in word_to_idx:\n        tf = 1 + math.log(count)\n        idf = math.log(N / doc_freq.get(word, 1))\n        query_vec = query_vec.at[word_to_idx[word]].set(tf * idf)\n\n# Cosine similarity (from chapter 01)\ndef cosine_sim(a, b):\n    return jnp.dot(a, b) / (jnp.linalg.norm(a) * jnp.linalg.norm(b) + 1e-8)\n\nprint(f\"Query: '{query}'\\n\")\nfor i, doc in enumerate(documents):\n    sim = cosine_sim(query_vec, tfidf_matrix[i])\n    print(f\"  Doc {i} (sim={sim:.3f}): '{doc}'\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2007%3A%20computational%20linguistics/03.%20embeddings%20and%20sequence%20models/","title":"Embeddings and Sequence Models","text":"<p>Word embeddings compress sparse, symbolic text into dense vector spaces where semantic similarity becomes geometric proximity. This file covers Word2Vec (CBOW, Skip-gram), GloVe, FastText, RNNs, LSTMs, GRUs, seq2seq with attention, and the encoder-decoder paradigm -- the progression from bag-of-words to contextual representations.</p> <ul> <li> <p>In file 01, we introduced the distributional hypothesis: words that appear in similar contexts tend to have similar meanings. In file 02, we represented text using sparse, hand-crafted features like TF-IDF vectors. These vectors live in very high-dimensional spaces (one dimension per vocabulary word) and are mostly zeros. Word embeddings compress this information into dense, low-dimensional vectors that capture semantic relationships, and they are learned directly from data.</p> </li> <li> <p>Word2Vec (Mikolov et al., 2013) learns word embeddings by training a shallow neural network on a simple prediction task. There are two architectures.</p> </li> <li> <p>The Continuous Bag of Words (CBOW) model predicts a target word from its surrounding context words. Given a window of context words (e.g., \"the cat ___ on the\"), the model averages their embedding vectors and passes the result through a linear layer to predict the missing word (\"sat\"). The training objective maximises:</p> </li> </ul> \\[P(w_t \\mid w_{t-k}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+k})\\] <ul> <li>The Skip-gram model does the reverse: given a target word, predict the surrounding context words. For the target word \"sat\", the model tries to predict \"the\", \"cat\", \"on\", \"the\" in separate predictions. The objective maximises:</li> </ul> \\[P(w_{t+j} \\mid w_t) \\quad \\text{for each } j \\in [-k, k], \\; j \\neq 0\\] <p></p> <ul> <li> <p>Skip-gram tends to work better for rare words because each word generates multiple training examples (one per context position). CBOW is faster and slightly better for frequent words because it averages over multiple context signals.</p> </li> <li> <p>Training on the full vocabulary is expensive because the softmax denominator sums over all \\(V\\) words. Negative sampling approximates this by turning the problem into binary classification: distinguish the true context word (positive sample) from randomly sampled noise words (negative samples). Instead of computing the full softmax, the model only updates embeddings for the target, the true context word, and a handful of negatives:</p> </li> </ul> \\[\\mathcal{L} = \\log \\sigma(v_{w_O}^T v_{w_I}) + \\sum_{i=1}^{k} \\mathbb{E}_{w_i \\sim P_n} [\\log \\sigma(-v_{w_i}^T v_{w_I})]\\] <ul> <li> <p>Here \\(v_{w_I}\\) is the input word embedding, \\(v_{w_O}\\) is the output (context) word embedding, and \\(P_n\\) is the noise distribution, typically the unigram frequency raised to the 3/4 power (which downweights very frequent words like \"the\").</p> </li> <li> <p>Why does this simple objective produce meaningful embeddings? Levy and Goldberg (2014) showed that skip-gram with negative sampling is implicitly factorising a shifted pointwise mutual information (PMI) matrix. At convergence, the dot product of two word vectors approximates:</p> </li> </ul> \\[v_w^T v_c \\approx \\text{PMI}(w, c) - \\log k\\] <ul> <li> <p>where \\(\\text{PMI}(w, c) = \\log \\frac{P(w, c)}{P(w) P(c)}\\) measures how much more likely words \\(w\\) and \\(c\\) co-occur than expected by chance (chapter 05 information theory), and \\(k\\) is the number of negative samples. Words that co-occur much more than chance have high PMI and therefore high dot product (similar embeddings). Words that co-occur less than expected have negative PMI and dissimilar embeddings. This reveals that Word2Vec is doing the same thing as classical distributional semantics methods like latent semantic analysis (SVD on co-occurrence matrices), but in a more scalable, online fashion.</p> </li> <li> <p>The most surprising property of Word2Vec embeddings is that they capture analogies through vector arithmetic. The vector \\(v_{\\text{king}} - v_{\\text{man}} + v_{\\text{woman}}\\) is closest to \\(v_{\\text{queen}}\\). This works because the embedding space encodes semantic relationships as approximately linear directions: the \"royalty\" direction is roughly \\(v_{\\text{king}} - v_{\\text{man}}\\), and adding it to \\(v_{\\text{woman}}\\) lands near \\(v_{\\text{queen}}\\). This connects to the linear algebra of chapter 01: semantic relationships are vector translations.</p> </li> <li> <p>GloVe (Global Vectors for Word Representation, Pennington et al., 2014) takes a different approach. Instead of learning from local context windows one at a time, it builds a global word co-occurrence matrix \\(X\\) where \\(X_{ij}\\) counts how often word \\(j\\) appears in the context of word \\(i\\) across the entire corpus. The model then learns embeddings whose dot product approximates the log co-occurrence:</p> </li> </ul> \\[w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j = \\log X_{ij}\\] <ul> <li>The loss function weights each pair by a capping function \\(f(X_{ij})\\) that prevents very frequent co-occurrences from dominating:</li> </ul> \\[\\mathcal{L} = \\sum_{i,j=1}^{V} f(X_{ij}) \\left(w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j - \\log X_{ij}\\right)^2\\] <ul> <li> <p>GloVe combines the benefits of global matrix factorisation (like latent semantic analysis) with the local context learning of Word2Vec. In practice, GloVe and Word2Vec produce embeddings of similar quality.</p> </li> <li> <p>FastText (Bojanowski et al., 2017) extends skip-gram by representing each word as a bag of character n-grams. The word \"where\" with \\(n = 3\\) becomes: \"\", plus the whole-word token \"\". The word's embedding is the sum of all its n-gram embeddings. <li> <p>This has a crucial advantage: FastText can produce embeddings for words it has never seen during training. The word \"whereabouts\" shares n-grams with \"where\", so its embedding will be reasonable even if \"whereabouts\" never appeared in the training data. This is especially useful for morphologically rich languages (file 01) where words have many inflected forms.</p> </li> <li> <p>Embedding evaluation typically uses two types of benchmarks. Analogy tasks test whether \\(v_a - v_b + v_c \\approx v_d\\) (e.g., \"Paris\" \\(-\\) \"France\" \\(+\\) \"Italy\" \\(\\approx\\) \"Rome\"). Similarity benchmarks compare the cosine similarity (chapter 01) between word pairs to human judgements. Common datasets include WordSim-353, SimLex-999, and the Google analogy test set. A practical caveat: embeddings that excel at analogies may not be best for downstream tasks like sentiment classification. The best evaluation is often the task itself.</p> </li> <li> <p>In chapter 06, we introduced RNNs, LSTMs, and GRUs as architectures for sequential data. Here we focus on how they are applied to language tasks specifically.</p> </li> <li> <p>A language model RNN reads tokens one at a time and predicts the next token at each step. The hidden state \\(h_t\\) compresses the entire history \\(w_1, \\ldots, w_t\\) into a fixed-size vector, and a linear layer plus softmax maps \\(h_t\\) to a distribution over the vocabulary. Training uses cross-entropy loss against the true next token, which is identical to minimising perplexity (file 02). The key limitation: the fixed-size hidden state must encode everything about the history, and information from early tokens gets progressively overwritten.</p> </li> <li> <p>Bidirectional RNNs process the sequence in both directions: one RNN reads left-to-right, another reads right-to-left. At each position \\(t\\), the forward hidden state \\(\\overrightarrow{h}_t\\) and backward hidden state \\(\\overleftarrow{h}_t\\) are concatenated to form a context-aware representation \\(h_t = [\\overrightarrow{h}_t ; \\overleftarrow{h}_t]\\). This gives the model access to both past and future context, which is powerful for tasks like POS tagging and NER (file 02) where a word's label depends on words both before and after it. Bidirectional RNNs cannot be used for language modelling because you cannot peek at future tokens when predicting them.</p> </li> <p></p> <ul> <li> <p>Deep stacked RNNs place multiple RNN layers on top of each other. The hidden states of layer \\(l\\) at all time steps become the input sequence for layer \\(l + 1\\). Stacking 2-4 layers typically improves performance by building hierarchical representations, similar to how deeper CNNs build feature hierarchies (chapter 06). Beyond 4 layers, vanishing gradients and overfitting become problems unless residual connections are added between layers.</p> </li> <li> <p>The sequence-to-sequence (seq2seq) architecture (Sutskever et al., 2014) maps a variable-length input sequence to a variable-length output sequence. It consists of an encoder RNN that reads the input and compresses it into a context vector (the final hidden state), and a decoder RNN that generates the output one token at a time, conditioned on this context vector.</p> </li> </ul> <p></p> <ul> <li> <p>Seq2seq was the breakthrough architecture for machine translation. The encoder reads a French sentence, the decoder produces the English translation. The decoder starts with a special start-of-sequence token and generates tokens autoregressively until it produces an end-of-sequence token. A practical trick: reversing the input sequence (feeding \"chat le\" instead of \"le chat\") improved results because it placed the first input word closer to the first output word in the computation graph, shortening the gradient path.</p> </li> <li> <p>The bottleneck problem: the entire input must be compressed into a single fixed-size vector. For long sentences, this vector cannot capture all the information, and performance degrades. This motivated attention mechanisms.</p> </li> <li> <p>Chapter 06 introduced the modern Q, K, V formulation of attention. The original attention mechanisms for NLP were formulated differently, as alignment models between encoder and decoder states.</p> </li> <li> <p>Bahdanau attention (additive attention, Bahdanau et al., 2015) computes an alignment score between the decoder hidden state \\(s_t\\) and each encoder hidden state \\(h_i\\) using a learned feed-forward network:</p> </li> </ul> \\[e_{ti} = v^T \\tanh(W_s s_{t-1} + W_h h_i)\\] <ul> <li>The scores are normalised to attention weights via softmax, and the context vector is a weighted sum of encoder states:</li> </ul> \\[\\alpha_{ti} = \\frac{\\exp(e_{ti})}{\\sum_j \\exp(e_{tj})}, \\quad c_t = \\sum_i \\alpha_{ti} h_i\\] <ul> <li> <p>The decoder then uses both \\(s_{t-1}\\) and \\(c_t\\) to produce the next output. The key insight: instead of one fixed context vector for the entire sentence, each decoder step gets a different weighted combination of encoder states, allowing the model to \"look back\" at the relevant parts of the input.</p> </li> <li> <p>Luong attention (multiplicative attention, Luong et al., 2015) simplifies the score computation. The dot variant uses \\(e_{ti} = s_t^T h_i\\). The general variant uses \\(e_{ti} = s_t^T W h_i\\). These are faster than Bahdanau's additive score because they use matrix multiplication instead of a feed-forward network. Luong attention also computes the context vector from the current decoder state \\(s_t\\) (rather than \\(s_{t-1}\\)), which gives it access to more information but makes the computation slightly different.</p> </li> </ul> <p></p> <ul> <li> <p>Attention weights are often visualised as heatmaps showing which input tokens the decoder focuses on when producing each output token. In translation, these heatmaps roughly trace the word alignment between source and target languages, with the diagonal pattern broken by reordering (e.g., adjective-noun order differs between French and English).</p> </li> <li> <p>At inference time, the decoder must choose a token at each step. Greedy decoding picks the highest-probability token at each position, but this can lead to suboptimal sequences: a locally good choice may force the model into a globally bad sentence. Beam search maintains the top \\(k\\) (the beam width) partial sequences at each step, expanding each by all possible next tokens and keeping the best \\(k\\) overall.</p> </li> <li> <p>With beam width \\(k = 1\\), beam search reduces to greedy decoding. Typical values are \\(k = 4\\) to \\(k = 10\\). Larger beams find better sequences but are proportionally slower. Beam search also needs length normalisation to avoid favouring shorter sequences, which naturally have higher total probability because they multiply fewer terms. The normalised score is:</p> </li> </ul> \\[\\text{score}(y) = \\frac{1}{|y|^\\alpha} \\sum_{t=1}^{|y|} \\log P(y_t \\mid y_{&lt;t})\\] <ul> <li> <p>where \\(|y|\\) is the sequence length and \\(\\alpha\\) (typically 0.6-0.7) controls the strength of the length penalty. With \\(\\alpha = 0\\), there is no length normalisation. With \\(\\alpha = 1\\), the score is the per-token log-probability (geometric mean). The intermediate value balances between favouring concise outputs and not truncating too early.</p> </li> <li> <p>While RNNs process text sequentially, 1D CNNs process it in parallel by sliding filters across the token sequence. Each filter detects a local pattern (an n-gram feature).</p> </li> <li> <p>TextCNN (Kim, 2014) applies multiple 1D convolutional filters of different widths (e.g., 3, 4, 5 tokens) to the input embedding matrix. Each filter produces a feature map, and max-over-time pooling takes the single maximum value from each feature map, capturing whether the pattern was detected anywhere in the text regardless of position. The pooled features from all filters are concatenated and passed to a classifier.</p> </li> </ul> <p></p> <ul> <li> <p>TextCNN is fast and surprisingly effective for text classification tasks like sentiment analysis. It captures local n-gram patterns but cannot model long-range dependencies: a filter of width 5 only sees 5 consecutive tokens. Dilated causal convolutions address this by inserting gaps (dilations) between filter elements. Stacking layers with exponentially increasing dilation rates (1, 2, 4, 8, ...) grows the receptive field exponentially without increasing parameters, allowing the model to capture dependencies across hundreds of tokens.</p> </li> <li> <p>All the embeddings discussed so far (Word2Vec, GloVe, FastText) produce a single vector per word type regardless of context. \"Bank\" gets the same embedding whether it means a financial institution or a river bank. This is a fundamental limitation that contextual embeddings address.</p> </li> <li> <p>ELMo (Embeddings from Language Models, Peters et al., 2018) produces contextual word representations by running a deep bidirectional LSTM language model on the input text. The forward LSTM predicts the next word at each position; a separate backward LSTM predicts the previous word. Both are trained as language models on large corpora.</p> </li> <li> <p>At each position \\(k\\), ELMo combines the hidden states from all \\(L\\) layers using task-specific learned weights:</p> </li> </ul> \\[\\text{ELMo}_k = \\gamma \\sum_{j=0}^{L} s_j \\, h_{k,j}\\] <ul> <li> <p>Here \\(h_{k,j}\\) is the hidden state at position \\(k\\) and layer \\(j\\) (layer 0 is the raw token embedding), \\(s_j\\) are softmax-normalised scalar weights, and \\(\\gamma\\) is a task-specific scaling factor. Different layers capture different information: lower layers capture syntax (POS tags, word morphology), upper layers capture semantics (word sense, semantic role). By mixing all layers with learned weights, ELMo embeddings adapt to diverse downstream tasks.</p> </li> <li> <p>ELMo marked the beginning of the pre-train then fine-tune paradigm: train a large language model on massive unlabelled text, then use its representations for downstream tasks. ELMo specifically uses the pre-trained representations as fixed or lightly tuned features that are concatenated with task-specific inputs. BERT and GPT (file 04) push this further by fine-tuning the entire model end-to-end, which proves dramatically more effective.</p> </li> <li> <p>The progression from Word2Vec to ELMo illustrates a recurring theme in NLP: moving from static to dynamic representations, from local to global context, and from shallow to deep models. Each step trades computational cost for richer representations. Transformers (file 04) complete this progression by replacing recurrence entirely with attention, enabling both deep contextualisation and parallel computation.</p> </li> </ul>"},{"location":"chapter%2007%3A%20computational%20linguistics/03.%20embeddings%20and%20sequence%20models/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement Word2Vec skip-gram with negative sampling from scratch. Train on a small corpus and visualise the learned embeddings using PCA. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Small corpus\ncorpus = \"\"\"the king ruled the kingdom . the queen ruled the kingdom .\nthe prince is the son of the king . the princess is the daughter of the queen .\na man worked in the castle . a woman worked in the castle .\nthe king and queen lived in the castle . the prince and princess played outside .\"\"\".lower().split()\n\nvocab = sorted(set(corpus))\nword2idx = {w: i for i, w in enumerate(vocab)}\nidx2word = {i: w for w, i in word2idx.items()}\nV = len(vocab)\n\n# Generate skip-gram pairs with window size 2\nwindow = 2\npairs = []\nfor i, word in enumerate(corpus):\n    for j in range(max(0, i - window), min(len(corpus), i + window + 1)):\n        if i != j:\n            pairs.append((word2idx[word], word2idx[corpus[j]]))\n\npairs = jnp.array(pairs)\nprint(f\"Vocabulary: {V} words, Training pairs: {len(pairs)}\")\n\n# Model parameters\nembed_dim = 16\nkey = jax.random.PRNGKey(42)\nk1, k2 = jax.random.split(key)\nW_in = jax.random.normal(k1, (V, embed_dim)) * 0.1    # input embeddings\nW_out = jax.random.normal(k2, (V, embed_dim)) * 0.1   # output embeddings\n\n# Negative sampling loss for one pair\ndef neg_sampling_loss(W_in, W_out, target, context, neg_ids):\n    v_in = W_in[target]      # (embed_dim,)\n    v_out = W_out[context]   # (embed_dim,)\n    v_neg = W_out[neg_ids]   # (k, embed_dim)\n\n    pos_loss = -jax.nn.log_sigmoid(jnp.dot(v_in, v_out))\n    neg_loss = -jnp.sum(jax.nn.log_sigmoid(-v_neg @ v_in))\n    return pos_loss + neg_loss\n\n# Training loop\nnum_neg = 5\nlr = 0.05\n\n@jax.jit\ndef train_step(W_in, W_out, target, context, neg_ids):\n    loss, (g_in, g_out) = jax.value_and_grad(neg_sampling_loss, argnums=(0, 1))(\n        W_in, W_out, target, context, neg_ids)\n    return loss, W_in - lr * g_in, W_out - lr * g_out\n\nkey = jax.random.PRNGKey(0)\nfor epoch in range(50):\n    total_loss = 0.0\n    for i in range(len(pairs)):\n        key, subkey = jax.random.split(key)\n        neg_ids = jax.random.randint(subkey, (num_neg,), 0, V)\n        loss, W_in, W_out = train_step(W_in, W_out, pairs[i, 0], pairs[i, 1], neg_ids)\n        total_loss += loss\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}: avg loss = {total_loss / len(pairs):.4f}\")\n\n# Visualise with PCA (chapter 01)\nembeddings = W_in\nmean = embeddings.mean(axis=0)\ncentered = embeddings - mean\nU, S, Vt = jnp.linalg.svd(centered, full_matrices=False)\ncoords = centered @ Vt[:2].T  # project onto top 2 PCs\n\nplt.figure(figsize=(10, 8))\nfor i, word in idx2word.items():\n    plt.scatter(coords[i, 0], coords[i, 1], c='#3498db', s=40)\n    plt.annotate(word, (coords[i, 0] + 0.02, coords[i, 1] + 0.02), fontsize=9)\nplt.title(\"Word2Vec Skip-gram Embeddings (PCA projection)\")\nplt.grid(alpha=0.3); plt.show()\n</code></pre></p> </li> <li> <p>Build a character-level RNN language model that learns to generate text from a small training string. <pre><code>import jax\nimport jax.numpy as jnp\n\n# Tiny training text\ntext = \"to be or not to be that is the question \"\nchars = sorted(set(text))\nchar2idx = {c: i for i, c in enumerate(chars)}\nidx2char = {i: c for c, i in char2idx.items()}\nV = len(chars)\ndata = jnp.array([char2idx[c] for c in text])\n\n# RNN parameters\nhidden_dim = 64\nkey = jax.random.PRNGKey(0)\nk1, k2, k3, k4, k5 = jax.random.split(key, 5)\n\nparams = {\n    'Wx': jax.random.normal(k1, (V, hidden_dim)) * 0.1,\n    'Wh': jax.random.normal(k2, (hidden_dim, hidden_dim)) * 0.05,\n    'bh': jnp.zeros(hidden_dim),\n    'Wy': jax.random.normal(k3, (hidden_dim, V)) * 0.1,\n    'by': jnp.zeros(V),\n}\n\ndef rnn_step(params, h, x_idx):\n    x = jnp.eye(V)[x_idx]  # one-hot\n    h = jnp.tanh(x @ params['Wx'] + h @ params['Wh'] + params['bh'])\n    logits = h @ params['Wy'] + params['by']\n    return h, logits\n\ndef loss_fn(params, inputs, targets):\n    h = jnp.zeros(hidden_dim)\n    total_loss = 0.0\n    for t in range(len(inputs)):\n        h, logits = rnn_step(params, h, inputs[t])\n        log_probs = jax.nn.log_softmax(logits)\n        total_loss -= log_probs[targets[t]]\n    return total_loss / len(inputs)\n\ngrad_fn = jax.jit(jax.grad(loss_fn))\n\n# Training\ninputs = data[:-1]\ntargets = data[1:]\nlr = 0.01\n\nfor step in range(500):\n    grads = grad_fn(params, inputs, targets)\n    params = {k: params[k] - lr * grads[k] for k in params}\n    if (step + 1) % 100 == 0:\n        l = loss_fn(params, inputs, targets)\n        print(f\"Step {step+1}: loss = {l:.4f}\")\n\n# Generate text\ndef generate(params, seed_char, length=60):\n    h = jnp.zeros(hidden_dim)\n    idx = char2idx[seed_char]\n    result = [seed_char]\n    key = jax.random.PRNGKey(42)\n    for _ in range(length):\n        h, logits = rnn_step(params, h, idx)\n        key, subkey = jax.random.split(key)\n        idx = jax.random.categorical(subkey, logits)\n        result.append(idx2char[int(idx)])\n    return ''.join(result)\n\nprint(f\"\\nGenerated: {generate(params, 't')}\")\n</code></pre></p> </li> <li> <p>Implement a toy seq2seq model with Bahdanau attention for sequence reversal. Visualise the attention alignment matrix. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Task: reverse a sequence of digits (e.g., [3, 1, 4] -&gt; [4, 1, 3])\nvocab_size = 10  # digits 0-9\nSOS, EOS = 10, 11  # special tokens\ntotal_vocab = 12\nembed_dim, hidden_dim = 16, 32\nmax_len = 5\n\nkey = jax.random.PRNGKey(42)\nkeys = jax.random.split(key, 8)\n\nparams = {\n    'embed': jax.random.normal(keys[0], (total_vocab, embed_dim)) * 0.1,\n    'enc_Wx': jax.random.normal(keys[1], (embed_dim, hidden_dim)) * 0.1,\n    'enc_Wh': jax.random.normal(keys[2], (hidden_dim, hidden_dim)) * 0.05,\n    'dec_Wx': jax.random.normal(keys[3], (embed_dim, hidden_dim)) * 0.1,\n    'dec_Wh': jax.random.normal(keys[4], (hidden_dim, hidden_dim)) * 0.05,\n    # Bahdanau attention\n    'Ws': jax.random.normal(keys[5], (hidden_dim, hidden_dim)) * 0.1,\n    'Wh_att': jax.random.normal(keys[6], (hidden_dim, hidden_dim)) * 0.1,\n    'v_att': jax.random.normal(keys[7], (hidden_dim,)) * 0.1,\n    # Output projection (from hidden + context to vocab)\n    'Wo': jax.random.normal(keys[0], (hidden_dim * 2, total_vocab)) * 0.1,\n}\n\ndef encode(params, seq):\n    \"\"\"Encode input sequence, return all hidden states.\"\"\"\n    h = jnp.zeros(hidden_dim)\n    states = []\n    for t in range(len(seq)):\n        x = params['embed'][seq[t]]\n        h = jnp.tanh(x @ params['enc_Wx'] + h @ params['enc_Wh'])\n        states.append(h)\n    return jnp.stack(states), h\n\ndef bahdanau_attention(params, dec_state, enc_states):\n    \"\"\"Compute Bahdanau attention weights and context vector.\"\"\"\n    scores = jnp.tanh(enc_states @ params['Wh_att'] + dec_state @ params['Ws'])\n    e = scores @ params['v_att']  # (src_len,)\n    alpha = jax.nn.softmax(e)\n    context = alpha @ enc_states\n    return context, alpha\n\ndef decode_step(params, dec_h, prev_token, enc_states):\n    x = params['embed'][prev_token]\n    dec_h = jnp.tanh(x @ params['dec_Wx'] + dec_h @ params['dec_Wh'])\n    context, alpha = bahdanau_attention(params, dec_h, enc_states)\n    combined = jnp.concatenate([dec_h, context])\n    logits = combined @ params['Wo']\n    return dec_h, logits, alpha\n\ndef seq2seq_loss(params, src, tgt):\n    enc_states, enc_final = encode(params, src)\n    dec_h = enc_final\n    loss = 0.0\n    prev_token = SOS\n    for t in range(len(tgt)):\n        dec_h, logits, _ = decode_step(params, dec_h, prev_token, enc_states)\n        log_probs = jax.nn.log_softmax(logits)\n        loss -= log_probs[tgt[t]]\n        prev_token = tgt[t]\n    return loss / len(tgt)\n\n# Generate training data: reverse sequences\nkey = jax.random.PRNGKey(0)\ntrain_srcs, train_tgts = [], []\nfor _ in range(200):\n    key, subkey = jax.random.split(key)\n    length = jax.random.randint(subkey, (), 3, max_len + 1)\n    key, subkey = jax.random.split(key)\n    seq = jax.random.randint(subkey, (int(length),), 0, vocab_size)\n    train_srcs.append(seq)\n    train_tgts.append(seq[::-1])  # reverse\n\n# Training\ngrad_fn = jax.grad(seq2seq_loss)\nlr = 0.01\n\nfor epoch in range(100):\n    total_loss = 0.0\n    for src, tgt in zip(train_srcs, train_tgts):\n        grads = grad_fn(params, src, tgt)\n        params = {k: params[k] - lr * grads[k] for k in params}\n        total_loss += seq2seq_loss(params, src, tgt)\n    if (epoch + 1) % 20 == 0:\n        print(f\"Epoch {epoch+1}: avg loss = {total_loss / len(train_srcs):.4f}\")\n\n# Visualise attention for one example\ntest_src = jnp.array([3, 1, 4, 1, 5])\ntest_tgt = test_src[::-1]\n\nenc_states, enc_final = encode(params, test_src)\ndec_h = enc_final\nattentions = []\nprev_token = SOS\nfor t in range(len(test_tgt)):\n    dec_h, logits, alpha = decode_step(params, dec_h, prev_token, enc_states)\n    attentions.append(alpha)\n    prev_token = test_tgt[t]\n\natt_matrix = jnp.stack(attentions)\nfig, ax = plt.subplots(figsize=(6, 5))\nim = ax.imshow(att_matrix, cmap='Blues')\nax.set_xlabel(\"Source position\"); ax.set_ylabel(\"Target position\")\nsrc_labels = [str(int(x)) for x in test_src]\ntgt_labels = [str(int(x)) for x in test_tgt]\nax.set_xticks(range(len(src_labels))); ax.set_xticklabels(src_labels)\nax.set_yticks(range(len(tgt_labels))); ax.set_yticklabels(tgt_labels)\nfor i in range(len(tgt_labels)):\n    for j in range(len(src_labels)):\n        ax.text(j, i, f\"{att_matrix[i,j]:.2f}\", ha='center', va='center', fontsize=9)\nax.set_title(\"Bahdanau Attention Alignment (sequence reversal)\")\nplt.colorbar(im); plt.tight_layout(); plt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2007%3A%20computational%20linguistics/04.%20transformers%20and%20language%20models/","title":"Transformers and Language Models","text":"<p>Transformers replaced recurrence with self-attention and became the dominant architecture for language understanding and generation. This file covers BERT, GPT, T5, positional encodings (sinusoidal, RoPE), pre-training objectives (MLM, CLM), fine-tuning, prompt engineering, and scaling laws -- the blueprint behind modern LLMs.</p> <ul> <li> <p>In chapter 06, we introduced the Transformer architecture: self-attention, multi-head attention, positional encoding, and the encoder-decoder structure. Here we focus on how transformers are adapted for specific NLP paradigms, the models that defined modern NLP (BERT, GPT, T5), and the techniques that make them practical at scale.</p> </li> <li> <p>Recall the core operation: scaled dot-product attention computes \\(\\text{softmax}(QK^T / \\sqrt{d_k}) V\\), where queries, keys, and values are linear projections of the input. Multi-head attention runs \\(h\\) parallel attention heads, each with different learned projections, and concatenates the results. The Transformer block wraps this with residual connections, layer normalisation, and a position-wise feed-forward network (chapter 06).</p> </li> <li> <p>A subtle but important architectural choice is the placement of layer normalisation. The original Transformer uses post-norm: the residual and normalisation come after the sublayer, as \\(\\text{LayerNorm}(x + \\text{Sublayer}(x))\\). </p> </li> <li> <p>Most modern models use pre-norm: normalise before the sublayer, as \\(x + \\text{Sublayer}(\\text{LayerNorm}(x))\\). Pre-norm is more stable during training because the residual connection passes gradients directly through the identity path without them being affected by the normalisation. This makes it easier to train very deep models without careful learning rate warmup.</p> </li> <li> <p>The feed-forward sublayer in each Transformer block is a two-layer MLP applied independently to each token position:</p> </li> </ul> \\[\\text{FFN}(x) = W_2 \\cdot \\text{GELU}(W_1 x + b_1) + b_2\\] <ul> <li> <p>The inner dimension is typically 4 times the model dimension (e.g., \\(d_{\\text{model}} = 768\\), \\(d_{\\text{ff}} = 3072\\)). This FFN accounts for about two-thirds of the parameters in each block and is thought to function as a key-value memory that stores factual knowledge learned during training.</p> </li> <li> <p>Positional encoding gives the model information about token order, since attention itself is permutation-equivariant. The original sinusoidal encoding (chapter 06) uses fixed sine and cosine functions at different frequencies. Learned positional embeddings simply add a trainable vector for each position (used in BERT and GPT-2). Both are absolute encodings: position 5 gets the same vector regardless of context.</p> </li> <li> <p>Rotary Position Embedding (RoPE) encodes position by rotating the query and key vectors in 2D subspaces. For a pair of dimensions \\((q_{2i}, q_{2i+1})\\), the rotation by angle \\(m\\theta_i\\) (where \\(m\\) is the position and \\(\\theta_i = 10000^{-2i/d}\\)) applies:</p> </li> </ul> \\[ \\begin{bmatrix} q'_{2i} \\\\ q'_{2i+1} \\end{bmatrix} = \\begin{bmatrix} \\cos m\\theta_i &amp; -\\sin m\\theta_i \\\\ \\sin m\\theta_i &amp; \\cos m\\theta_i \\end{bmatrix} \\begin{bmatrix} q_{2i} \\\\ q_{2i+1} \\end{bmatrix} \\] <p></p> <ul> <li> <p>The beauty of RoPE is that the dot product \\(q'^T k'\\) between rotated queries and keys depends only on the relative position \\(m - n\\), not the absolute positions. </p> </li> <li> <p>To see why, write the rotation as \\(q' = R_m q\\) and \\(k' = R_n k\\), where \\(R_m\\) is a block-diagonal rotation matrix. The attention score becomes:</p> </li> </ul> \\[q'^T k' = (R_m q)^T (R_n k) = q^T R_m^T R_n \\, k = q^T R_{n-m} \\, k\\] <ul> <li> <p>The last step follows from the rotation group property: \\(R_m^T R_n = R_{n-m}\\) (rotating back by \\(m\\) then forward by \\(n\\) equals rotating by \\(n - m\\)). </p> </li> <li> <p>This means the attention score depends only on the relative distance \\(n - m\\), not the absolute positions \\(m\\) and \\(n\\) individually. </p> </li> <li> <p>The model gains a natural notion of distance without any learned position parameters and can generalise to sequence lengths not seen during training.</p> </li> <li> <p>ALiBi (Attention with Linear Biases) takes an even simpler approach: it adds a fixed linear penalty to the attention scores based on distance, as \\(\\text{score}_{ij} = q_i^T k_j - m \\cdot |i - j|\\), where \\(m\\) is a head-specific slope. Different heads use different slopes, allowing some heads to focus locally and others globally. ALiBi requires no learned parameters for position and generalises well to sequences longer than those seen during training.</p> </li> <li> <p>The three dominant paradigms for Transformer-based language models are encoder-only, decoder-only, and encoder-decoder. They differ in what the model can see (the attention mask) and how they are trained.</p> </li> </ul> <p></p> <ul> <li> <p>BERT (Bidirectional Encoder Representations from Transformers, Devlin et al., 2019) is the canonical encoder-only model. It processes text with full bidirectional attention: every token can attend to every other token, both left and right. This gives BERT rich contextual representations but means it cannot generate text autoregressively.</p> </li> <li> <p>BERT is pre-trained with two objectives. Masked language modelling (MLM) randomly masks 15% of input tokens and trains the model to predict them. Of the selected tokens, 80% are replaced with a [MASK] token, 10% with a random word, and 10% are left unchanged (to prevent the model from learning to only predict when it sees [MASK]). The training objective is:</p> </li> </ul> \\[\\mathcal{L}_{\\text{MLM}} = -\\sum_{i \\in \\mathcal{M}} \\log P(w_i \\mid w_{\\backslash \\mathcal{M}})\\] <ul> <li>where \\(\\mathcal{M}\\) is the set of masked positions and \\(w_{\\backslash \\mathcal{M}}\\) is the sentence with those positions masked. This is a denoising objective: the model learns to reconstruct corrupted input.</li> </ul> <p></p> <ul> <li> <p>Next Sentence Prediction (NSP) trains BERT to predict whether two sentences are consecutive in the original text. A special [CLS] token at the start of the input is used for this binary classification. NSP was included to help with tasks like question answering that require understanding sentence relationships, though later work (RoBERTa) showed it contributes little and can be dropped.</p> </li> <li> <p>BERT's pre-trained representations are adapted to downstream tasks by adding a task-specific head (a simple linear layer) on top and fine-tuning the entire model. For classification tasks, the [CLS] token representation is used. For token-level tasks (NER, POS tagging), each token's representation is used. This fine-tuning approach transfers the linguistic knowledge learned during pre-training to new tasks with relatively little labelled data.</p> </li> <li> <p>GPT (Generative Pre-trained Transformer, Radford et al., 2018) is the canonical decoder-only model. It uses causal (autoregressive) attention: each token can only attend to tokens at earlier positions (and itself). This is enforced by masking future positions in the attention matrix (setting their scores to \\(-\\infty\\) before the softmax). The training objective is simple causal language modelling: predict the next token given all previous tokens.</p> </li> </ul> \\[\\mathcal{L}_{\\text{CLM}} = -\\sum_{i=1}^{n} \\log P(w_i \\mid w_1, \\ldots, w_{i-1})\\] <ul> <li> <p>This is the same n-gram language model objective from file 02, but with a Transformer parameterisation that can condition on the entire preceding context rather than just the last \\(k-1\\) tokens.</p> </li> <li> <p>GPT-2 scaled this up to 1.5 billion parameters and demonstrated strong zero-shot performance: without any fine-tuning, it could perform tasks by conditioning on a natural language prompt (\"Translate English to French: ...\"). </p> </li> <li> <p>GPT-3 (175 billion parameters) showed that scale alone could enable in-context learning: by providing a few input-output examples in the prompt, the model could perform new tasks without any gradient updates.</p> </li> <li> <p>Encoder-decoder models like T5 (Text-to-Text Transfer Transformer, Raffel et al., 2020) frame every NLP task as text-to-text: the input is a text string (possibly with a task prefix like \"translate English to German:\") and the output is a text string. The encoder processes the input with bidirectional attention, and the decoder generates the output autoregressively with cross-attention to the encoder.</p> </li> <li> <p>T5 is pre-trained with span corruption: random contiguous spans of tokens are replaced with sentinel tokens, and the model must generate the original tokens. For example, \"The cat sat on the mat\" might become \"The [X] on [Y]\" as input, and the target is \"[X] cat sat [Y] the mat\". This is a generalisation of BERT's MLM to spans rather than individual tokens.</p> </li> <li> <p>BART (Lewis et al., 2020) is another encoder-decoder model pre-trained with a denoising objective, but it applies a broader set of corruption strategies: token masking, token deletion, span masking, sentence permutation, and document rotation. The diversity of corruption forces the model to learn more robust representations.</p> </li> <li> <p>As language models grow larger, full fine-tuning (updating all parameters) becomes impractical: a 175B parameter model requires hundreds of gigabytes just to store the optimizer states. Parameter-efficient fine-tuning (PEFT) methods adapt only a small fraction of parameters.</p> </li> <li> <p>Adapters insert small bottleneck layers (typically two linear layers with a nonlinearity: down-project to a small dimension, then up-project back) between the existing Transformer layers. Only the adapter weights are trained; the original model weights are frozen. This adds less than 5% new parameters while matching full fine-tuning performance on most tasks.</p> </li> <li> <p>LoRA (Low-Rank Adaptation) modifies the weight matrices themselves without adding new layers. Instead of updating the full weight matrix \\(W\\), LoRA learns a low-rank decomposition of the update: \\(W' = W + BA\\), where \\(B\\) is \\(d \\times r\\) and \\(A\\) is \\(r \\times d\\) with \\(r \\ll d\\) (typically \\(r = 4\\) to \\(r = 64\\)). The original \\(W\\) is frozen; only \\(A\\) and \\(B\\) are trained. At inference time, the update can be merged into the original weights with no additional latency:</p> </li> </ul> \\[W' = W + BA\\] <p></p> <ul> <li> <p>Prefix tuning prepends a sequence of learnable \"virtual tokens\" to the key and value matrices of each attention layer. The model attends to these prefix vectors as if they were real tokens, and only the prefix parameters are trained. This is similar to prompt tuning but operates in the activation space rather than the embedding space.</p> </li> <li> <p>Prompt engineering is the art of designing input text that elicits the desired behaviour from a pre-trained model without any parameter updates. </p> <ul> <li> <p>Zero-shot prompting describes the task in natural language (\"Classify the sentiment of the following review:\"). </p> </li> <li> <p>Few-shot prompting provides input-output examples before the actual query. </p> </li> <li> <p>Chain-of-thought (CoT) prompting adds \"Let's think step by step\" or includes reasoning traces in the examples, which dramatically improves performance on arithmetic and logical reasoning tasks by guiding the model to decompose problems.</p> </li> </ul> </li> <li> <p>In-context learning (ICL) is the phenomenon where large language models can learn to perform tasks from examples provided in the prompt, without any gradient updates. The model's weights do not change; it uses the examples as a kind of implicit specification. </p> </li> <li> <p>How ICL works mechanically remains an active research question; one hypothesis is that the attention layers implement a form of gradient descent in their forward pass, effectively \"training\" on the in-context examples.</p> </li> <li> <p>Scaling laws describe predictable relationships between model size, data size, compute budget, and performance (measured by loss). Kaplan et al. (2020) found that loss follows a power law in each variable:</p> </li> </ul> \\[L(N) \\propto N^{-\\alpha_N}, \\quad L(D) \\propto D^{-\\alpha_D}, \\quad L(C) \\propto C^{-\\alpha_C}\\] <ul> <li>where \\(N\\) is the number of parameters, \\(D\\) is the dataset size, and \\(C\\) is the compute budget. These power laws hold over many orders of magnitude and suggest that simply scaling up yields predictable improvements.</li> </ul> <p></p> <ul> <li>The Chinchilla scaling laws (Hoffmann et al., 2022) revised this by showing that most large models are undertrained. For a fixed compute budget \\(C\\), the optimal allocation scales model size and training data equally:</li> </ul> \\[N_{\\text{opt}} \\propto C^{0.5}, \\quad D_{\\text{opt}} \\propto C^{0.5}\\] <ul> <li> <p>This means that if you double your compute budget, you should increase both model size and dataset size by a factor of \\(\\sqrt{2}\\), not just make the model bigger. </p> </li> <li> <p>Kaplan et al. had recommended scaling \\(N\\) faster than \\(D\\), which led to very large but undertrained models. Chinchilla (70B parameters, 1.4T tokens) matched the performance of Gopher (280B parameters, 300B tokens) with the same compute budget, demonstrating that the earlier models were severely data-starved. </p> </li> <li> <p>The practical rule of thumb: train on roughly 20 tokens per parameter.</p> </li> <li> <p>Mixture of Experts (MoE) is an architecture that scales model capacity without proportionally scaling computation. Instead of one large feed-forward layer, MoE uses multiple expert FFN layers and a gating network (router) that selects which experts to activate for each token.</p> </li> <li> <p>The gating function computes a routing score for each expert and selects the top-\\(k\\) (typically \\(k = 1\\) or \\(k = 2\\)):</p> </li> </ul> \\[G(x) = \\text{TopK}(\\text{softmax}(W_g x))\\] <ul> <li>Only the selected experts process the token, so the computational cost scales with \\(k\\) (the number of active experts) rather than the total number of experts \\(E\\). A model with 8 experts and top-2 routing has 4x the parameters of a dense model but only 2x the computation.</li> </ul> <p></p> <ul> <li>A critical challenge in MoE is load balancing: if the router sends most tokens to a few popular experts, the others are wasted. Training adds an auxiliary load balancing loss that encourages uniform expert utilisation:</li> </ul> \\[\\mathcal{L}_{\\text{balance}} = E \\cdot \\sum_{i=1}^{E} f_i \\cdot p_i\\] <ul> <li> <p>where \\(f_i\\) is the fraction of tokens assigned to expert \\(i\\) and \\(p_i\\) is the average routing probability for expert \\(i\\). This product is minimised when both the token fractions and probabilities are uniform (each equal to \\(1/E\\)).</p> </li> <li> <p>Expert parallelism distributes different experts across different accelerators. During the forward pass, an all-to-all communication step routes tokens to the device hosting their assigned expert, then routes the results back. This communication cost is the main engineering challenge of MoE at scale. Models like Switch Transformer, Mixtral, and GShard use MoE to achieve strong performance with practical inference costs.</p> </li> <li> <p>Building models is half the job; measuring whether they work is the other half. NLP evaluation is uniquely difficult because language is ambiguous, subjective, and open-ended.</p> </li> <li> <p>A translation can be correct in many different ways. A summary can be good even if it shares no exact words with a reference.</p> </li> <li> <p>A chatbot response can be helpful, harmless, and honest, yet reasonable humans will disagree.</p> </li> <li> <p>Exact match (EM) is the simplest metric: does the model's output exactly match the gold answer? It is used for tasks with short, unambiguous answers like extractive question answering (SQuAD) or closed-form maths.</p> </li> <li> <p>EM is harsh; \"New York City\" and \"new york city\" fail to match unless normalisation is applied \u2014 but its simplicity makes it unambiguous.</p> </li> <li> <p>Token-level metrics treat NLP as a classification problem at the token level, using precision, recall, and F1 from chapter 06.</p> </li> <li> <p>Precision measures what fraction of the model's predicted tokens are correct: \\(P = \\text{TP} / (\\text{TP} + \\text{FP})\\). A model that predicts very few entities but gets them all right has high precision.</p> </li> <li> <p>Recall measures what fraction of the gold tokens the model found: \\(R = \\text{TP} / (\\text{TP} + \\text{FN})\\). A model that predicts every token as an entity has perfect recall but terrible precision.</p> </li> <li> <p>F1 is the harmonic mean of precision and recall:</p> </li> </ul> \\[F_1 = \\frac{2PR}{P + R}\\] <ul> <li> <p>The harmonic mean (rather than arithmetic) penalises imbalance: if either \\(P\\) or \\(R\\) is low, \\(F_1\\) is low. For NER (file 02), F1 is computed per entity type and then macro-averaged across types. For POS tagging, token-level accuracy is more common because every token gets a tag.</p> </li> <li> <p>Span-level F1 (used in SQuAD) compares the set of tokens in the predicted span to the set in the gold span. This is more forgiving than exact match: if the gold answer is \"the Eiffel Tower\" and the model predicts \"Eiffel Tower\", the span F1 is high (4 overlapping tokens out of 5) even though EM is zero.</p> </li> <li> <p>BLEU (Bilingual Evaluation Understudy, Papineni et al., 2002) is the classic metric for machine translation. It measures n-gram overlap between the candidate translation and one or more reference translations. The score combines precision at multiple n-gram levels (unigram through 4-gram) with a brevity penalty:</p> </li> </ul> \\[\\text{BLEU} = \\text{BP} \\cdot \\exp\\!\\left(\\sum_{n=1}^{N} w_n \\log p_n\\right)\\] <ul> <li> <p>where \\(p_n\\) is the modified n-gram precision: the count of each n-gram in the candidate is clipped to its maximum count in any reference, preventing a degenerate candidate like \"the the the the\" from scoring high. The weights \\(w_n\\) are typically uniform (\\(w_n = 1/N\\), with \\(N = 4\\)).</p> </li> <li> <p>The brevity penalty \\(\\text{BP} = \\min(1, \\exp(1 - r/c))\\) penalises candidates shorter than the reference (\\(c\\) is candidate length, \\(r\\) is reference length). Without this, a model could achieve high precision by outputting very few, very safe words.</p> </li> <li> <p>BLEU correlates reasonably with human judgement at the corpus level (averaged over many sentences) but poorly at the sentence level.</p> </li> <li> <p>It rewards exact n-gram matches and misses valid paraphrases: \"the cat is on the mat\" and \"a feline sits atop the rug\" have zero bigram overlap despite meaning the same thing.</p> </li> <li> <p>BLEU also ignores recall entirely \u2014 a candidate that produces only the most common words scores well on precision.</p> </li> <li> <p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation, Lin, 2004) is the standard metric for summarisation. Unlike BLEU, which emphasises precision, ROUGE emphasises recall: what fraction of the reference n-grams appear in the candidate?</p> </li> <li> <p>ROUGE-N computes recall of n-grams: \\(\\text{ROUGE-N} = \\frac{|\\text{n-grams}_{\\text{ref}} \\cap \\text{n-grams}_{\\text{cand}}|}{|\\text{n-grams}_{\\text{ref}}|}\\). ROUGE-1 (unigram) and ROUGE-2 (bigram) are most common.</p> </li> <li> <p>ROUGE-L uses the longest common subsequence (LCS) between candidate and reference, which captures sentence-level word ordering without requiring consecutive matches.</p> </li> <li> <p>The LCS length normalised by reference length gives recall, normalised by candidate length gives precision, and the F-measure combines them.</p> </li> <li> <p>LCS is computed via dynamic programming in \\(O(mn)\\) time (similar to edit distance from file 02):</p> </li> </ul> \\[R_{\\text{LCS}} = \\frac{\\text{LCS}(X, Y)}{m}, \\quad P_{\\text{LCS}} = \\frac{\\text{LCS}(X, Y)}{n}, \\quad F_{\\text{LCS}} = \\frac{(1 + \\beta^2) R_{\\text{LCS}} P_{\\text{LCS}}}{R_{\\text{LCS}} + \\beta^2 P_{\\text{LCS}}}\\] <ul> <li> <p>where \\(m\\) and \\(n\\) are the lengths of reference and candidate, and \\(\\beta\\) is typically set to favour recall (\\(\\beta \\to \\infty\\) gives pure recall).</p> </li> <li> <p>METEOR (Metric for Evaluation of Translation with Explicit ORdering, Banerjee and Lavie, 2005) addresses BLEU's weaknesses by incorporating synonyms, stemming, and word order.</p> </li> <li> <p>It first aligns words between candidate and reference using exact matches, stem matches (via Porter stemming from file 02), and synonym matches (via WordNet from file 01).</p> </li> <li> <p>Then it computes a harmonic mean of unigram precision and recall weighted toward recall, and applies a fragmentation penalty that penalises candidates where matched words appear in a different order than the reference.</p> </li> <li> <p>ChrF (Character n-gram F-score) computes F-score over character n-grams rather than word n-grams. This makes it robust to morphological variation (critical for agglutinative languages from file 01) and partially handles tokenisation differences. ChrF++ adds word bigrams to the character n-grams.</p> </li> <li> <p>It has become a recommended metric for machine translation alongside BLEU, especially for morphologically rich languages.</p> </li> <li> <p>Perplexity (file 02) measures how well a language model predicts a held-out test set. It is the standard intrinsic metric for language models: \\(\\text{PPL} = \\exp(-\\frac{1}{N} \\sum_{i} \\log P(w_i \\mid w_{&lt;i}))\\). Lower is better.</p> </li> <li> <p>Perplexity is comparable only between models using the same tokenisation, since different tokenisers produce different sequence lengths \\(N\\) for the same text.</p> </li> <li> <p>A model with a larger vocabulary tends to have lower perplexity per token but processes fewer tokens per sentence.</p> </li> <li> <p>Bits-per-byte (BPB) normalises by the number of UTF-8 bytes in the text rather than the number of tokens, making it tokenisation-independent:</p> </li> </ul> \\[ \\text{BPB} = \\frac{-\\sum_{i} \\log_2 P(w_i \\mid w_{ <ul> <li>BERTScore (Zhang et al., 2020) moves beyond surface-level n-gram matching by computing similarity in embedding space. Each token in the candidate is matched to its most similar token in the reference using cosine similarity of contextual embeddings (typically from a pre-trained BERT model). The scores are aggregated into precision, recall, and F1:</li> </ul> \\[R_{\\text{BERT}} = \\frac{1}{|r|} \\sum_{r_i \\in r} \\max_{c_j \\in c} \\cos(r_i, c_j), \\quad P_{\\text{BERT}} = \\frac{1}{|c|} \\sum_{c_j \\in c} \\max_{r_i \\in r} \\cos(c_j, r_i)\\] <ul> <li> <p>where \\(r_i\\) and \\(c_j\\) are contextual embeddings of reference and candidate tokens. This captures semantic similarity that n-gram metrics miss: \"automobile\" and \"car\" score highly because their BERT embeddings are similar, even though they share no characters.</p> </li> <li> <p>BLEURT (Sellam et al., 2020) takes this further by fine-tuning a BERT model directly on human quality judgements. Given a reference and candidate pair, it outputs a scalar quality score. BLEURT is trained on synthetic data (random perturbations of reference translations rated by metrics like BLEU and METEOR) and then fine-tuned on human ratings. It correlates better with human judgement than any surface-level metric.</p> </li> <li> <p>COMET (Crosslingual Optimized Metric for Evaluation of Translation, Rei et al., 2020) is a learned metric for machine translation that conditions on the source sentence, reference, and candidate \u2014 not just reference and candidate. It uses a multilingual encoder (XLM-R) to embed all three and predicts a quality score. By seeing the source, COMET can detect meaning errors that reference-only metrics miss (e.g., a fluent but factually wrong translation).</p> </li> <li> <p>LLM-as-judge is the modern approach to evaluation at scale. Instead of computing metrics against references, a powerful language model (GPT-4, Claude) is prompted to evaluate the quality of model outputs. The judge receives the input, the model's response, and optionally a reference answer, and produces a rating (e.g., 1-5) or a pairwise preference (response A is better than response B).</p> </li> <li> <p>Pairwise comparison (used in Chatbot Arena) is the most reliable LLM-as-judge format. The judge sees two responses and picks the better one, rather than assigning absolute scores. This avoids calibration issues (different judges may have different baselines for \"3 out of 5\"). Results are aggregated into Elo ratings (from chess), where each model starts with a base rating and gains or loses points based on wins and losses against other models. The expected win probability of model \\(A\\) against model \\(B\\) is:</p> </li> </ul> \\[P(A \\succ B) = \\frac{1}{1 + 10^{(R_B - R_A) / 400}}\\] <ul> <li> <p>where \\(R_A, R_B\\) are the Elo ratings. After each comparison, ratings are updated: \\(R_A' = R_A + K(S - P(A \\succ B))\\), where \\(S \\in \\{0, 1\\}\\) is the actual outcome and \\(K\\) controls the update magnitude. Models that consistently beat strong opponents rise quickly; models that lose to weak opponents fall.</p> </li> <li> <p>Position bias is a known issue with LLM judges: they tend to prefer the response presented first (or in some models, the response presented second). Swapping (evaluating each pair twice with responses in both orders) and averaging the results mitigates this.</p> </li> <li> <p>Verbosity bias is another: judges tend to prefer longer, more detailed responses even when a concise answer is better.</p> </li> <li> <p>Self-consistency checks whether the judge gives the same rating across multiple evaluations of the same input. High variance indicates the evaluation signal is noisy.</p> </li> <li> <p>Inter-annotator agreement (Cohen's kappa or Krippendorff's alpha) measures whether multiple judges agree, providing an upper bound on evaluation reliability.</p> </li> <li> <p>Contamination is a critical concern: if the evaluation data appeared in the model's training set, benchmark scores are inflated and meaningless.</p> </li> <li> <p>This is especially problematic for LLMs trained on web-scraped data, where popular benchmarks are likely present. Mitigation strategies include: using held-out test sets that are not publicly released, creating dynamic benchmarks that regenerate questions periodically, canary strings (unique identifiers embedded in benchmark data to detect leakage), and comparing performance on contaminated vs clean subsets.</p> </li> <li> <p>Standard NLU benchmarks evaluate language understanding across diverse tasks.</p> </li> <li> <p>GLUE (General Language Understanding Evaluation) and SuperGLUE are multi-task benchmarks covering sentiment (SST-2), textual similarity (STS-B), natural language inference (MNLI, RTE), coreference (WSC), and question answering (BoolQ).</p> </li> <li> <p>Models are evaluated on each task separately and scored by an aggregate metric. GLUE is now considered saturated (models exceed human performance on most tasks); SuperGLUE remains more challenging.</p> </li> <li> <p>MMLU (Massive Multitask Language Understanding) evaluates knowledge and reasoning across 57 academic subjects (mathematics, history, law, medicine, computer science, etc.) using multiple-choice questions.</p> </li> <li> <p>It tests whether a model has absorbed broad knowledge during pre-training. Scores are reported per subject and as a macro-average.</p> </li> <li> <p>MMLU-Pro adds harder, multi-step reasoning questions with 10 answer choices instead of 4.</p> </li> <li> <p>HellaSwag tests commonsense reasoning by asking the model to choose the most plausible continuation of a scenario. The wrong answers are generated adversarially (using models) to be superficially plausible but semantically wrong.</p> </li> <li> <p>WinoGrande tests commonsense coreference resolution with minimal pairs that differ by one word.</p> </li> <li> <p>ARC (AI2 Reasoning Challenge) uses grade-school science questions in easy and challenge sets, testing factual and reasoning ability.</p> </li> <li> <p>Reasoning and maths benchmarks evaluate the problem-solving capabilities that separate strong LLMs from weak ones.</p> </li> <li> <p>GSM8K (Grade School Math 8K) contains 8,500 elementary maths word problems requiring multi-step arithmetic reasoning. It is the standard benchmark for basic mathematical reasoning and for evaluating chain-of-thought prompting (file 04).</p> </li> <li> <p>MATH is a harder dataset of competition-level maths problems across algebra, number theory, geometry, counting, and probability. Problems require multi-step symbolic reasoning, and MATH-500 is a commonly reported 500-problem subset.</p> </li> <li> <p>AIME (American Invitational Mathematics Examination) problems are competition-level: solving them correctly requires deep mathematical reasoning over many steps. DeepSeek-R1 scores 79.8% on AIME 2024, demonstrating that RL-trained reasoning models (file 05) can approach strong human competitors.</p> </li> <li> <p>HumanEval and MBPP (Mostly Basic Programming Problems) evaluate code generation by checking whether the model's code passes unit tests. HumanEval contains 164 Python problems with function signatures and docstrings; the model must generate the function body.</p> </li> <li> <p>The metric is pass@k: the probability that at least one of \\(k\\) generated solutions passes all tests. For a single sample:</p> </li> </ul> \\[\\text{pass@}k = 1 - \\frac{\\binom{n-c}{k}}{\\binom{n}{k}}\\] <ul> <li> <p>where \\(n\\) is the total number of generated samples and \\(c\\) is the number that pass. This formula corrects for the bias in simply taking the best of \\(k\\) samples.</p> </li> <li> <p>SWE-bench goes further, evaluating whether models can resolve real GitHub issues by modifying existing codebases \u2014 a much harder test of practical software engineering ability.</p> </li> <li> <p>GPQA (Graduate-Level Google-Proof QA) contains expert-level questions in biology, physics, and chemistry that are difficult even for domain experts. It tests whether models have genuine understanding rather than pattern matching. The \"Diamond\" subset is the hardest.</p> </li> <li> <p>Safety and alignment benchmarks evaluate whether models are helpful, harmless, and honest.</p> </li> <li> <p>TruthfulQA tests whether models reproduce common misconceptions. Questions are designed so that the most common internet answers are wrong (e.g., \"What happens if you swallow gum?\", the common myth is that it stays for 7 years, but the truthful answer is that it passes through normally). Models that have memorised popular but incorrect claims score poorly.</p> </li> <li> <p>BBQ (Bias Benchmark for QA) tests for social biases across categories like age, gender, race, and religion. Questions are structured so that a biased model would systematically choose stereotypical answers. Toxigen evaluates the model's tendency to generate toxic content about specific demographic groups.</p> </li> <li> <p>MT-Bench evaluates multi-turn conversation ability using 80 carefully designed questions across writing, roleplay, reasoning, maths, coding, extraction, STEM, and humanities. An LLM judge (GPT-4) scores responses on a 1-10 scale. The multi-turn format tests whether models can follow up, maintain context, and handle clarification requests.</p> </li> <li> <p>Chatbot Arena (LMSYS) uses real users to conduct blind pairwise comparisons between anonymous models. Users submit prompts and vote for the better response without knowing which model produced it. The resulting Elo leaderboard is considered the most ecologically valid evaluation of general-purpose LLM quality because it reflects real user preferences on diverse, uncurated prompts.</p> </li> <li> <p>AlpacaEval automates pairwise evaluation by comparing model outputs against a reference model (GPT-4) on a fixed set of instructions. A judge model determines the win rate.</p> </li> <li> <p>AlpacaEval 2.0 uses length-controlled win rates to correct for verbosity bias.</p> </li> <li> <p>Task-specific evaluation requires tailored metrics for specialised domains.</p> </li> <li> <p>Word Error Rate (WER) for speech recognition: \\(\\text{WER} = (S + D + I) / N\\), where \\(S\\), \\(D\\), \\(I\\) are substitution, deletion, and insertion errors and \\(N\\) is the number of reference words. This is the edit distance (file 02) normalised by reference length, applied at the word level.</p> </li> <li> <p>Slot F1 for task-oriented dialogue systems measures whether the model correctly extracts structured information from user utterances (e.g., extracting \"destination: Paris\" and \"date: tomorrow\" from \"Book me a flight to Paris tomorrow\").</p> </li> <li> <p>Citation accuracy for RAG systems (file 05) checks whether the model's generated citations actually support the claims made. A claim is verified against the retrieved passage, and the metric counts the fraction of claims that are fully, partially, or not supported.</p> </li> <li> <p>Evaluation pitfalls are common and can invalidate entire benchmark comparisons.</p> </li> <li> <p>Teaching to the test: optimising for benchmark performance rather than genuine capability. A model fine-tuned on MMLU-style multiple choice will score well on MMLU but may fail on the same questions posed in open-ended format.</p> </li> <li> <p>Metric gaming: models can be optimised to produce outputs that score well on automatic metrics (high BLEU, low perplexity) without being genuinely good. The BLEU-optimal translation is often a safe, generic paraphrase rather than a natural, fluent one.</p> </li> <li> <p>Benchmark saturation: when models approach or exceed human performance on a benchmark, the benchmark stops being informative. GLUE, SQuAD 1.1, and several others are now saturated.</p> </li> <li> <p>The field continuously creates harder benchmarks, but the cycle of creation, saturation, and replacement makes longitudinal comparison difficult.</p> </li> <li> <p>Human evaluation remains the gold standard but is expensive, slow, and hard to reproduce. Different annotator pools (crowdworkers vs domain experts, different cultures, different languages) produce different judgements. Reporting inter-annotator agreement and annotator demographics is essential for reproducibility.</p> </li> </ul>"},{"location":"chapter%2007%3A%20computational%20linguistics/04.%20transformers%20and%20language%20models/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement a full Transformer encoder block from scratch (multi-head attention, feed-forward, residual connections, layer norm). Apply it to a simple sequence classification task. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef layer_norm(x, gamma, beta, eps=1e-5):\n    mean = x.mean(axis=-1, keepdims=True)\n    var = x.var(axis=-1, keepdims=True)\n    return gamma * (x - mean) / jnp.sqrt(var + eps) + beta\n\ndef multi_head_attention(Q, K, V, W_q, W_k, W_v, W_o, n_heads):\n    B, T, D = Q.shape\n    head_dim = D // n_heads\n\n    q = Q @ W_q  # (B, T, D)\n    k = K @ W_k\n    v = V @ W_v\n\n    # Reshape to (B, n_heads, T, head_dim)\n    q = q.reshape(B, T, n_heads, head_dim).transpose(0, 2, 1, 3)\n    k = k.reshape(B, T, n_heads, head_dim).transpose(0, 2, 1, 3)\n    v = v.reshape(B, T, n_heads, head_dim).transpose(0, 2, 1, 3)\n\n    scores = q @ k.transpose(0, 1, 3, 2) / jnp.sqrt(head_dim)\n    weights = jax.nn.softmax(scores, axis=-1)\n    out = (weights @ v).transpose(0, 2, 1, 3).reshape(B, T, D)\n    return out @ W_o, weights\n\ndef transformer_block(x, params):\n    # Pre-norm multi-head self-attention\n    normed = layer_norm(x, params['ln1_g'], params['ln1_b'])\n    attn_out, weights = multi_head_attention(\n        normed, normed, normed,\n        params['W_q'], params['W_k'], params['W_v'], params['W_o'],\n        n_heads=4\n    )\n    x = x + attn_out\n\n    # Pre-norm feed-forward\n    normed = layer_norm(x, params['ln2_g'], params['ln2_b'])\n    ff = jax.nn.gelu(normed @ params['W1'] + params['b1'])\n    ff = ff @ params['W2'] + params['b2']\n    x = x + ff\n    return x, weights\n\n# Initialise parameters\nd_model, d_ff, n_heads = 32, 128, 4\nkey = jax.random.PRNGKey(42)\nkeys = jax.random.split(key, 10)\n\nparams = {\n    'W_q': jax.random.normal(keys[0], (d_model, d_model)) * 0.05,\n    'W_k': jax.random.normal(keys[1], (d_model, d_model)) * 0.05,\n    'W_v': jax.random.normal(keys[2], (d_model, d_model)) * 0.05,\n    'W_o': jax.random.normal(keys[3], (d_model, d_model)) * 0.05,\n    'ln1_g': jnp.ones(d_model), 'ln1_b': jnp.zeros(d_model),\n    'ln2_g': jnp.ones(d_model), 'ln2_b': jnp.zeros(d_model),\n    'W1': jax.random.normal(keys[4], (d_model, d_ff)) * 0.05,\n    'b1': jnp.zeros(d_ff),\n    'W2': jax.random.normal(keys[5], (d_ff, d_model)) * 0.05,\n    'b2': jnp.zeros(d_model),\n}\n\n# Test with random input\nx = jax.random.normal(keys[6], (2, 8, d_model))  # batch=2, seq_len=8\nout, attn_weights = transformer_block(x, params)\nprint(f\"Input shape:  {x.shape}\")\nprint(f\"Output shape: {out.shape}\")\nprint(f\"Attention weights shape: {attn_weights.shape}\")  # (B, n_heads, T, T)\n\n# Visualise attention patterns for each head\nfig, axes = plt.subplots(1, 4, figsize=(16, 3.5))\nfor h in range(4):\n    im = axes[h].imshow(attn_weights[0, h], cmap='Blues', vmin=0)\n    axes[h].set_title(f\"Head {h}\")\n    axes[h].set_xlabel(\"Key pos\"); axes[h].set_ylabel(\"Query pos\")\nplt.suptitle(\"Multi-Head Attention Patterns\")\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Implement causal (autoregressive) attention masking and compare it with bidirectional attention. Show how the mask prevents information from flowing from future to past tokens. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef attention(Q, K, V, mask=None):\n    d_k = Q.shape[-1]\n    scores = Q @ K.T / jnp.sqrt(d_k)\n    if mask is not None:\n        scores = jnp.where(mask, scores, -1e9)\n    weights = jax.nn.softmax(scores, axis=-1)\n    return weights @ V, weights\n\nseq_len, d_model = 6, 8\nkey = jax.random.PRNGKey(0)\nk1, k2, k3 = jax.random.split(key, 3)\nQ = jax.random.normal(k1, (seq_len, d_model))\nK = jax.random.normal(k2, (seq_len, d_model))\nV = jax.random.normal(k3, (seq_len, d_model))\n\n# Bidirectional (encoder-style): all positions visible\nbidir_mask = jnp.ones((seq_len, seq_len), dtype=bool)\nbidir_out, bidir_weights = attention(Q, K, V, bidir_mask)\n\n# Causal (decoder-style): only past and current positions visible\ncausal_mask = jnp.tril(jnp.ones((seq_len, seq_len), dtype=bool))\ncausal_out, causal_weights = attention(Q, K, V, causal_mask)\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\ntokens = [f\"t{i}\" for i in range(seq_len)]\n\naxes[0].imshow(bidir_weights, cmap='Blues', vmin=0, vmax=0.5)\naxes[0].set_title(\"Bidirectional Attention\\n(BERT-style)\")\naxes[0].set_xticks(range(seq_len)); axes[0].set_xticklabels(tokens)\naxes[0].set_yticks(range(seq_len)); axes[0].set_yticklabels(tokens)\n\naxes[1].imshow(causal_mask.astype(float), cmap='Greys', vmin=0, vmax=1)\naxes[1].set_title(\"Causal Mask\\n(1 = allowed, 0 = blocked)\")\naxes[1].set_xticks(range(seq_len)); axes[1].set_xticklabels(tokens)\naxes[1].set_yticks(range(seq_len)); axes[1].set_yticklabels(tokens)\n\naxes[2].imshow(causal_weights, cmap='Blues', vmin=0, vmax=0.5)\naxes[2].set_title(\"Causal Attention\\n(GPT-style)\")\naxes[2].set_xticks(range(seq_len)); axes[2].set_xticklabels(tokens)\naxes[2].set_yticks(range(seq_len)); axes[2].set_yticklabels(tokens)\n\nfor ax in axes:\n    ax.set_xlabel(\"Key\"); ax.set_ylabel(\"Query\")\nplt.tight_layout(); plt.show()\n\n# Verify: in causal attention, output at position i depends only on positions &lt;= i\nprint(\"Causal attention weight at position 2 (should only attend to 0, 1, 2):\")\nprint(f\"  Weights: {causal_weights[2]}\")\nprint(f\"  Sum of future weights (should be ~0): {causal_weights[2, 3:].sum():.6f}\")\n</code></pre></p> </li> <li> <p>Implement LoRA (Low-Rank Adaptation) and show how it modifies a weight matrix with far fewer trainable parameters than full fine-tuning. <pre><code>import jax\nimport jax.numpy as jnp\n\nd_model = 256\nrank = 4  # LoRA rank (much smaller than d_model)\n\nkey = jax.random.PRNGKey(42)\nk1, k2, k3 = jax.random.split(key, 3)\n\n# Original frozen weight matrix\nW_frozen = jax.random.normal(k1, (d_model, d_model)) * 0.02\n\n# LoRA matrices (only these are trainable)\nB = jnp.zeros((d_model, rank))       # initialised to zero\nA = jax.random.normal(k2, (rank, d_model)) * 0.01  # random init\n\n# Forward pass: W_effective = W_frozen + B @ A\nx = jax.random.normal(k3, (8, d_model))\n\n# Without LoRA\ny_original = x @ W_frozen.T\n\n# With LoRA\nW_effective = W_frozen + B @ A\ny_lora = x @ W_effective.T\n\n# Parameter counts\nfull_params = d_model * d_model\nlora_params = d_model * rank + rank * d_model  # B + A\n\nprint(f\"Model dimension: {d_model}\")\nprint(f\"LoRA rank: {rank}\")\nprint(f\"Full fine-tuning parameters: {full_params:,}\")\nprint(f\"LoRA parameters: {lora_params:,}\")\nprint(f\"Parameter reduction: {full_params / lora_params:.1f}x\")\nprint(f\"\\nSince B is initialised to zeros, initial LoRA output matches original:\")\nprint(f\"  Max difference: {jnp.abs(y_original - y_lora).max():.2e}\")\n\n# Simulate training: only update A and B\ndef lora_forward(A, B, W_frozen, x):\n    return x @ (W_frozen + B @ A).T\n\ndef dummy_loss(A, B, W_frozen, x, target):\n    pred = lora_forward(A, B, W_frozen, x)\n    return jnp.mean((pred - target) ** 2)\n\n# Target: some transformation of x\ntarget = x @ jax.random.normal(jax.random.PRNGKey(99), (d_model, d_model)).T * 0.02\n\ngrad_fn = jax.jit(jax.grad(dummy_loss, argnums=(0, 1)))\nlr = 0.01\n\nfor step in range(200):\n    gA, gB = grad_fn(A, B, W_frozen, x, target)\n    A = A - lr * gA\n    B = B - lr * gB\n\nloss_before = dummy_loss(jnp.zeros_like(A), jnp.zeros_like(B), W_frozen, x, target)\nloss_after = dummy_loss(A, B, W_frozen, x, target)\nprint(f\"\\nLoss before LoRA: {loss_before:.6f}\")\nprint(f\"Loss after LoRA:  {loss_after:.6f}\")\nprint(f\"Effective weight change rank: {jnp.linalg.matrix_rank(B @ A)}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2007%3A%20computational%20linguistics/05.%20advanced%20text%20generation/","title":"Advanced Text Generation","text":"<p>Advanced text generation goes beyond vanilla autoregressive decoding to improve quality, controllability, and speed. This file covers text diffusion models (D3PM, MDLM), OCR, RLHF and DPO for alignment, long-context methods (RoPE scaling, ring attention), retrieval-augmented generation, and speculative decoding for faster inference.</p> <ul> <li> <p>Standard autoregressive generation (file 04) produces text one token at a time, left to right. This is simple and effective, but it is inherently sequential, allows no global planning, and gives limited control over the output. This file covers methods that go beyond vanilla autoregressive decoding: diffusion models for text, optical character recognition, controllable generation through human feedback, handling long contexts, retrieval-augmented generation, and speculative decoding for faster inference.</p> </li> <li> <p>Text diffusion models apply the diffusion framework (introduced for images in chapter 08) to discrete text. The core challenge is that text is discrete: you cannot add continuous Gaussian noise to tokens the way you add noise to pixels. Several approaches address this.</p> </li> <li> <p>D3PM (Discrete Denoising Diffusion Probabilistic Models, Austin et al., 2021) defines a forward corruption process directly over discrete tokens using transition matrices. At each forward step, a token has some probability of being replaced by another token (uniform noise), masked (absorbing state), or staying the same. The reverse process learns to denoise, predicting the clean token from the corrupted one. The transition matrix \\(Q_t\\) at step \\(t\\) controls corruption:</p> </li> </ul> \\[q(x_t \\mid x_{t-1}) = \\text{Cat}(x_t ; \\, x_{t-1} Q_t)\\] <ul> <li>where \\(\\text{Cat}\\) denotes a categorical distribution and \\(x\\) is a one-hot vector. The multi-step forward process \\(q(x_t \\mid x_0)\\) has a closed form: \\(q(x_t \\mid x_0) = \\text{Cat}(x_t ; \\, x_0 \\bar{Q}_t)\\) where \\(\\bar{Q}_t = Q_1 Q_2 \\cdots Q_t\\) is the product of all transition matrices up to step \\(t\\). Training minimises a variational lower bound (ELBO) that decomposes across timesteps, similar to the continuous case (chapter 08):</li> </ul> \\[\\mathcal{L}_{\\text{D3PM}} = D_{\\text{KL}}(q(x_T \\mid x_0) \\| p(x_T)) + \\sum_{t=2}^{T} D_{\\text{KL}}(q(x_{t-1} \\mid x_t, x_0) \\| p_\\theta(x_{t-1} \\mid x_t)) - \\log p_\\theta(x_0 \\mid x_1)\\] <ul> <li> <p>The first term ensures the fully corrupted distribution matches the prior (uniform or all-mask). The sum of KL terms trains the model to reverse each corruption step: the true reverse posterior \\(q(x_{t-1} \\mid x_t, x_0)\\) can be computed in closed form using Bayes' rule and the known transition matrices, and the model \\(p_\\theta(x_{t-1} \\mid x_t)\\) is trained to match it. </p> </li> <li> <p>Since both distributions are categorical, the KL divergence is a simple sum over vocabulary entries. The final term measures reconstruction quality from the least corrupted state.</p> </li> <li> <p>MDLM (Masked Diffusion Language Models, Sahoo et al., 2024) simplifies D3PM by using masking as the only corruption operation: the forward process gradually replaces tokens with a [MASK] token, and the reverse process predicts the original tokens. This connects text diffusion to masked language modelling (BERT, file 04), with the diffusion timestep controlling what fraction of tokens are masked. At \\(t = 0\\) the text is fully clean; at \\(t = T\\) it is fully masked.</p> </li> <li> <p>Continuous text diffusion sidesteps the discrete problem by working in the continuous embedding space. Tokens are first mapped to their embedding vectors (chapter 06), noise is added in this continuous space, and a denoising model (typically a Transformer) learns to reverse the process. At generation time, the model produces continuous vectors that are mapped back to discrete tokens by finding the nearest embedding. The challenge is that small errors in continuous space can map to completely wrong tokens, so careful rounding and clamping are needed.</p> </li> </ul> <p></p> <ul> <li> <p>The appeal of text diffusion is that it generates all tokens simultaneously through iterative refinement, rather than left-to-right. This allows global coherence and easy infilling (generating missing text in the middle of a passage), but current text diffusion models still lag behind autoregressive models in generation quality for long-form text.</p> </li> <li> <p>Text OCR (Optical Character Recognition) is the task of extracting text from images. While not traditionally grouped with language generation, modern OCR systems are deeply integrated with NLP and increasingly use language model components.</p> </li> <li> <p>Scene text detection locates text regions in natural images (street signs, product labels, licence plates). This is challenging because text in the wild appears at arbitrary angles, scales, fonts, and against cluttered backgrounds. Detection methods typically use CNN or Transformer backbones to produce bounding boxes or segmentation masks around text regions.</p> </li> <li> <p>CRNN (Convolutional Recurrent Neural Network, Shi et al., 2017) is a classic text recognition architecture. A CNN extracts visual features from the text image, the feature map is sliced into a sequence of columns (one per horizontal position), and a bidirectional LSTM reads this sequence to model context. The output is decoded using CTC (Connectionist Temporal Classification), which handles the alignment between input columns and output characters without requiring explicit segmentation.</p> </li> <li> <p>The fundamental problem CTC solves: the model produces \\(T\\) output distributions (one per input column), but the target text has \\(L \\leq T\\) characters. </p> </li> <li> <p>We do not know which columns correspond to which characters. CTC introduces a blank token \\(\\epsilon\\) and defines a many-to-one mapping \\(\\mathcal{B}\\) that collapses repeated characters and removes blanks: \\(\\mathcal{B}(\\text{\"HH-ee-ll-ll-oo\"}) = \\text{\"Hello\"}\\) (where \"-\" is blank). </p> </li> <li> <p>The probability of the target sequence \\(y\\) is the sum over all input alignments that collapse to \\(y\\):</p> </li> </ul> \\[P(y \\mid x) = \\sum_{\\pi \\in \\mathcal{B}^{-1}(y)} \\prod_{t=1}^{T} P(\\pi_t \\mid x)\\] <ul> <li> <p>where \\(\\pi\\) is an alignment path of length \\(T\\) (one label per column, including blanks). Naively summing over all paths is exponential, but the forward algorithm (chapter 05 HMMs) computes this sum efficiently in \\(O(T \\cdot L)\\) time using dynamic programming. </p> </li> <li> <p>The blank token is essential: without it, repeated characters like \"ll\" in \"Hello\" would be indistinguishable from a single \"l\". Training maximises \\(\\log P(y \\mid x)\\), and at inference time, the best path is found by beam search or greedy decoding over the CTC output.</p> </li> <li> <p>Document OCR processes structured documents (invoices, forms, scientific papers) and must understand layout in addition to recognising characters. Modern systems like LayoutLM combine text recognition with spatial position features: each token gets both its text embedding and a positional embedding encoding its \\((x, y)\\) coordinates on the page. This allows the model to understand that a number appearing below \"Total:\" is the total amount.</p> </li> </ul> <p></p> <ul> <li> <p>Vision-language OCR models like TrOCR treat text recognition as image-to-text generation: a Vision Transformer encoder processes the image, and a language model decoder generates the text character by character. This leverages the power of pre-trained vision and language models and handles diverse scripts, fonts, and layouts without handcrafted feature engineering.</p> </li> <li> <p>Controllable generation is the challenge of steering a language model to produce outputs with desired properties: a particular style, topic, sentiment, safety level, or factual accuracy. The model should follow instructions while remaining fluent and coherent.</p> </li> <li> <p>Classifier-free guidance (CFG) for text adapts a technique from image generation. During training, the conditioning signal (e.g., a prompt) is randomly dropped some fraction of the time, training both a conditional and unconditional model in one. At inference, the output logits are interpolated:</p> </li> </ul> \\[\\text{logits}_{\\text{guided}} = (1 + w) \\cdot \\text{logits}_{\\text{conditional}} - w \\cdot \\text{logits}_{\\text{unconditional}}\\] <ul> <li> <p>where \\(w &gt; 0\\) amplifies the influence of the condition. Higher \\(w\\) makes the output more strongly follow the prompt but reduces diversity.</p> </li> <li> <p>RLHF (Reinforcement Learning from Human Feedback, Ouyang et al., 2022) is the dominant method for aligning language models with human preferences. The process has three stages:</p> </li> <li> <p>First, supervised fine-tuning (SFT): fine-tune the base language model on a dataset of high-quality human-written responses to prompts.</p> </li> <li> <p>Second, reward model training: collect human comparisons (given prompt \\(x\\) and two responses \\(y_1, y_2\\), which is better?) and train a reward model \\(r_\\phi(x, y)\\) to predict human preferences. The reward model is trained with a pairwise ranking loss:</p> </li> </ul> \\[\\mathcal{L}_{\\text{RM}} = -\\log \\sigma(r_\\phi(x, y_w) - r_\\phi(x, y_l))\\] <ul> <li> <p>where \\(y_w\\) is the preferred response and \\(y_l\\) is the dispreferred one.</p> </li> <li> <p>Third, RL fine-tuning: optimise the language model to maximise the reward while staying close to the SFT model (to prevent mode collapse). This uses PPO (Proximal Policy Optimisation, from chapter 06) with a KL penalty:</p> </li> </ul> \\[\\mathcal{L}_{\\text{RL}} = -\\mathbb{E}\\left[r_\\phi(x, y) - \\beta \\, D_{\\text{KL}}(\\pi_\\theta \\| \\pi_{\\text{SFT}})\\right]\\] <ul> <li>The KL term prevents the model from drifting too far from the base model and exploiting quirks of the reward model (\"reward hacking\").</li> </ul> <p></p> <ul> <li>DPO (Direct Preference Optimisation, Rafailov et al., 2023) simplifies RLHF by eliminating the reward model entirely. The key mathematical insight is that the KL-constrained RL objective above has a closed-form optimal policy:</li> </ul> \\[\\pi^\\ast(y \\mid x) = \\frac{1}{Z(x)} \\pi_{\\text{ref}}(y \\mid x) \\exp\\!\\left(\\frac{r(x, y)}{\\beta}\\right)\\] <ul> <li>where \\(Z(x)\\) is a normalising partition function. Rearranging this for the reward gives \\(r(x, y) = \\beta \\log \\frac{\\pi^\\ast(y \\mid x)}{\\pi_{\\text{ref}}(y \\mid x)} + \\beta \\log Z(x)\\). Substituting this implicit reward into the Bradley-Terry preference model \\(P(y_w \\succ y_l) = \\sigma(r(x, y_w) - r(x, y_l))\\) causes the intractable \\(Z(x)\\) terms to cancel, yielding the DPO loss directly:</li> </ul> \\[\\mathcal{L}_{\\text{DPO}} = -\\log \\sigma\\!\\left(\\beta \\log \\frac{\\pi_\\theta(y_w \\mid x)}{\\pi_{\\text{ref}}(y_w \\mid x)} - \\beta \\log \\frac{\\pi_\\theta(y_l \\mid x)}{\\pi_{\\text{ref}}(y_l \\mid x)}\\right)\\] <ul> <li> <p>This is mathematically equivalent to RLHF but collapses the reward model and RL training into a single supervised step. </p> </li> <li> <p>The expression inside the sigmoid can be read as: \"increase the relative probability of the preferred response and decrease the relative probability of the dispreferred response, measured against the reference model.\" </p> </li> <li> <p>The \\(\\beta\\) parameter controls how much the policy can deviate from the reference. In practice, DPO is simpler to implement (just compute log-probabilities under the current and reference models for both completions) and avoids the instabilities of PPO training.</p> </li> <li> <p>Constitutional AI (Bai et al., 2022) automates parts of the alignment process. Instead of collecting human comparisons, it uses the language model itself to critique and revise its own outputs according to a set of principles (the \"constitution\"), such as \"choose the response that is less harmful.\" The AI-generated comparisons are then used for preference training (RLAIF: RL from AI Feedback).</p> </li> <li> <p>Long-context methods address the \\(O(n^2)\\) memory and compute cost of standard self-attention, which limits sequence length. As \\(n\\) grows into the tens or hundreds of thousands of tokens, standard attention becomes infeasible.</p> </li> <li> <p>Sparse attention replaces the dense \\(n \\times n\\) attention matrix with a sparse pattern where each token attends to only a subset of other tokens. Common patterns include local attention (each token attends to a fixed-size window of neighbours), strided attention (attend to every \\(k\\)-th token), and random attention (attend to a random subset). Combinations of these patterns (used in BigBird, Longformer) achieve \\(O(n)\\) or \\(O(n \\sqrt{n})\\) complexity while maintaining the ability to capture both local and global dependencies.</p> </li> </ul> <p></p> <ul> <li> <p>Sliding window attention restricts each token to attend only to the previous \\(w\\) tokens (its local window). This is \\(O(nw)\\) rather than \\(O(n^2)\\), but long-range information must propagate through overlapping windows across layers. With \\(L\\) layers and window size \\(w\\), the effective receptive field is \\(L \\times w\\) tokens.</p> </li> <li> <p>Ring attention distributes long sequences across multiple devices by arranging them in a ring topology. Each device holds a chunk of the sequence and computes attention for its chunk while simultaneously sending key-value blocks to the next device in the ring. This overlaps computation with communication and allows sequences of arbitrary length limited only by the total memory across all devices, not the memory of any single one.</p> </li> <li> <p>Memory-augmented models extend context by equipping the Transformer with an external memory bank. At each layer, the model can read from and write to this memory using attention. Memorizing Transformers cache key-value pairs from previous chunks and attend to them in subsequent chunks, effectively extending context beyond the training window. The retrieval is approximate (using \\(k\\)-nearest neighbours over cached keys) to keep it efficient.</p> </li> <li> <p>The methods above are architectural solutions to long context. Equally important is how models are trained to use long contexts effectively.</p> </li> <li> <p>Progressive context extension is the standard approach. Training on very long sequences from the start is prohibitively expensive (\\(O(n^2)\\) attention cost), so models are pre-trained at a short context length (typically 4K\u20138K tokens) and then continued pre-training extends to the target length in stages. </p> </li> <li> <p>Llama 3.1 extends from 8K to 128K over 800B tokens with gradually increasing sequence length. DeepSeek-V3 trains at 4K, then extends to 32K, then 128K. </p> </li> <li> <p>Each stage uses a modest number of tokens (relative to the full pre-training budget) because the model only needs to learn how to use longer positions, not relearn language itself.</p> </li> <li> <p>The position encoding must be adjusted during extension. RoPE interpolation scales down the position indices so that the model sees the same rotation angles it was trained on, just spread over a longer sequence. If the model was trained at length \\(L\\) and you want to extend to \\(L' = 4L\\), you divide all position indices by 4. </p> </li> <li> <p>This means the model never sees a rotation angle it has not encountered, but the effective resolution between adjacent positions drops. </p> </li> <li> <p>RoPE extrapolation keeps the original position indices unchanged and simply applies RoPE to positions beyond \\(L\\), relying on the model generalising to unseen angles. </p> </li> <li> <p>Interpolation is much more stable; extrapolation degrades rapidly without base frequency adjustment (ABF).</p> </li> <li> <p>YaRN (Yet another RoPE extensioN) improves on naive interpolation by recognising that not all RoPE dimensions should be treated equally. </p> </li> <li> <p>High-frequency dimensions (small \\(i\\) in \\(\\theta_i = \\theta_{\\text{base}}^{-2i/d}\\)) rotate many times within the training length and can extrapolate well. </p> </li> <li> <p>Low-frequency dimensions (large \\(i\\)) rotate slowly and are more sensitive to length extension. </p> </li> <li> <p>YaRN interpolates only the low-frequency dimensions, extrapolates the high-frequency ones, and applies a temperature scaling \\(t\\) to the attention logits to compensate for the distributional shift:</p> </li> </ul> \\[\\text{score}'_{ij} = \\frac{q_i^T k_j}{t \\sqrt{d_k}}\\] <ul> <li> <p>where \\(t &gt; 1\\) flattens the attention distribution, preventing the model from attending too sharply to nearby tokens when position signals are compressed.</p> </li> <li> <p>Long-context data curation is a critical and often underestimated challenge. Most pre-training corpora consist of short documents (news articles, web pages, social media posts). </p> </li> <li> <p>Long-context training requires a data mix that actually exercises the full context window: books, code repositories, long-form scientific articles, multi-turn conversation logs, and concatenated thematically related documents. </p> </li> <li> <p>If the model is only trained on short documents padded or packed to fill the context window, it learns to ignore distant tokens because they are never relevant.</p> </li> <li> <p>Sequence packing is a training efficiency technique: multiple documents are concatenated into a single training sequence to avoid padding waste, with attention masks preventing cross-document attention. </p> </li> <li> <p>For long-context training, the packing strategy matters: packing many unrelated short documents teaches the model that distant tokens are noise, while packing fewer, genuinely long documents teaches it to use the full context.</p> </li> <li> <p>A known failure mode is the \"lost in the middle\" phenomenon (Liu et al., 2023): language models tend to use information at the beginning and end of the context window effectively but struggle with information placed in the middle. </p> </li> <li> <p>This resembles the serial position effect in human memory (primacy and recency). </p> </li> <li> <p>It arises partly from training data distributions (important information is often at the start or end of documents) and partly from attention patterns that concentrate on nearby and initial tokens. </p> </li> <li> <p>Long-context training with diverse placement of key information mitigates but does not fully solve this.</p> </li> <li> <p>Needle-in-a-haystack evaluation tests whether a model can retrieve a specific fact (\"the needle\") placed at various positions within a long distractor context (\"the haystack\"). </p> </li> <li> <p>A model with genuine long-context ability should achieve near-perfect retrieval regardless of where the needle is placed. </p> </li> <li> <p>This test reveals the lost-in-the-middle effect clearly and is used to benchmark context extension methods.</p> </li> <li> <p>Long-context fine-tuning after pre-training uses targeted SFT data: long multi-turn dialogues, document QA with evidence scattered across thousands of tokens, long-form summarisation, and repository-level code understanding. </p> </li> <li> <p>Qwen3 uses Dual Chunk Attention (DCA) during this stage, which processes long sequences as pairs of chunks where intra-chunk attention is full and inter-chunk attention is efficient, achieving 4x the effective sequence capacity during fine-tuning.</p> </li> <li> <p>State Space Models (SSMs) offer a fundamentally different approach to long-sequence modelling. Rather than modifying attention, they replace it entirely with a linear dynamical system inspired by continuous-time control theory. </p> </li> <li> <p>An SSM maps an input sequence \\(u(t)\\) to an output \\(y(t)\\) through a latent state \\(x(t) \\in \\mathbb{R}^N\\) governed by:</p> </li> </ul> \\[x'(t) = Ax(t) + Bu(t), \\quad y(t) = Cx(t) + Du(t)\\] <ul> <li> <p>where \\(A \\in \\mathbb{R}^{N \\times N}\\) is the state transition matrix, \\(B \\in \\mathbb{R}^{N \\times 1}\\) is the input projection, \\(C \\in \\mathbb{R}^{1 \\times N}\\) is the output projection, and \\(D\\) is a skip connection. </p> </li> <li> <p>To apply this to discrete sequences (tokens), the continuous system is discretised using a step size \\(\\Delta\\). The zero-order hold discretisation gives:</p> </li> </ul> \\[\\bar{A} = \\exp(\\Delta A), \\quad \\bar{B} = (\\Delta A)^{-1}(\\exp(\\Delta A) - I) \\cdot \\Delta B\\] <ul> <li> <p>The discrete recurrence then becomes \\(x_k = \\bar{A} x_{k-1} + \\bar{B} u_k\\), \\(y_k = C x_k + D u_k\\), which looks like an RNN: process one token at a time with a hidden state. </p> </li> <li> <p>Unlike RNNs, this recurrence can also be unrolled as a global convolution: because the system is linear, the output is \\(y = \\bar{K} \\ast u\\) where the kernel \\(\\bar{K} = (C\\bar{B}, \\, C\\bar{A}\\bar{B}, \\, C\\bar{A}^2\\bar{B}, \\ldots)\\) depends only on the fixed parameters. </p> </li> <li> <p>This dual view \u2014 recurrence for efficient autoregressive inference (\\(O(1)\\) per step) and convolution for efficient parallel training (\\(O(n \\log n)\\) via FFT) \u2014 is the central insight of SSMs.</p> </li> </ul> <p></p> <ul> <li> <p>S4 (Structured State Spaces for Sequence Modeling, Gu et al., 2022) made SSMs practical by solving the key numerical challenge: the state matrix \\(A\\) must capture long-range dependencies, but naively parameterising it leads to vanishing or exploding dynamics (the same problem as vanilla RNNs). </p> </li> <li> <p>S4 initialises \\(A\\) using the HiPPO (High-order Polynomial Projection Operators) matrix, which is derived from the theory of optimal polynomial approximation of continuous signals. The HiPPO matrix has a specific structure that provably enables the state to maintain a compressed representation of the entire input history with graceful decay:</p> </li> </ul> \\[ A_{nk} = -\\begin{cases} (2n+1)^{1/2}(2k+1)^{1/2} &amp; \\text{if } n &gt; k \\\\ n+1 &amp; \\text{if } n = k \\\\ 0 &amp; \\text{if } n &lt; k \\end{cases} \\] <ul> <li> <p>This lower-triangular structure ensures that the state acts as an online approximation of the input signal using Legendre polynomials. Computing \\(\\bar{A}^k\\) for long kernels is expensive, so S4 uses the fact that the HiPPO matrix can be decomposed as a sum of low-rank and diagonal terms, enabling \\(O(n \\log n)\\) kernel computation.</p> </li> <li> <p>Mamba (Gu and Dao, 2023) introduces the critical innovation of selective state spaces: making the SSM parameters input-dependent. In S4, the matrices \\(A\\), \\(B\\), \\(C\\), and the step size \\(\\Delta\\) are fixed \u2014 the same dynamics apply to every token regardless of content. Mamba makes \\(B\\), \\(C\\), and \\(\\Delta\\) functions of the input:</p> </li> </ul> \\[B_k = \\text{Linear}(u_k), \\quad C_k = \\text{Linear}(u_k), \\quad \\Delta_k = \\text{softplus}(\\text{Linear}(u_k))\\] <ul> <li> <p>This selectivity allows the model to decide, at each position, what information to store in the state and what to ignore \u2014 analogous to how attention selects relevant tokens, but without the quadratic cost. The step size \\(\\Delta_k\\) controls the \"gate\": a large \\(\\Delta\\) causes the state to integrate the current input strongly (the continuous dynamics advance a large step, effectively resetting the state), while a small \\(\\Delta\\) preserves the existing state and ignores the current input.</p> </li> <li> <p>The trade-off is that input-dependent parameters break the convolution view (the kernel is no longer fixed), so Mamba cannot use FFT-based training. Instead, it uses a hardware-aware parallel scan algorithm that exploits the associativity of the recurrence: the state update \\((x_k, u_k) \\mapsto x_{k+1}\\) can be expressed as a sequence of associative operations and parallelised using a prefix sum (scan), analogous to parallel prefix addition in hardware design. This runs in \\(O(n)\\) time with \\(O(\\log n)\\) depth on a GPU, nearly matching the efficiency of convolution.</p> </li> <li> <p>Mamba achieves inference that is truly \\(O(1)\\) per token (just update the fixed-size state, no KV cache that grows with context), making it fundamentally more memory-efficient than Transformers at long sequence lengths. The state size \\(N\\) (typically 16) is much smaller than a Transformer's KV cache, which stores \\(O(n \\cdot d)\\) values. In practice, Mamba matches or exceeds Transformer quality at the same parameter count on language modelling benchmarks, with significantly faster inference on long sequences.</p> </li> <li> <p>Hybrid architectures combine SSM layers with attention layers, using SSMs for the majority of layers (efficient long-range propagation) and sprinkling in a few attention layers (precise content-based retrieval). Models like Jamba and Zamba interleave Mamba and Transformer blocks, achieving better quality than pure SSMs while maintaining much of the inference efficiency advantage. This suggests that attention and SSMs capture complementary capabilities: SSMs excel at smooth, long-range state propagation while attention excels at precise, content-dependent lookups.</p> </li> <li> <p>Retrieval-Augmented Generation (RAG) addresses the knowledge limitations of language models by giving them access to an external knowledge base at inference time. Instead of relying solely on knowledge encoded in model parameters during training, RAG retrieves relevant documents and conditions generation on them.</p> </li> <li> <p>The classic retriever-reader architecture has two components. The retriever takes a query and fetches the top-\\(k\\) most relevant passages from a corpus. The reader (a language model) generates the answer conditioned on both the query and the retrieved passages. The retriever can use sparse methods (BM25, which extends TF-IDF from file 02) or dense methods.</p> </li> <li> <p>Dense passage retrieval (DPR) uses a dual-encoder architecture: one encoder maps questions to vectors, another maps passages to vectors. Both are typically BERT-based. At indexing time, all passages are encoded and stored. At query time, the question is encoded and the nearest passages are found using approximate nearest neighbour search (such as FAISS). The similarity metric is the dot product between question and passage vectors.</p> </li> <li> <p>Chunking strategies affect retrieval quality significantly. Documents must be split into passages small enough for the retriever to handle, but large enough to contain complete ideas. Fixed-size chunking (e.g., 256 tokens with 50-token overlap) is simple but may split sentences awkwardly. Semantic chunking splits at paragraph or section boundaries. Hierarchical chunking creates a tree of summaries at different granularities.</p> </li> </ul> <p></p> <ul> <li> <p>RAG provides several advantages: the knowledge base can be updated without retraining the model, the model can cite sources, and hallucination is reduced because the model can ground its answers in retrieved text. The main challenges are retrieval quality (if the wrong passages are retrieved, the model may produce wrong answers confidently) and latency (retrieval adds a step to inference).</p> </li> <li> <p>Speculative decoding accelerates autoregressive generation by using a small, fast draft model to propose multiple tokens in parallel, which are then verified by the large target model in a single forward pass.</p> </li> <li> <p>The algorithm works as follows: the draft model generates \\(k\\) candidate tokens autoregressively (this is fast because the draft model is small). </p> </li> <li> <p>The target model then scores all \\(k\\) tokens simultaneously in a single forward pass (this is efficient because the work is batched). </p> </li> <li> <p>For each candidate token \\(t\\) sampled from the draft distribution \\(p_d(t)\\), it is accepted with probability \\(\\min(1, \\, p_{\\text{target}}(t) / p_d(t))\\). If rejected, a corrected token is resampled from the adjusted distribution \\(p_{\\text{adj}}(t) = \\max(0, \\, p_{\\text{target}}(t) - p_d(t))\\), normalised.</p> </li> <li> <p>This acceptance-rejection scheme guarantees that the output distribution is identical to the target model alone. </p> </li> <li> <p>To see why, consider the effective probability of emitting token \\(t\\). It can be accepted directly (probability \\(p_d(t) \\cdot \\min(1, p_{\\text{target}}(t)/p_d(t))\\)) or produced through resampling. </p> </li> <li> <p>For tokens where \\(p_{\\text{target}}(t) \\leq p_d(t)\\), the direct acceptance contributes \\(p_{\\text{target}}(t)\\). For tokens where \\(p_{\\text{target}}(t) &gt; p_d(t)\\), direct acceptance contributes \\(p_d(t)\\) and resampling contributes the remainder \\(p_{\\text{target}}(t) - p_d(t)\\) (after accounting for the rejection probability). </p> </li> <li> <p>In both cases, the total probability of emitting \\(t\\) equals \\(p_{\\text{target}}(t)\\). The draft model affects only speed, not quality.</p> </li> </ul> <p></p> <ul> <li> <p>The speedup depends on the acceptance rate: if the draft model is well-aligned with the target model, most tokens are accepted and the wall-clock time is roughly that of the draft model. Typical speedups are 2-3x with no quality degradation.</p> </li> <li> <p>Medusa (Cai et al., 2024) takes a different approach: instead of a separate draft model, it adds multiple lightweight prediction heads to the target model itself. Each head predicts a different future token position simultaneously (\\(k = 1, 2, 3, \\ldots\\) steps ahead). At each step, Medusa proposes several candidate continuations using a tree structure, and a single forward pass through the target model's attention layers verifies which candidates are consistent. This avoids the need for a separate draft model entirely.</p> </li> <li> <p>Parallel generation methods more broadly aim to break the sequential bottleneck of autoregressive decoding. Jacobi decoding initialises all positions with guesses and iteratively refines them in parallel until convergence, treating generation as a fixed-point iteration. Non-autoregressive models (NAT) generate all tokens simultaneously in a single forward pass but typically suffer quality degradation and require techniques like iterative refinement, CTC loss, or knowledge distillation from autoregressive teachers to close the gap.</p> </li> <li> <p>The techniques above \u2014 alignment, long context, retrieval, efficient decoding, state space models \u2014 come together in modern production LLMs. </p> </li> <li> <p>The remainder of this file surveys the architectural innovations in frontier models, showing how theoretical ideas from files 01\u201304 and the methods above are combined in practice.</p> </li> <li> <p>Grouped Query Attention (GQA) is the most widely adopted attention efficiency technique. Standard multi-head attention (MHA) maintains separate key and value projections per head, requiring \\(n_{\\text{heads}} \\times d_{\\text{head}}\\) values cached per token. GQA groups multiple query heads to share a single key-value head. </p> </li> <li> <p>With 64 query heads and 8 KV heads (a common configuration in Llama 3, Qwen, Gemma), each KV head is shared by 8 query heads, reducing the KV cache by 8x compared to MHA. </p> </li> <li> <p>The output quality is nearly identical to MHA because the queries can still attend to different patterns, they just share the same key-value subspace. Multi-query attention (MQA) is the extreme case with a single KV head for all queries, but GQA provides a better quality-efficiency trade-off.</p> </li> <li> <p>Multi-head Latent Attention (MLA), introduced in DeepSeek-V2, achieves even more aggressive KV cache compression. Instead of caching the full key-value projections (even with GQA), MLA down-projects the hidden state into a low-rank latent vector \\(c_t \\in \\mathbb{R}^{d_c}\\) with \\(d_c \\ll n_{\\text{heads}} \\times d_{\\text{head}}\\):</p> </li> </ul> \\[c_t = W_{\\text{down}} \\, h_t\\] <ul> <li> <p>Only this compressed vector is cached. At attention time, the full key and value representations are reconstructed via up-projection: \\(k_t = W_{\\text{up}}^K c_t\\), \\(v_t = W_{\\text{up}}^V c_t\\). In DeepSeek-V3 (671B total parameters, 37B active), the compression dimension is \\(d_c = 512\\) versus \\(128 \\times 128 = 16{,}384\\) for full MHA, a 93% reduction in KV cache. </p> </li> <li> <p>A subtlety: standard RoPE is position-dependent and incompatible with the shared compression, so MLA uses decoupled RoPE: a small separate stream of the query and key (64 dimensions per head) carries position information via RoPE, while the bulk of the representation flows through the compressed latent path.</p> </li> </ul> <p></p> <ul> <li> <p>Position encoding at scale has diverged significantly from the original sinusoidal scheme. All frontier models use RoPE (file 04), but with key modifications for long context. The base frequency \\(\\theta_{\\text{base}}\\) in the original RoPE formula \\(\\theta_i = \\theta_{\\text{base}}^{-2i/d}\\) is typically 10,000, which limits extrapolation beyond the training length. </p> </li> <li> <p>Adjusted Base Frequency (ABF) simply increases \\(\\theta_{\\text{base}}\\) to 500,000 (Llama 3) or 1,000,000 (Qwen3, Gemma 3), stretching the rotation periods so the model encounters fewer full rotations during training and can extrapolate further. </p> </li> <li> <p>YaRN (Yet another RoPE extensioN) applies frequency-dependent interpolation: low-frequency dimensions are interpolated (scaled down), high-frequency dimensions are extrapolated, and a temperature factor adjusts the attention distribution. DeepSeek-V3, Qwen, and Kimi K2 all use YaRN-based extension to reach 128K context from models pre-trained at 4K\u20138K.</p> </li> <li> <p>iRoPE (interleaved RoPE), introduced in Llama 4, takes a more radical approach: every 4th attention layer uses no positional encoding at all (NoPE), while the other layers use standard RoPE with chunked attention. </p> </li> <li> <p>The NoPE layers can attend to all positions without any positional bias, while the RoPE layers provide local ordering. Combined with temperature scaling at inference, this enables Llama 4 Scout's 10M-token context window \u2014 orders of magnitude beyond any pure RoPE approach.</p> </li> <li> <p>Mixture of Experts at scale has become the dominant architecture for frontier models (file 04 introduced MoE fundamentals). The key design choices are the number of experts, routing sparsity, and load balancing.</p> </li> <li> <p>Routing sparsity varies significantly: DeepSeek-V3 uses 256 experts with top-8 routing (32x sparsity), Qwen3 uses 128 experts with top-8 (16x sparsity), Mixtral uses 8 experts with top-2 (4x sparsity), and Llama 4 Maverick uses 128 experts with top-1 plus a shared expert (128x sparsity). </p> </li> <li> <p>Higher sparsity means more total parameters for the same active compute, but requires more careful load balancing and communication infrastructure.</p> </li> <li> <p>Auxiliary-loss-free load balancing (DeepSeek-V3) replaces the traditional load balancing loss (file 04) which was found to degrade model quality. Instead, each expert maintains a dynamic bias term adjusted per training step: overloaded experts have their bias decreased (receiving fewer tokens), underloaded experts have their bias increased. This achieves balanced routing without any auxiliary loss polluting the main training signal.</p> </li> <li> <p>Shared experts appear in most MoE designs: one or more expert FFNs that process every token regardless of routing. These handle common patterns that all tokens need (basic syntax, function words), freeing the routed experts to specialise. Llama 4 uses 1 shared expert plus 1 routed expert per token (very sparse); DeepSeek-V3 uses 1 shared plus 8 routed.</p> </li> <li> <p>Alternating dense and MoE layers provide another design axis. Gemma 2 and 3 alternate local/global attention layers (5:1 ratio in Gemma 3, where local layers use a 1,024-token sliding window and only global layers cache the full 128K context). </p> </li> <li> <p>Llama 4 Maverick interleaves dense FFN layers with MoE layers. Kimi K2 uses hybrid-sparsity layers (one dense layer interspersed among expert layers). This heterogeneous design allows different layers to serve different functions.</p> </li> <li> <p>Multi-token prediction (MTP), used in DeepSeek-V3, trains the model to predict not just the next token but also the token after that. At each position, a secondary prediction module (sharing the main model's embeddings) predicts one additional future token. The MTP loss is weighted at 0.1\u20130.3 relative to the main next-token loss. Beyond improving representation quality during training, the MTP heads can serve as draft heads for speculative decoding at inference time, providing a free speedup.</p> </li> <li> <p>Knowledge distillation is a training strategy where a large \"teacher\" model's outputs guide the training of a smaller \"student\" model. Gemma 2 and 3 use distillation extensively: the smaller models (2B, 4B) are trained on 50x the compute-optimal amount of data with the teacher's probability distributions as soft targets. This is why Gemma 3-4B matches Gemma 2-27B in quality. </p> </li> <li> <p>The distillation loss replaces or supplements the standard cross-entropy: the student minimises the KL divergence between its output distribution and the teacher's:</p> </li> </ul> \\[\\mathcal{L}_{\\text{distill}} = D_{\\text{KL}}(p_{\\text{teacher}}(\\cdot \\mid x) \\| p_{\\text{student}}(\\cdot \\mid x))\\] <ul> <li> <p>DeepSeek-R1 distilled its 671B reasoning model into dense models as small as 1.5B using 800K curated chain-of-thought samples, producing small models with disproportionately strong reasoning.</p> </li> <li> <p>Reasoning via reinforcement learning represents the most significant recent advance in LLM capabilities. DeepSeek-R1 demonstrated that pure reinforcement learning on a base model (without supervised fine-tuning) can elicit chain-of-thought reasoning, self-verification, and error correction, behaviours that emerge spontaneously when the model is rewarded for correct final answers.</p> </li> <li> <p>DeepSeek-R1 uses GRPO (Group Relative Policy Optimisation), which eliminates the value network required by PPO. For each prompt, GRPO samples a group of \\(G\\) outputs, computes their rewards, and normalises advantages within the group:</p> </li> </ul> \\[A_i = \\frac{r_i - \\text{mean}(r_1, \\ldots, r_G)}{\\text{std}(r_1, \\ldots, r_G)}\\] <ul> <li> <p>The policy gradient then uses these group-relative advantages with a clipped objective (similar to PPO's clipping). </p> </li> <li> <p>Eliminating the critic network halves the memory and compute requirements of RL training, making it practical to train 671B-parameter models with RL. </p> </li> <li> <p>A critical design choice: DeepSeek-R1 uses rule-based rewards (checking mathematical answers against ground truth, running code test cases) rather than neural reward models, because neural reward models were found to be susceptible to reward hacking at this scale.</p> </li> <li> <p>Qwen3's hybrid thinking mode integrates reasoning (with <code>&lt;think&gt;</code> tags for step-by-step chain-of-thought) and fast direct response into a single model, allowing users to control a \"thinking budget\" that trades latency for reasoning depth. </p> </li> <li> <p>This is achieved by training on both thinking and non-thinking data, not through separate model checkpoints.</p> </li> <li> <p>Training stabilisation at scale requires new techniques beyond standard practices. Logit soft-capping (Gemma 2) passes attention scores through \\(s \\cdot \\tanh(\\text{logits} / s)\\) with a soft cap \\(s\\) (typically 30\u201350) to prevent unbounded growth. </p> </li> <li> <p>QK-Norm (Qwen3) applies RMSNorm to query and key vectors before computing attention scores, replacing the need for QKV bias. QK-Clip (Kimi K2's MuonClip optimiser) monitors the maximum attention logit during training and rescales query-key weight matrices when they exceed a threshold, enabling stable pre-training of 1T-parameter models with zero instability events.</p> </li> <li> <p>FP8 mixed-precision training (DeepSeek-V3) uses 8-bit floating point for the compute-intensive matrix multiplications in the forward and backward passes while keeping master weights in higher precision. </p> </li> <li> <p>This roughly doubles throughput compared to BF16/FP16 training with negligible quality loss. DeepSeek-V3 trained its 671B-parameter model for only 2.8M H800 GPU-hours \u2014 a fraction of comparable models \u2014 largely due to this and other engineering optimisations.</p> </li> </ul>"},{"location":"chapter%2007%3A%20computational%20linguistics/05.%20advanced%20text%20generation/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement a simple retrieval-augmented generation pipeline from scratch. Index a set of documents using TF-IDF (file 02), retrieve the most relevant passage for a query, and prepend it to a prompt. <pre><code>import jax.numpy as jnp\nimport math\nfrom collections import Counter\n\n# Knowledge base: a set of short passages\nknowledge_base = [\n    \"The Eiffel Tower is a wrought-iron lattice tower in Paris, France. It was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair.\",\n    \"The Great Wall of China is a series of fortifications built along the northern borders of China. Construction began in the 7th century BC.\",\n    \"Photosynthesis is the process by which plants convert sunlight, water, and carbon dioxide into glucose and oxygen using chlorophyll.\",\n    \"The theory of general relativity, published by Albert Einstein in 1915, describes gravity as the curvature of spacetime caused by mass and energy.\",\n    \"Python is a high-level programming language known for its simple syntax and readability. It was created by Guido van Rossum and released in 1991.\",\n    \"The mitochondria are organelles found in eukaryotic cells. They generate most of the cell's supply of ATP, used as a source of chemical energy.\",\n]\n\n# Build TF-IDF index (reusing concepts from file 02)\ndef tokenise(text):\n    return text.lower().split()\n\nvocab = sorted(set(w for doc in knowledge_base for w in tokenise(doc)))\nword2idx = {w: i for i, w in enumerate(vocab)}\nV = len(vocab)\nN = len(knowledge_base)\n\n# Document frequencies\ndoc_freq = Counter()\nfor doc in knowledge_base:\n    for w in set(tokenise(doc)):\n        doc_freq[w] += 1\n\ndef tfidf_vector(text):\n    words = tokenise(text)\n    counts = Counter(words)\n    vec = jnp.zeros(V)\n    for w, c in counts.items():\n        if w in word2idx:\n            tf = 1 + math.log(c)\n            idf = math.log(N / (doc_freq.get(w, 0) + 1))\n            vec = vec.at[word2idx[w]].set(tf * idf)\n    return vec\n\n# Index all documents\ndoc_vectors = jnp.stack([tfidf_vector(doc) for doc in knowledge_base])\n\ndef cosine_sim(a, b):\n    return jnp.dot(a, b) / (jnp.linalg.norm(a) * jnp.linalg.norm(b) + 1e-8)\n\ndef retrieve(query, top_k=2):\n    \"\"\"Retrieve top-k most relevant passages for a query.\"\"\"\n    q_vec = tfidf_vector(query)\n    sims = jnp.array([cosine_sim(q_vec, doc_vectors[i]) for i in range(N)])\n    top_indices = jnp.argsort(-sims)[:top_k]\n    return [(int(i), float(sims[i]), knowledge_base[int(i)]) for i in top_indices]\n\n# Test retrieval\nqueries = [\n    \"Who built the Eiffel Tower?\",\n    \"How do plants make food?\",\n    \"What did Einstein discover?\",\n]\n\nfor query in queries:\n    results = retrieve(query, top_k=1)\n    print(f\"\\nQuery: '{query}'\")\n    for idx, sim, passage in results:\n        print(f\"  Retrieved (sim={sim:.3f}): '{passage[:80]}...'\")\n\n    # RAG-style prompt construction\n    context = results[0][2]\n    rag_prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n    print(f\"  RAG prompt:\\n    {rag_prompt[:120]}...\")\n</code></pre></p> </li> <li> <p>Implement speculative decoding with a toy draft and target model. Show that the accepted output matches the target model's distribution. <pre><code>import jax\nimport jax.numpy as jnp\n\n# Simulate a draft model (fast, less accurate) and target model (slow, accurate)\nvocab_size = 8\nseq_len = 5\n\nkey = jax.random.PRNGKey(42)\n\n# Target model: returns logits given a sequence\ndef target_model(seq, key):\n    \"\"\"Simulated target model: produces token logits (expensive).\"\"\"\n    # In practice this would be a large Transformer forward pass\n    k1, k2 = jax.random.split(key)\n    logits = jax.random.normal(k1, (len(seq), vocab_size)) * 2\n    # Make it somewhat predictable: bias toward token (seq[-1] + 1) % vocab_size\n    for i in range(len(seq)):\n        logits = logits.at[i, (seq[i] + 1) % vocab_size].add(3.0)\n    return logits\n\ndef draft_model(seq, key):\n    \"\"\"Simulated draft model: similar but noisier (cheap).\"\"\"\n    k1, k2 = jax.random.split(key)\n    logits = jax.random.normal(k1, (len(seq), vocab_size))\n    for i in range(len(seq)):\n        logits = logits.at[i, (seq[i] + 1) % vocab_size].add(2.0)\n    return logits\n\ndef sample_token(logits, key):\n    return jax.random.categorical(key, logits)\n\ndef speculative_decode(prefix, draft_steps=3, key=jax.random.PRNGKey(0)):\n    \"\"\"Speculative decoding: draft proposes, target verifies.\"\"\"\n    seq = list(prefix)\n    total_accepted = 0\n    total_proposed = 0\n\n    for _ in range(4):  # generate 4 rounds\n        key, *subkeys = jax.random.split(key, draft_steps + 3)\n\n        # Draft model proposes draft_steps tokens\n        draft_tokens = []\n        draft_probs = []\n        draft_seq = list(seq)\n        for i in range(draft_steps):\n            d_logits = draft_model(jnp.array(draft_seq), subkeys[i])\n            d_probs = jax.nn.softmax(d_logits[-1])\n            tok = sample_token(d_logits[-1], subkeys[i])\n            draft_tokens.append(int(tok))\n            draft_probs.append(d_probs)\n            draft_seq.append(int(tok))\n\n        # Target model scores all draft tokens in one pass\n        target_logits = target_model(jnp.array(draft_seq), subkeys[draft_steps])\n        target_start = len(seq) - 1  # position of last prefix token\n\n        # Accept/reject each draft token\n        accepted = 0\n        for i in range(draft_steps):\n            t_probs = jax.nn.softmax(target_logits[target_start + i])\n            d_prob = draft_probs[i][draft_tokens[i]]\n            t_prob = t_probs[draft_tokens[i]]\n\n            # Accept with probability min(1, target_prob / draft_prob)\n            accept_prob = jnp.minimum(1.0, t_prob / (d_prob + 1e-10))\n            key, accept_key = jax.random.split(key)\n            if jax.random.uniform(accept_key) &lt; accept_prob:\n                seq.append(draft_tokens[i])\n                accepted += 1\n            else:\n                # Reject: sample from adjusted distribution\n                key, resample_key = jax.random.split(key)\n                adjusted = jnp.maximum(0, t_probs - draft_probs[i])\n                adjusted = adjusted / (adjusted.sum() + 1e-10)\n                new_tok = jax.random.categorical(resample_key, jnp.log(adjusted + 1e-10))\n                seq.append(int(new_tok))\n                break\n\n        total_accepted += accepted\n        total_proposed += draft_steps\n\n    return seq, total_accepted, total_proposed\n\n# Run speculative decoding\nprefix = [0, 1]\nresult_seq, accepted, proposed = speculative_decode(prefix)\nacceptance_rate = accepted / proposed if proposed &gt; 0 else 0\n\nprint(f\"Prefix: {prefix}\")\nprint(f\"Generated sequence: {result_seq}\")\nprint(f\"Draft proposals: {proposed}\")\nprint(f\"Accepted: {accepted}\")\nprint(f\"Acceptance rate: {acceptance_rate:.1%}\")\nprint(f\"Speedup potential: {(accepted + proposed) / proposed:.2f}x\")\n</code></pre></p> </li> <li> <p>Build a simple DPO training loop. Given pairs of preferred and dispreferred completions, update a small model using the DPO loss. <pre><code>import jax\nimport jax.numpy as jnp\n\n# Tiny language model: linear projection from one-hot to logits\nvocab_size = 10\nseq_len = 4\n\nkey = jax.random.PRNGKey(42)\nk1, k2 = jax.random.split(key)\n\n# Current policy parameters (trainable)\ntheta = jax.random.normal(k1, (vocab_size, vocab_size)) * 0.1\n# Reference policy parameters (frozen copy of initial theta)\ntheta_ref = theta.copy()\n\ndef log_prob_sequence(params, sequence):\n    \"\"\"Compute log P(sequence) under a simple autoregressive model.\"\"\"\n    total = 0.0\n    for t in range(1, len(sequence)):\n        # Simple: logits at position t depend on token at t-1\n        logits = params[sequence[t-1]]\n        log_probs = jax.nn.log_softmax(logits)\n        total += log_probs[sequence[t]]\n    return total\n\ndef dpo_loss(theta, theta_ref, preferred, dispreferred, beta=0.1):\n    \"\"\"Direct Preference Optimisation loss for one pair.\"\"\"\n    log_pi_w = log_prob_sequence(theta, preferred)\n    log_pi_l = log_prob_sequence(theta, dispreferred)\n    log_ref_w = log_prob_sequence(theta_ref, preferred)\n    log_ref_l = log_prob_sequence(theta_ref, dispreferred)\n\n    # DPO objective\n    return -jax.nn.log_sigmoid(\n        beta * ((log_pi_w - log_ref_w) - (log_pi_l - log_ref_l))\n    )\n\n# Preference dataset: (prompt_prefix, preferred_completion, dispreferred_completion)\npreferences = [\n    (jnp.array([1, 3, 5, 7]), jnp.array([1, 3, 5, 2])),  # prefer 7 over 2 at end\n    (jnp.array([0, 2, 4, 6]), jnp.array([0, 2, 4, 9])),  # prefer 6 over 9\n    (jnp.array([3, 3, 3, 3]), jnp.array([3, 3, 3, 0])),  # prefer repeating over 0\n    (jnp.array([5, 6, 7, 8]), jnp.array([5, 6, 7, 1])),  # prefer 8 over 1\n]\n\ngrad_fn = jax.jit(jax.grad(dpo_loss))\nlr = 0.05\n\nprint(\"Training DPO...\")\nfor epoch in range(100):\n    total_loss = 0.0\n    for preferred, dispreferred in preferences:\n        loss = dpo_loss(theta, theta_ref, preferred, dispreferred)\n        grads = grad_fn(theta, theta_ref, preferred, dispreferred)\n        theta = theta - lr * grads\n        total_loss += loss\n    if (epoch + 1) % 20 == 0:\n        avg_loss = total_loss / len(preferences)\n        print(f\"  Epoch {epoch+1}: avg DPO loss = {avg_loss:.4f}\")\n\n# Check: the model should now prefer the preferred completions\nprint(\"\\nPreference check after DPO training:\")\nfor preferred, dispreferred in preferences:\n    lp_w = log_prob_sequence(theta, preferred)\n    lp_l = log_prob_sequence(theta, dispreferred)\n    print(f\"  Preferred {list(preferred.astype(int))}: logP={lp_w:.3f}  \"\n          f\"Dispreferred {list(dispreferred.astype(int))}: logP={lp_l:.3f}  \"\n          f\"{'correct' if lp_w &gt; lp_l else 'WRONG'}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2008%3A%20computer%20vision/01.%20image%20fundamentals/","title":"Image Fundamentals","text":"<p>Image fundamentals explain how digital images are represented, formed, and pre-processed before any model sees them. This file covers pixels, colour spaces (RGB, HSV, YCbCr, LAB), the pinhole camera model, convolution, edge detection (Sobel, Canny), histograms, and feature descriptors (SIFT, ORB) -- the low-level vision toolkit.</p> <ul> <li> <p>A digital image is a 2D grid of numbers. Each cell in the grid is a pixel (picture element), and its value represents intensity or colour. A grayscale image is a single 2D matrix where each pixel holds a brightness value, typically from 0 (black) to 255 (white) for 8-bit images.</p> </li> <li> <p>A colour image extends this to three channels. In the RGB colour space, each pixel stores three values: red, green, and blue intensity. </p> </li> <li> <p>The couloured image is a 3D tensor (matrix) of shape (height, width, 3). Mixing these three channels at different intensities produces the full spectrum of visible colour.</p> </li> </ul> <p></p> <ul> <li> <p>Bit depth determines how many distinct intensity levels each channel can represent. </p> </li> <li> <p>An 8-bit image has \\(2^8 = 256\\) levels per channel, giving \\(256^3 \\approx 16.7\\) million possible colours. A 16-bit image has 65,536 levels per channel, used in medical imaging and HDR photography where fine intensity distinctions matter.</p> </li> <li> <p>RGB is convenient for displays, but other colour spaces are better suited for different tasks.</p> </li> <li> <p>HSV (Hue, Saturation, Value) separates colour information from brightness. Hue is the pure colour (0-360 degrees around a colour wheel), saturation is how vivid the colour is (0 = grey, 1 = pure colour), and value is brightness. HSV is useful for colour-based segmentation because you can threshold on hue alone, regardless of lighting conditions. Detecting \"red objects\" is much easier in HSV than in RGB.</p> </li> <li> <p>YCbCr separates luminance (Y, perceived brightness) from chrominance (Cb, Cr, colour difference signals). This is the colour space used in JPEG compression and video codecs. Human vision is more sensitive to brightness than colour, so chrominance can be stored at lower resolution (chroma subsampling) with little perceptual loss.</p> </li> <li> <p>LAB (CIELAB) is designed so that numerical distance between two colours corresponds to perceptual difference. Equal steps in LAB space look like equal steps to a human observer. The L channel is lightness, A goes from green to red, and B goes from blue to yellow. LAB is used when you need perceptually uniform colour comparisons.</p> </li> <li> <p>Image formation describes how a 3D scene becomes a 2D image. The simplest model is the pinhole camera: light from the scene passes through a tiny hole and projects onto a sensor plane behind it. A point \\((X, Y, Z)\\) in world coordinates projects to pixel coordinates \\((u, v)\\):</p> </li> </ul> \\[ \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = \\frac{1}{Z} \\begin{bmatrix} f_x &amp; 0 &amp; c_x \\\\ 0 &amp; f_y &amp; c_y \\\\ 0 &amp; 0 &amp; 1 \\end{bmatrix} \\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix} \\] <ul> <li>The 3x3 matrix is the intrinsic matrix \\(K\\). It encodes the camera's internal properties: focal lengths \\(f_x, f_y\\) (how strongly the lens converges light) and the principal point \\((c_x, c_y)\\) (where the optical axis meets the sensor, usually near the image centre). These are fixed for a given camera and lens combination.</li> </ul> <p></p> <ul> <li>The extrinsic parameters describe where the camera is in the world: a rotation matrix \\(R\\) (3x3, from chapter 02) and a translation vector \\(t\\) (3x1). Together, they transform world coordinates to camera coordinates. The full projection is:</li> </ul> \\[\\mathbf{p} = K [R \\mid t] \\mathbf{P}\\] <ul> <li> <p>where \\(\\mathbf{P} = [X, Y, Z, 1]^T\\) is the 3D point in homogeneous coordinates and \\(\\mathbf{p} = [u, v, 1]^T\\) is the projected pixel. The \\([R \\mid t]\\) matrix is 3x4, stacking the rotation and translation side by side. This is all linear algebra from chapter 02.</p> </li> <li> <p>Real lenses introduce distortion. </p> <ul> <li>Radial distortion bends straight lines into curves (barrel distortion makes the image bulge outward; pincushion distortion squeezes it inward).  Tangential distortion arises when the lens is not perfectly parallel to the sensor. </li> </ul> </li> <li> <p>Camera calibration estimates both intrinsic parameters and distortion coefficients from images of a known pattern (like a checkerboard), then corrects (undistorts) images.</p> </li> <li> <p>Spatial filtering is the foundation of classical image processing. A filter (or kernel) is a small matrix (typically 3x3 or 5x5) that slides over the image. At each position, the filter values are multiplied element-wise with the overlapping image patch and summed to produce one output pixel. This is a 2D convolution, the same operation that powers CNNs (file 02), but here the filter weights are hand-designed rather than learned.</p> </li> </ul> \\[(\\text{image} * K)[i,j] = \\sum_{m} \\sum_{n} \\text{image}[i+m, j+n] \\cdot K[m, n]\\] <ul> <li> <p>This is a 2D extension of the 1D convolution from chapter 06. The filter determines what the operation detects: different filters detect different features.</p> </li> <li> <p>Blurring smooths an image by averaging neighbouring pixels. A box filter gives equal weight to all neighbours. </p> </li> <li> <p>A Gaussian filter weights neighbours by a 2D Gaussian (chapter 05), giving more weight to nearby pixels and less to distant ones. Gaussian blur is the most common smoothing operation and is parametrised by \\(\\sigma\\): larger \\(\\sigma\\) means more smoothing.</p> </li> <li> <p>Median filtering replaces each pixel with the median of its neighbourhood instead of a weighted average. It is particularly effective at removing salt-and-pepper noise (random black and white pixels) while preserving edges, because the median is robust to outliers (as discussed in chapter 04).</p> </li> <li> <p>Edge detection identifies boundaries where pixel intensity changes sharply. Edges carry most of the structural information in an image; you can recognise objects from their edges alone.</p> </li> <li> <p>The Sobel operator uses two 3x3 filters to estimate the gradient in the horizontal and vertical directions:</p> </li> </ul> \\[ G_x = \\begin{bmatrix} -1 &amp; 0 &amp; 1 \\\\ -2 &amp; 0 &amp; 2 \\\\ -1 &amp; 0 &amp; 1 \\end{bmatrix}, \\quad G_y = \\begin{bmatrix} -1 &amp; -2 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\end{bmatrix} \\] <ul> <li> <p>Convolving the image with \\(G_x\\) gives the horizontal gradient (strong response at vertical edges), and \\(G_y\\) gives the vertical gradient (strong response at horizontal edges). </p> </li> <li> <p>The gradient magnitude \\(\\sqrt{G_x^2 + G_y^2}\\) and direction \\(\\arctan(G_y / G_x)\\) together describe the edge strength and orientation at each pixel. This is the image-domain analogue of the gradient from chapter 03.</p> </li> </ul> <p></p> <ul> <li> <p>The Canny edge detector is the gold standard for edge detection. It applies four steps:</p> <ol> <li>Smooth the image with a Gaussian filter to reduce noise</li> <li>Compute gradient magnitude and direction (using Sobel)</li> <li>Non-maximum suppression: thin edges by keeping only pixels that are local maxima along the gradient direction</li> <li>Hysteresis thresholding: use two thresholds (high and low). Pixels above the high threshold are definite edges. Pixels between the thresholds are edges only if connected to a definite edge. Pixels below the low threshold are discarded.</li> </ol> </li> <li> <p>The two thresholds in Canny make it more robust than a single threshold: strong edges are always kept, and weak edges are kept only if they are part of a continuous edge structure.</p> </li> <li> <p>Frequency domain analysis reveals patterns that are hard to see in the spatial domain. The 2D Fourier transform (extending the 1D version from chapter 03) decomposes an image into a sum of 2D sinusoidal patterns at different frequencies and orientations:</p> </li> </ul> \\[F(u, v) = \\sum_{x=0}^{M-1} \\sum_{y=0}^{N-1} f(x, y) \\cdot e^{-j2\\pi(ux/M + vy/N)}\\] <ul> <li> <p>Low frequencies correspond to smooth, slowly-varying regions (the sky, a wall). High frequencies correspond to sharp transitions (edges, textures, noise). The magnitude spectrum shows how much energy exists at each frequency, and the phase spectrum encodes the spatial arrangement.</p> </li> <li> <p>Low-pass filtering removes high frequencies, which smooths the image (equivalent to Gaussian blur in the spatial domain). High-pass filtering removes low frequencies, emphasising edges and fine detail. Band-pass filtering keeps only a range of frequencies, useful for texture analysis.</p> </li> <li> <p>In practice, filtering in the frequency domain can be faster than spatial convolution for large filters, because convolution in the spatial domain is equivalent to element-wise multiplication in the frequency domain (the convolution theorem). This connects directly to the Fourier transform properties from chapter 03.</p> </li> <li> <p>Histograms summarise the distribution of pixel intensities. A histogram counts how many pixels have each intensity value (0-255 for 8-bit images). It is the same frequency distribution from chapter 04 applied to pixel values.</p> </li> </ul> <p></p> <ul> <li> <p>A dark image has its histogram concentrated on the left (low values). A bright image has it concentrated on the right. A low-contrast image has a narrow histogram. A high-contrast image has a wide, spread-out histogram.</p> </li> <li> <p>Histogram equalisation stretches the histogram to span the full intensity range, improving contrast. The idea is to find a mapping that makes the cumulative distribution function (CDF) of pixel intensities approximately linear. This is a direct application of the CDF concept from chapter 04.</p> </li> <li> <p>Otsu's method automatically finds the best threshold to separate an image into foreground and background. It tries every possible threshold and picks the one that minimises the within-class variance (or equivalently, maximises the between-class variance). This is the same variance concept from chapter 04, applied to pixel intensity populations.</p> </li> <li> <p>Feature extraction identifies distinctive points or regions in an image that can be used for matching, recognition, and 3D reconstruction. Good features should be repeatable (found again in a different view), distinctive (distinguishable from other features), and efficient to compute.</p> </li> <li> <p>Corner detection finds points where the image intensity changes significantly in multiple directions. A smooth region has little change in any direction. An edge has change in one direction. A corner has change in at least two directions, making it locally unique and therefore a reliable landmark.</p> </li> <li> <p>The Harris corner detector analyses the structure tensor (also called the second-moment matrix) at each pixel:</p> </li> </ul> \\[ M = \\sum_{(x,y) \\in W} w(x,y) \\begin{bmatrix} I_x^2 &amp; I_x I_y \\\\ I_x I_y &amp; I_y^2 \\end{bmatrix} \\] <ul> <li> <p>where \\(I_x\\) and \\(I_y\\) are the image gradients (computed with Sobel), \\(W\\) is a local window, and \\(w\\) is a Gaussian weighting function. The eigenvalues of \\(M\\) (from chapter 02) tell you the type of feature:</p> <ul> <li>Both eigenvalues small: flat region (no feature)</li> <li>One large, one small: edge</li> <li>Both large: corner</li> </ul> </li> <li> <p>Instead of computing eigenvalues explicitly, Harris uses a corner response function: \\(R = \\det(M) - k \\cdot (\\text{trace}(M))^2\\), where \\(\\det(M) = \\lambda_1 \\lambda_2\\) and \\(\\text{trace}(M) = \\lambda_1 + \\lambda_2\\) (both from chapter 02). Large positive \\(R\\) indicates a corner. The constant \\(k\\) is typically 0.04-0.06.</p> </li> <li> <p>The Shi-Tomasi detector simplifies this to \\(R = \\min(\\lambda_1, \\lambda_2)\\), directly checking that the smaller eigenvalue is large enough. This is slightly more stable in practice.</p> </li> <li> <p>Blob detection finds regions that differ from their surroundings. Unlike corners (which are point features), blobs have a characteristic size.</p> </li> <li> <p>SIFT (Scale-Invariant Feature Transform, Lowe, 2004) detects blobs at multiple scales and constructs a descriptor that is invariant to rotation, scale, and partially invariant to illumination changes. It works by:</p> <ol> <li>Building a scale space (see below) using Gaussian blur at increasing \\(\\sigma\\)</li> <li>Finding extrema in the Difference of Gaussians (DoG) across scales</li> <li>Refining keypoint locations and removing low-contrast points and edge responses</li> <li>Assigning a dominant orientation based on local gradient directions</li> <li>Building a 128-dimensional descriptor from gradient histograms in a 16x16 patch around the keypoint</li> </ol> </li> <li> <p>SURF (Speeded-Up Robust Features) approximates SIFT using box filters and integral images for faster computation. ORB (Oriented FAST and Rotated BRIEF) is a fast, open-source alternative that combines the FAST corner detector with the BRIEF binary descriptor, adding rotation invariance.</p> </li> <li> <p>HOG (Histogram of Oriented Gradients) descriptors divide the image into small cells, compute a histogram of gradient directions within each cell, and normalise across blocks of cells. HOG captures the distribution of edge orientations, which is highly informative for object shape. Before deep learning, HOG + SVM (chapter 06) was the dominant approach for pedestrian detection and object recognition.</p> </li> <li> <p>Image pyramids represent an image at multiple resolutions. </p> <ul> <li>A Gaussian pyramid is built by repeatedly blurring and downsampling (halving the resolution). Each level is a coarser version of the original. </li> <li>A Laplacian pyramid stores the difference between consecutive Gaussian levels, capturing the detail lost at each downsampling step. The Laplacian pyramid is invertible: you can reconstruct the original image from it.</li> </ul> </li> </ul> <p></p> <ul> <li>Scale space formalises the idea that objects exist at different scales. A tree is a large blob; a leaf on that tree is a small blob. To detect both, you need to search across scales. The scale space of an image is the family of images produced by convolving with Gaussians at increasing \\(\\sigma\\):</li> </ul> \\[L(x, y, \\sigma) = G(x, y, \\sigma) * I(x, y)\\] <ul> <li>where \\(G\\) is a 2D Gaussian with standard deviation \\(\\sigma\\). Features that persist across multiple scales are more likely to be meaningful structures rather than noise. Scale space is the theoretical foundation of SIFT and of the multi-scale processing used throughout modern computer vision, including the feature pyramid networks in object detection (file 03).</li> </ul>"},{"location":"chapter%2008%3A%20computer%20vision/01.%20image%20fundamentals/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Load an image, convert it to different colour spaces (RGB, HSV, LAB), and visualise the individual channels. Observe how colour information is distributed differently across spaces. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\n\n# Create a synthetic test image with distinct colours\nH, W = 128, 256\nimg = np.zeros((H, W, 3), dtype=np.uint8)\nimg[:, :64] = [255, 50, 50]     # red\nimg[:, 64:128] = [50, 255, 50]  # green\nimg[:, 128:192] = [50, 50, 255] # blue\nimg[:, 192:] = [255, 255, 50]   # yellow\n\n# Add a brightness gradient\nfor y in range(H):\n    scale = 0.3 + 0.7 * y / H\n    img[y] = (img[y] * scale).astype(np.uint8)\n\nimg_jnp = jnp.array(img, dtype=jnp.float32) / 255.0\n\n# Manual RGB to HSV conversion\ndef rgb_to_hsv(rgb):\n    r, g, b = rgb[..., 0], rgb[..., 1], rgb[..., 2]\n    maxc = jnp.max(rgb, axis=-1)\n    minc = jnp.min(rgb, axis=-1)\n    diff = maxc - minc + 1e-7\n\n    # Hue\n    h = jnp.where(maxc == minc, 0.0,\n        jnp.where(maxc == r, 60 * ((g - b) / diff % 6),\n        jnp.where(maxc == g, 60 * ((b - r) / diff + 2),\n                              60 * ((r - g) / diff + 4))))\n    s = jnp.where(maxc &lt; 1e-7, 0.0, diff / maxc)\n    v = maxc\n    return jnp.stack([h / 360, s, v], axis=-1)\n\nhsv = rgb_to_hsv(img_jnp)\n\nfig, axes = plt.subplots(2, 3, figsize=(14, 8))\nfor i, (ch, name) in enumerate(zip([img_jnp[...,0], img_jnp[...,1], img_jnp[...,2]],\n                                     ['Red', 'Green', 'Blue'])):\n    axes[0, i].imshow(ch, cmap='gray', vmin=0, vmax=1)\n    axes[0, i].set_title(f'RGB: {name}'); axes[0, i].axis('off')\n\nfor i, (ch, name) in enumerate(zip([hsv[...,0], hsv[...,1], hsv[...,2]],\n                                     ['Hue', 'Saturation', 'Value'])):\n    axes[1, i].imshow(ch, cmap='gray', vmin=0, vmax=1)\n    axes[1, i].set_title(f'HSV: {name}'); axes[1, i].axis('off')\n\nplt.suptitle('RGB vs HSV Channels')\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Implement Sobel edge detection and Gaussian blur from scratch using 2D convolution. Apply them to an image and compare the results. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef conv2d(image, kernel):\n    \"\"\"2D convolution (valid mode) from scratch.\"\"\"\n    H, W = image.shape\n    kH, kW = kernel.shape\n    out_h, out_w = H - kH + 1, W - kW + 1\n    output = jnp.zeros((out_h, out_w))\n    for i in range(out_h):\n        for j in range(out_w):\n            patch = image[i:i+kH, j:j+kW]\n            output = output.at[i, j].set(jnp.sum(patch * kernel))\n    return output\n\n# Create a test image: white rectangle on dark background\nimg = jnp.zeros((64, 64))\nimg = img.at[15:50, 20:45].set(1.0)\n# Add some noise\nkey = jax.random.PRNGKey(42)\nimg = img + jax.random.normal(key, img.shape) * 0.05\n\n# Sobel filters\nsobel_x = jnp.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=jnp.float32)\nsobel_y = jnp.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=jnp.float32)\n\n# Gaussian blur kernel (5x5, sigma=1)\nax = jnp.arange(-2, 3, dtype=jnp.float32)\nxx, yy = jnp.meshgrid(ax, ax)\ngaussian = jnp.exp(-(xx**2 + yy**2) / (2 * 1.0**2))\ngaussian = gaussian / gaussian.sum()\n\n# Apply filters\ngx = conv2d(img, sobel_x)\ngy = conv2d(img, sobel_y)\nedges = jnp.sqrt(gx**2 + gy**2)\nblurred = conv2d(img, gaussian)\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nfor ax, data, title in zip(axes,\n    [img, edges, blurred, gx],\n    ['Original', 'Edge Magnitude', 'Gaussian Blur', 'Horizontal Gradient']):\n    ax.imshow(data, cmap='gray')\n    ax.set_title(title); ax.axis('off')\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Implement histogram equalisation from scratch and apply it to a low-contrast grayscale image. Compare histograms before and after. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Create a low-contrast image (values clustered in a narrow range)\nkey = __import__('jax').random.PRNGKey(42)\nimg = __import__('jax').random.uniform(key, (128, 128)) * 0.3 + 0.3  # values in [0.3, 0.6]\n\ndef histogram_equalise(img, n_bins=256):\n    \"\"\"Histogram equalisation for a grayscale image.\"\"\"\n    # Quantise to bins\n    bins = jnp.linspace(0, 1, n_bins + 1)\n    hist = jnp.histogram(img, bins=bins)[0]\n\n    # Compute CDF\n    cdf = jnp.cumsum(hist)\n    cdf_normalised = (cdf - cdf.min()) / (cdf.max() - cdf.min())\n\n    # Map each pixel through the CDF\n    indices = jnp.clip((img * n_bins).astype(jnp.int32), 0, n_bins - 1)\n    equalised = cdf_normalised[indices]\n    return equalised\n\neq_img = histogram_equalise(img)\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\naxes[0, 0].imshow(img, cmap='gray', vmin=0, vmax=1)\naxes[0, 0].set_title('Original (Low Contrast)'); axes[0, 0].axis('off')\naxes[0, 1].imshow(eq_img, cmap='gray', vmin=0, vmax=1)\naxes[0, 1].set_title('After Histogram Equalisation'); axes[0, 1].axis('off')\n\naxes[1, 0].hist(img.ravel(), bins=64, color='#3498db', alpha=0.8)\naxes[1, 0].set_title('Histogram Before'); axes[1, 0].set_xlim(0, 1)\naxes[1, 1].hist(eq_img.ravel(), bins=64, color='#e74c3c', alpha=0.8)\naxes[1, 1].set_title('Histogram After'); axes[1, 1].set_xlim(0, 1)\n\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Implement the Harris corner detector from scratch. Detect corners in a simple image and visualise them. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef harris_corners(img, k=0.05, threshold=0.01):\n    \"\"\"Harris corner detection from scratch.\"\"\"\n    # Compute gradients with Sobel\n    sobel_x = jnp.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=jnp.float32)\n    sobel_y = jnp.array([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=jnp.float32)\n\n    # Pad image for valid convolution to preserve size\n    img_pad = jnp.pad(img, 1, mode='edge')\n    H, W = img.shape\n\n    Ix = jnp.zeros_like(img)\n    Iy = jnp.zeros_like(img)\n    for i in range(H):\n        for j in range(W):\n            patch = img_pad[i:i+3, j:j+3]\n            Ix = Ix.at[i, j].set(jnp.sum(patch * sobel_x))\n            Iy = Iy.at[i, j].set(jnp.sum(patch * sobel_y))\n\n    # Structure tensor components\n    Ixx = Ix * Ix\n    Iyy = Iy * Iy\n    Ixy = Ix * Iy\n\n    # Gaussian smoothing of structure tensor (approximate with window sum)\n    w = 3  # window half-size\n    R = jnp.zeros_like(img)\n    pad_xx = jnp.pad(Ixx, w, mode='constant')\n    pad_yy = jnp.pad(Iyy, w, mode='constant')\n    pad_xy = jnp.pad(Ixy, w, mode='constant')\n\n    for i in range(H):\n        for j in range(W):\n            sxx = jnp.sum(pad_xx[i:i+2*w+1, j:j+2*w+1])\n            syy = jnp.sum(pad_yy[i:i+2*w+1, j:j+2*w+1])\n            sxy = jnp.sum(pad_xy[i:i+2*w+1, j:j+2*w+1])\n            det = sxx * syy - sxy * sxy\n            trace = sxx + syy\n            R = R.at[i, j].set(det - k * trace * trace)\n\n    # Threshold\n    corners = R &gt; threshold * R.max()\n    return R, corners\n\n# Test image: checkerboard pattern (lots of corners)\nblock = 16\nn = 4\nchecker = jnp.zeros((block * n, block * n))\nfor i in range(n):\n    for j in range(n):\n        if (i + j) % 2 == 0:\n            checker = checker.at[i*block:(i+1)*block, j*block:(j+1)*block].set(1.0)\n\nR, corners = harris_corners(checker)\ncy, cx = jnp.where(corners)\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\naxes[0].imshow(checker, cmap='gray')\naxes[0].set_title('Checkerboard'); axes[0].axis('off')\naxes[1].imshow(R, cmap='hot')\naxes[1].set_title('Harris Response'); axes[1].axis('off')\naxes[2].imshow(checker, cmap='gray')\naxes[2].scatter(cx, cy, c='#e74c3c', s=15, marker='x')\naxes[2].set_title(f'Detected Corners ({len(cx)})'); axes[2].axis('off')\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2008%3A%20computer%20vision/02.%20convolutional%20networks/","title":"Convolutional Networks","text":"<p>Convolutional neural networks learn spatial feature hierarchies directly from pixel data, replacing hand-designed filters with gradient-optimised ones. This file covers convolution mechanics, pooling, stride, dilation, receptive fields, and landmark architectures (LeNet, AlexNet, VGG, ResNet, Inception, EfficientNet) that defined image classification.</p> <ul> <li> <p>In file 01, we hand-designed filters for edge detection, blurring, and corner detection. The natural question is: can we learn the optimal filters from data? That is exactly what convolutional neural networks (CNNs) do. </p> </li> <li> <p>Instead of choosing filter weights by hand, CNNs learn them via gradient descent (chapter 06), discovering features that are directly useful for the task at hand.</p> </li> <li> <p>In chapter 06, we introduced the convolution operation, CNN basics, and the idea of filter learning. Here we go deeper into the architectural innovations that made CNNs the dominant paradigm in computer vision for over a decade.</p> </li> <li> <p>Recall the core convolution operation: a filter \\(K\\) of size \\(k \\times k\\) slides over the input feature map, computing a dot product at each position (chapter 06). The output size is controlled by three hyperparameters:</p> <ul> <li>Stride: how many pixels the filter moves between positions. Stride 1 means the filter shifts one pixel at a time. Stride 2 means it shifts two pixels, halving the spatial dimensions. Strided convolution is an alternative to pooling for downsampling.</li> <li>Padding: adding zeros around the input border. \"Same\" padding (\\(p = \\lfloor k/2 \\rfloor\\)) preserves spatial dimensions. \"Valid\" padding (\\(p = 0\\)) reduces them.</li> <li>Dilation: inserting gaps between filter elements. A 3x3 filter with dilation 2 covers a 5x5 receptive field using only 9 parameters. Dilated convolutions expand the receptive field without increasing computation.</li> </ul> </li> <li> <p>The output spatial size after convolution:</p> </li> </ul> \\[\\text{out} = \\left\\lfloor \\frac{\\text{in} - k + 2p}{s} \\right\\rfloor + 1\\] <ul> <li> <p>where \\(\\text{in}\\) is the input size, \\(k\\) is the kernel size, \\(p\\) is padding, and \\(s\\) is stride. This formula applies independently to height and width.</p> </li> <li> <p>The receptive field of a neuron is the region of the original input that can influence its value. </p> <ul> <li>Early layers have small receptive fields (they see local patterns like edges). </li> <li>Deeper layers have larger receptive fields (they see larger structures like object parts). </li> </ul> </li> <li> <p>The receptive field grows with each layer: roughly by \\(k - 1\\) pixels per convolutional layer (more with stride or dilation). </p> </li> </ul> <p></p> <ul> <li> <p>Pooling layers reduce spatial dimensions while retaining the most important information. </p> <ul> <li>Max pooling takes the maximum value in each window, preserving the strongest activation (the most prominent feature). </li> <li>Average pooling takes the mean, smoothing the feature map. A 2x2 pool with stride 2 halves both spatial dimensions.</li> </ul> </li> <li> <p>Global Average Pooling (GAP) averages the entire spatial extent of each channel into a single number, producing a vector of length equal to the number of channels. GAP replaces the fully connected layers at the end of many modern architectures, drastically reducing parameter count and acting as a structural regulariser.</p> </li> <li> <p>Batch Normalisation (BatchNorm) normalises activations within each mini-batch to have zero mean and unit variance, then applies a learnable scale and shift (chapter 06). In CNNs, BatchNorm is applied per-channel: statistics are computed across the batch and spatial dimensions for each channel independently. It stabilises training, allows higher learning rates, and acts as a mild regulariser.</p> </li> <li> <p>Dropout (chapter 06) randomly zeroes neurons during training. </p> </li> <li> <p>In CNNs, spatial dropout (Dropout2D) drops entire feature map channels rather than individual pixels, which is more effective because neighbouring pixels in a feature map are highly correlated.</p> </li> <li> <p>Data augmentation artificially expands the training set by applying random transformations to each image during training: horizontal flips, random crops, rotations, colour jitter (adjusting brightness, contrast, saturation, hue), and cutout (masking random rectangular patches). The network sees each image in many different forms, forcing it to learn transformation-invariant features rather than memorising specific pixel patterns.</p> </li> <li> <p>Advanced augmentation strategies include Mixup (blending two images and their labels: \\(\\tilde{x} = \\lambda x_i + (1-\\lambda) x_j\\), \\(\\tilde{y} = \\lambda y_i + (1-\\lambda) y_j\\)), CutMix (pasting a rectangular patch from one image onto another and mixing labels proportionally to area), and RandAugment (randomly sampling a sequence of augmentations from a fixed set with a single strength parameter).</p> </li> <li> <p>The history of CNN architectures is a story of progressively deeper, more efficient designs, each solving a problem that limited its predecessor.</p> </li> <li> <p>LeNet-5 (LeCun et al., 1998) was the original CNN, designed for handwritten digit recognition. Two convolutional layers followed by three fully connected layers, with average pooling and tanh activations. It proved that learned filters outperform hand-designed features, but it was tiny by modern standards (60K parameters).</p> </li> <li> <p>AlexNet (Krizhevsky et al., 2012) won the ImageNet competition by a massive margin, igniting the deep learning revolution. Key innovations: ReLU activation (instead of tanh, which suffers from vanishing gradients), dropout for regularisation, data augmentation, and training on GPUs. Five convolutional layers, three fully connected layers, 60 million parameters.</p> </li> <li> <p>VGG (Simonyan and Zisserman, 2014) showed that using only 3x3 filters stacked deeply works better than larger filters. Two stacked 3x3 filters have the same receptive field as one 5x5 filter but fewer parameters (\\(2 \\times 3^2 = 18\\) vs \\(5^2 = 25\\)) and an extra nonlinearity. VGG-16 (16 layers) and VGG-19 (19 layers) are still widely used as feature extractors. The architecture is remarkably simple: convolution blocks with increasing channels (64, 128, 256, 512), each followed by max pooling.</p> </li> </ul> <p></p> <ul> <li>GoogLeNet/Inception (Szegedy et al., 2014) introduced the Inception module: instead of choosing a single filter size, use 1x1, 3x3, and 5x5 convolutions in parallel, concatenate their outputs, and let the network decide which scale is most useful. 1x1 convolutions are used as bottlenecks before the larger filters to reduce computation. GoogLeNet achieved better accuracy than VGG with 12x fewer parameters (6.8M vs 138M).</li> </ul> <p></p> <ul> <li> <p>The Inception module captures features at multiple scales simultaneously. A 1x1 filter captures point-wise patterns, a 3x3 captures local texture, and a 5x5 captures larger structures. The concatenation combines all perspectives into a rich representation.</p> </li> <li> <p>ResNet (He et al., 2016) solved the degradation problem: deeper networks performed worse than shallower ones, not because of overfitting, but because they were harder to optimise. The solution is the skip connection (residual connection):</p> </li> </ul> \\[\\text{output} = F(x) + x\\] <ul> <li>The layer learns the residual \\(F(x) = \\text{output} - x\\). If the optimal transformation is close to identity (which is common in deep networks), learning a near-zero residual is much easier than learning the full mapping. Skip connections also provide a direct gradient highway, reducing vanishing gradients. ResNet trained networks with 152 layers, far deeper than anything before.</li> </ul> <p></p> <ul> <li> <p>When the input and output dimensions differ (due to stride or channel change), a projection shortcut applies a 1x1 convolution to \\(x\\) to match dimensions: \\(\\text{output} = F(x) + W_s x\\).</p> </li> <li> <p>The bottleneck block (used in ResNet-50 and deeper) uses three convolutions: 1x1 to reduce channels, 3x3 for spatial processing, and 1x1 to expand channels back. This is cheaper than two 3x3 convolutions and allows much deeper networks.</p> </li> <li> <p>DenseNet (Huang et al., 2017) takes the skip connection idea further: every layer is connected to every subsequent layer within a dense block. Layer \\(l\\) receives the feature maps from all preceding layers as input: \\(x_l = H_l([x_0, x_1, \\ldots, x_{l-1}])\\), where \\([\\cdot]\\) denotes concatenation along the channel dimension. This encourages feature reuse, strengthens gradient flow, and reduces the total number of parameters.</p> </li> </ul> <p></p> <ul> <li> <p>Efficient architectures target deployment on mobile devices and edge hardware, where compute, memory, and energy are constrained.</p> </li> <li> <p>MobileNet (Howard et al., 2017) replaces standard convolutions with depthwise separable convolutions, which factorise the operation into two steps:</p> <ol> <li>Depthwise convolution: apply a single \\(k \\times k\\) filter per input channel (no cross-channel interaction)</li> <li>Pointwise convolution: apply 1x1 convolutions to combine information across channels</li> </ol> </li> <li> <p>A standard \\(k \\times k\\) convolution with \\(C_{\\text{in}}\\) input channels and \\(C_{\\text{out}}\\) output channels costs \\(k^2 \\cdot C_{\\text{in}} \\cdot C_{\\text{out}}\\) multiplications per spatial position. Depthwise separable convolution costs \\(k^2 \\cdot C_{\\text{in}} + C_{\\text{in}} \\cdot C_{\\text{out}}\\), a reduction of roughly \\(k^2\\) times. For a 3x3 filter, this is approximately 9x cheaper.</p> </li> </ul> <p></p> <ul> <li> <p>MobileNet-V2 introduced the inverted residual block: expand channels with a 1x1 convolution, apply depthwise convolution in the expanded space, then project back down with a 1x1 convolution. The skip connection is placed on the narrow (bottleneck) layers, inverting the ResNet pattern. The expansion ratio is typically 6.</p> </li> <li> <p>EfficientNet (Tan and Le, 2019) introduced compound scaling: instead of scaling only depth, only width, or only resolution independently, scale all three dimensions together using a fixed ratio. Given a scaling coefficient \\(\\phi\\):</p> </li> </ul> \\[\\text{depth}: d = \\alpha^\\phi, \\quad \\text{width}: w = \\beta^\\phi, \\quad \\text{resolution}: r = \\gamma^\\phi\\] <ul> <li>subject to \\(\\alpha \\cdot \\beta^2 \\cdot \\gamma^2 \\approx 2\\) (so that total computation roughly doubles per unit increase in \\(\\phi\\)). A grid search finds \\(\\alpha = 1.2\\), \\(\\beta = 1.1\\), \\(\\gamma = 1.15\\) as the baseline ratios. EfficientNet-B0 through B7 scale up progressively, achieving state-of-the-art accuracy with far fewer parameters and FLOPs than previous models.</li> </ul> <p></p> <ul> <li> <p>ShuffleNet reduces the cost of 1x1 convolutions (which dominate in MobileNet-style architectures) by using group convolutions followed by a channel shuffle. Group convolutions split channels into groups and convolve within each group independently, but this prevents cross-group information flow. The shuffle operation rearranges channels between groups, restoring the information mixing at negligible cost.</p> </li> <li> <p>Transfer learning is the practice of taking a model trained on one task and adapting it to a different task. In computer vision, this almost always means starting from a model pre-trained on ImageNet (1.4 million images, 1,000 classes) and adapting to a domain-specific dataset (medical images, satellite images, manufacturing defects).</p> </li> <li> <p>Feature extraction: freeze all convolutional layers, remove the final classification head, and train only a new head on top. The frozen layers act as a generic feature extractor. This works well when the target domain is similar to ImageNet and the target dataset is small.</p> </li> <li> <p>Fine-tuning: unfreeze some or all convolutional layers and train with a small learning rate. The pre-trained weights serve as a starting point rather than fixed features. Fine-tuning typically starts by unfreezing only the later layers (which capture high-level, task-specific features) and optionally unfreezing earlier layers as well.</p> </li> <li> <p>Transfer learning works because the early layers of a CNN learn universal features (edges, textures, colours) that are useful across tasks, while later layers learn task-specific features. A network trained to classify animals still has useful edge detectors for classifying buildings.</p> </li> <li> <p>Visualising CNNs reveals what the network has learned and helps debug unexpected behaviour.</p> </li> <li> <p>Activation maps (feature maps) show the output of each filter for a given input image. Early layer activations look like edge maps; deeper layers produce increasingly abstract, spatially coarse activations.</p> </li> <li> <p>Grad-CAM (Gradient-weighted Class Activation Mapping, Selvaraju et al., 2017) highlights the regions of the input image that were most important for the model's prediction. It works by:</p> <ol> <li>Computing the gradient of the target class score with respect to the feature maps of the last convolutional layer (using the chain rule from chapter 03)</li> <li>Global average pooling these gradients to get per-channel importance weights</li> <li>Computing a weighted combination of the feature maps and applying ReLU</li> </ol> </li> </ul> \\[L_{\\text{Grad-CAM}} = \\text{ReLU}\\!\\left(\\sum_k \\alpha_k A^k\\right), \\quad \\alpha_k = \\frac{1}{Z} \\sum_i \\sum_j \\frac{\\partial y^c}{\\partial A^k_{ij}}\\] <ul> <li>where \\(A^k\\) is the \\(k\\)-th feature map, \\(\\alpha_k\\) is the importance weight for channel \\(k\\), and \\(y^c\\) is the score for class \\(c\\). The result is a coarse heatmap showing which regions drove the classification. ReLU is applied because we are interested in features that have a positive influence on the class.</li> </ul> <p></p> <ul> <li> <p>Feature inversion reconstructs an input image from its feature representation by optimising a random image to match the target features (using gradient descent on the pixel values). This reveals what information the network retains at each layer. Early layers reconstruct near-perfect images; deeper layers produce recognisable but distorted images, showing that fine spatial detail is lost while semantic content is preserved.</p> </li> <li> <p>Deep Dream and neural style transfer are creative applications of feature visualisation. Deep Dream maximises the activation of neurons at a chosen layer to produce surreal, pattern-amplified images. Neural style transfer optimises a target image to match the content features (from a deep layer) of one image and the style features (Gram matrix of filter activations, which captures texture statistics) of another.</p> </li> </ul>"},{"location":"chapter%2008%3A%20computer%20vision/02.%20convolutional%20networks/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement a simple CNN from scratch in JAX with two convolutional layers, max pooling, and a classification head. Train it on a synthetic 2D pattern classification task. <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.lax as lax\nimport matplotlib.pyplot as plt\n\ndef conv2d(x, kernel, stride=1):\n    \"\"\"Simple 2D convolution for single input, single filter.\"\"\"\n    return lax.conv(x[None, None], kernel[None, None], (stride, stride), 'SAME')[0, 0]\n\ndef max_pool(x, size=2):\n    \"\"\"2x2 max pooling.\"\"\"\n    H, W = x.shape\n    x = x[:H//size*size, :W//size*size]\n    return x.reshape(H//size, size, W//size, size).max(axis=(1, 3))\n\ndef init_cnn(key):\n    k1, k2, k3 = jax.random.split(key, 3)\n    return {\n        'conv1': jax.random.normal(k1, (5, 5)) * 0.3,\n        'conv2': jax.random.normal(k2, (3, 3)) * 0.3,\n        'fc_w': jax.random.normal(k3, (64, 1)) * 0.1,\n        'fc_b': jnp.zeros(1),\n    }\n\ndef forward_cnn(params, img):\n    # Conv1 -&gt; ReLU -&gt; Pool\n    h = jnp.maximum(0, conv2d(img, params['conv1']))\n    h = max_pool(h)\n    # Conv2 -&gt; ReLU -&gt; Pool\n    h = jnp.maximum(0, conv2d(h, params['conv2']))\n    h = max_pool(h)\n    # Flatten and classify\n    flat = h.ravel()\n    # Pad or truncate to fixed size\n    flat = jnp.pad(flat, (0, max(0, 64 - len(flat))))[:64]\n    logit = (flat @ params['fc_w'] + params['fc_b']).squeeze()\n    return jax.nn.sigmoid(logit)\n\n# Generate synthetic data: class 0 = low-freq pattern, class 1 = high-freq\ndef make_data(key, n=200):\n    images, labels = [], []\n    for i in range(n):\n        k1, key = jax.random.split(key)\n        x, y = jnp.meshgrid(jnp.linspace(0, 4*jnp.pi, 32), jnp.linspace(0, 4*jnp.pi, 32))\n        if i &lt; n // 2:\n            img = jnp.sin(x) + jax.random.normal(k1, (32, 32)) * 0.1\n            labels.append(0)\n        else:\n            img = jnp.sin(4 * x) * jnp.sin(4 * y) + jax.random.normal(k1, (32, 32)) * 0.1\n            labels.append(1)\n        images.append(img)\n    return images, jnp.array(labels, dtype=jnp.float32)\n\nkey = jax.random.PRNGKey(42)\nimages, labels = make_data(key)\nparams = init_cnn(jax.random.PRNGKey(0))\n\ndef loss_fn(params, img, label):\n    pred = forward_cnn(params, img)\n    return -(label * jnp.log(pred + 1e-7) + (1 - label) * jnp.log(1 - pred + 1e-7))\n\ngrad_fn = jax.grad(loss_fn)\nlr = 0.01\n\nfor epoch in range(5):\n    total_loss = 0.0\n    for img, label in zip(images, labels):\n        grads = grad_fn(params, img, label)\n        params = {k: params[k] - lr * grads[k] for k in params}\n        total_loss += loss_fn(params, img, label)\n    print(f\"Epoch {epoch}: loss = {total_loss / len(images):.4f}\")\n\n# Test accuracy\npreds = jnp.array([forward_cnn(params, img) &gt; 0.5 for img in images])\nacc = jnp.mean(preds == labels)\nprint(f\"Accuracy: {acc:.2%}\")\n</code></pre></p> </li> <li> <p>Visualise how different filter sizes affect the receptive field. Show that two stacked 3x3 filters cover the same receptive field as one 5x5 filter but with fewer parameters. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef compute_receptive_field(layers):\n    \"\"\"Compute receptive field size from a list of (kernel_size, stride) tuples.\"\"\"\n    rf = 1  # start with 1 pixel\n    stride_product = 1\n    for k, s in layers:\n        rf += (k - 1) * stride_product\n        stride_product *= s\n    return rf\n\n# Compare architectures\nconfigs = {\n    'Single 5x5': [(5, 1)],\n    'Two 3x3':    [(3, 1), (3, 1)],\n    'Three 3x3':  [(3, 1), (3, 1), (3, 1)],\n    'Single 7x7': [(7, 1)],\n    '3x3 stride 2 + 3x3': [(3, 2), (3, 1)],\n}\n\nprint(f\"{'Config':&lt;25} {'RF':&gt;4} {'Params (per channel)':&gt;20}\")\nprint('-' * 55)\nfor name, layers in configs.items():\n    rf = compute_receptive_field(layers)\n    # Parameters: sum of k^2 for each layer (per input-output channel pair)\n    params = sum(k * k for k, s in layers)\n    print(f\"{name:&lt;25} {rf:&gt;4} {params:&gt;20}\")\n\n# Visualise receptive fields\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\nfor ax, (name, rf_size) in zip(axes, [('5x5 filter', 5), ('Two 3x3 filters', 5), ('Three 3x3 filters', 7)]):\n    grid = jnp.zeros((9, 9))\n    c = 4  # centre\n    half = rf_size // 2\n    grid = grid.at[c-half:c+half+1, c-half:c+half+1].set(1.0)\n    ax.imshow(grid, cmap='Blues', vmin=0, vmax=1)\n    ax.set_title(f'{name}\\nRF = {rf_size}x{rf_size}')\n    ax.set_xticks(range(9)); ax.set_yticks(range(9))\n    ax.grid(True, alpha=0.3)\nplt.suptitle('Receptive Field Comparison')\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Implement Grad-CAM from scratch. Given a pre-built simple CNN, compute the gradient-weighted activation map for a specific class and visualise it as a heatmap. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef simple_cnn(params, img):\n    \"\"\"Simple CNN that returns both the prediction and last conv activations.\"\"\"\n    # Conv layer (our \"last conv layer\" for Grad-CAM)\n    H, W = img.shape\n    k = params['conv'].shape[0]\n    pad = k // 2\n    img_pad = jnp.pad(img, pad, mode='edge')\n    activation_map = jnp.zeros((H, W))\n    for i in range(H):\n        for j in range(W):\n            activation_map = activation_map.at[i, j].set(\n                jnp.sum(img_pad[i:i+k, j:j+k] * params['conv'])\n            )\n    activation_map = jnp.maximum(0, activation_map)  # ReLU\n\n    # Global average pool -&gt; dense -&gt; output\n    pooled = activation_map.mean()\n    logit = pooled * params['w'] + params['b']\n    return jax.nn.sigmoid(logit), activation_map\n\n# Create test image: bright region on the left (class indicator)\nimg = jnp.zeros((32, 32))\nimg = img.at[8:24, 4:16].set(1.0)\nimg = img.at[5:10, 20:28].set(0.3)\n\nkey = jax.random.PRNGKey(42)\nparams = {\n    'conv': jax.random.normal(key, (5, 5)) * 0.3,\n    'w': jnp.array(2.0),\n    'b': jnp.array(-0.5),\n}\n\n# Compute Grad-CAM\ndef class_score(params, img):\n    pred, _ = simple_cnn(params, img)\n    return pred\n\n# Get activation map and gradients\npred, act_map = simple_cnn(params, img)\ngrad_fn = jax.grad(lambda img: simple_cnn(params, img)[0])\nimg_grad = grad_fn(img)\n\n# Weight = global average of gradients (simplified 1-channel Grad-CAM)\nalpha = img_grad.mean()\ngrad_cam = jnp.maximum(0, alpha * act_map)  # ReLU\ngrad_cam = (grad_cam - grad_cam.min()) / (grad_cam.max() - grad_cam.min() + 1e-8)\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\naxes[0].imshow(img, cmap='gray'); axes[0].set_title('Input Image'); axes[0].axis('off')\naxes[1].imshow(act_map, cmap='viridis'); axes[1].set_title('Activation Map'); axes[1].axis('off')\naxes[2].imshow(img, cmap='gray', alpha=0.6)\naxes[2].imshow(grad_cam, cmap='jet', alpha=0.4)\naxes[2].set_title(f'Grad-CAM (pred={pred:.2f})'); axes[2].axis('off')\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Compare depthwise separable convolution with standard convolution. Count the parameters and FLOPs for both and show they produce similar outputs with far less computation. <pre><code>import jax\nimport jax.numpy as jnp\n\ndef standard_conv(x, kernel):\n    \"\"\"Standard convolution: (H, W, C_in) * (k, k, C_in, C_out) -&gt; (H, W, C_out).\"\"\"\n    H, W, C_in = x.shape\n    k, _, _, C_out = kernel.shape\n    pad = k // 2\n    x_pad = jnp.pad(x, ((pad, pad), (pad, pad), (0, 0)), mode='constant')\n    out = jnp.zeros((H, W, C_out))\n    for i in range(H):\n        for j in range(W):\n            patch = x_pad[i:i+k, j:j+k, :]  # (k, k, C_in)\n            for c in range(C_out):\n                out = out.at[i, j, c].set(jnp.sum(patch * kernel[:, :, :, c]))\n    return out\n\ndef depthwise_separable_conv(x, dw_kernel, pw_kernel):\n    \"\"\"Depthwise separable: depthwise (k,k,C_in) then pointwise (C_in, C_out).\"\"\"\n    H, W, C_in = x.shape\n    k = dw_kernel.shape[0]\n    pad = k // 2\n    x_pad = jnp.pad(x, ((pad, pad), (pad, pad), (0, 0)), mode='constant')\n\n    # Depthwise: one filter per channel\n    dw_out = jnp.zeros((H, W, C_in))\n    for i in range(H):\n        for j in range(W):\n            for c in range(C_in):\n                patch = x_pad[i:i+k, j:j+k, c]\n                dw_out = dw_out.at[i, j, c].set(jnp.sum(patch * dw_kernel[:, :, c]))\n\n    # Pointwise: 1x1 conv across channels\n    out = dw_out @ pw_kernel\n    return out\n\n# Setup\nH, W, C_in, C_out, k = 8, 8, 16, 32, 3\nkey = jax.random.PRNGKey(42)\nk1, k2, k3, k4 = jax.random.split(key, 4)\n\nx = jax.random.normal(k1, (H, W, C_in))\nstd_kernel = jax.random.normal(k2, (k, k, C_in, C_out)) * 0.1\ndw_kernel = jax.random.normal(k3, (k, k, C_in)) * 0.1\npw_kernel = jax.random.normal(k4, (C_in, C_out)) * 0.1\n\n# Compare\nstd_params = k * k * C_in * C_out\ndw_params = k * k * C_in + C_in * C_out\n\nstd_flops = H * W * k * k * C_in * C_out\ndw_flops = H * W * (k * k * C_in + C_in * C_out)\n\nprint(f\"Standard conv:            {std_params:&gt;8,} params,  {std_flops:&gt;10,} FLOPs\")\nprint(f\"Depthwise separable conv: {dw_params:&gt;8,} params,  {dw_flops:&gt;10,} FLOPs\")\nprint(f\"Parameter reduction:      {std_params / dw_params:.1f}x\")\nprint(f\"FLOP reduction:           {std_flops / dw_flops:.1f}x\")\n\nstd_out = standard_conv(x, std_kernel)\nds_out = depthwise_separable_conv(x, dw_kernel, pw_kernel)\nprint(f\"\\nStandard output shape:    {std_out.shape}\")\nprint(f\"Depthwise sep output shape: {ds_out.shape}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2008%3A%20computer%20vision/03.%20object%20detection%20and%20segmentation/","title":"Object Detection and Segmentation","text":"<p>Object detection localises and classifies every object in an image; segmentation assigns a label to every pixel. This file covers IoU, mAP, anchor boxes, R-CNN family, YOLO, SSD, Feature Pyramid Networks, semantic/instance/panoptic segmentation (U-Net, Mask R-CNN, SAM), and the metrics that benchmark them.</p> <ul> <li> <p>Image classification (file 02) answers \"what is in this image?\" Object detection asks a harder question: \"what objects are in this image, and where are they?\" </p> </li> <li> <p>Segmentation goes further still: \"which pixels belong to which object or category?\" These tasks form a hierarchy of increasingly precise spatial understanding.</p> </li> <li> <p>An object detection model outputs a set of bounding boxes, each defined by four coordinates (top-left corner \\(x, y\\), width, height) and a class label with a confidence score. A single image may contain zero, one, or hundreds of objects from multiple classes.</p> </li> </ul> <p></p> <ul> <li>Intersection over Union (IoU) measures how well a predicted bounding box matches the ground truth. It is the area of overlap divided by the area of union:</li> </ul> \\[\\text{IoU} = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}\\] <ul> <li> <p>An IoU of 1 means perfect overlap; an IoU of 0 means no overlap at all. The standard threshold for a \"correct\" detection is IoU \\(\\geq 0.5\\), though stricter thresholds (0.75, 0.9) are also used.</p> </li> <li> <p>A detection is a true positive (TP) if its IoU with a ground truth box exceeds the threshold and the class is correct. </p> </li> <li> <p>A false positive (FP) is a predicted box that does not match any ground truth. </p> </li> <li> <p>A false negative (FN) is a ground truth object that no prediction matched. These are the same precision/recall concepts from chapter 06.</p> </li> <li> <p>Average Precision (AP) summarises detection quality for one class. For each class, rank all detections by confidence score, compute precision and recall at each rank, and calculate the area under the precision-recall curve:</p> </li> </ul> \\[\\text{AP} = \\int_0^1 p(r) \\, dr\\] <ul> <li> <p>In practice, the curve is interpolated: at each recall level, precision is set to the maximum precision at any recall \\(\\geq r\\). This smooths the curve and makes it monotonically decreasing.</p> </li> <li> <p>Mean Average Precision (mAP) averages AP across all classes. \"mAP@0.5\" uses IoU threshold 0.5. \"mAP@[.5:.95]\" (the COCO standard) averages mAP over ten IoU thresholds from 0.5 to 0.95 in steps of 0.05, rewarding both detection and precise localisation.</p> </li> <li> <p>Non-Maximum Suppression (NMS) removes duplicate detections. When a model predicts multiple overlapping boxes for the same object, NMS keeps the highest-confidence box and removes all others that overlap with it above an IoU threshold. This is applied per class after the model produces its raw predictions.</p> </li> <li> <p>Two-stage detectors first propose candidate regions, then classify and refine each proposal.</p> </li> <li> <p>R-CNN (Girshick et al., 2014) was the first successful deep learning detector. It uses selective search (a classical algorithm) to propose ~2,000 candidate regions, warps each region to a fixed size, runs each through a CNN independently, and classifies with an SVM (chapter 06). R-CNN was accurate but extremely slow: it ran the CNN 2,000 times per image.</p> </li> <li> <p>Fast R-CNN (Girshick, 2015) solved the redundancy by running the CNN once on the entire image to produce a shared feature map, then extracting features for each proposal from that shared map using RoI pooling (Region of Interest pooling). </p> </li> <li> <p>RoI pooling takes a variable-sized region of the feature map and produces a fixed-size output by dividing the region into a grid and max-pooling within each cell. This is much faster because the expensive CNN computation happens only once.</p> </li> <li> <p>Faster R-CNN (Ren et al., 2015) eliminated the external region proposal algorithm by introducing the Region Proposal Network (RPN), a small CNN that runs on top of the shared feature map and predicts proposals directly. The RPN slides a small window over the feature map and, at each position, predicts \\(k\\) proposals (one for each anchor box).</p> </li> </ul> <p></p> <ul> <li> <p>Anchor boxes are predefined bounding boxes at each spatial position of the feature map, covering different scales and aspect ratios (e.g., three scales \\(\\times\\) three ratios = 9 anchors per position). The RPN predicts two things for each anchor: an objectness score (object vs background) and coordinate offsets that refine the anchor into a tighter proposal. This parametrisation makes the regression problem easier: instead of predicting absolute coordinates, the network predicts small adjustments to a reasonable starting box.</p> </li> <li> <p>The anchor offsets are parametrised as:</p> </li> </ul> \\[t_x = \\frac{x - x_a}{w_a}, \\quad t_y = \\frac{y - y_a}{h_a}, \\quad t_w = \\log\\frac{w}{w_a}, \\quad t_h = \\log\\frac{h}{h_a}\\] <ul> <li> <p>where \\((x, y, w, h)\\) are the predicted box centre and size, and \\((x_a, y_a, w_a, h_a)\\) are the anchor. The log transform for width and height ensures the predicted box is always positive and makes the regression scale-invariant.</p> </li> <li> <p>Faster R-CNN trains with a multi-task loss: classification loss (cross-entropy from chapter 05) for the class label, plus a smooth L1 loss for box regression. Smooth L1 is less sensitive to outliers than L2:</p> </li> </ul> \\[ \\text{smooth}_{L1}(x) = \\begin{cases} 0.5x^2 &amp; \\text{if } |x| &lt; 1 \\\\ |x| - 0.5 &amp; \\text{otherwise} \\end{cases} \\] <ul> <li> <p>Feature Pyramid Networks (FPN) (Lin et al., 2017) address the multi-scale problem by building a top-down pathway with lateral connections that merges high-level semantics with low-level spatial detail. The backbone produces feature maps at multiple scales (each pooling layer halves the resolution). FPN adds a top-down path where each level receives upsampled features from the level above and merges them with the corresponding bottom-up level via lateral 1x1 convolutions. The result is a pyramid of feature maps, each with both strong semantics and good spatial resolution.</p> </li> <li> <p>Small objects are detected from the higher-resolution levels of the pyramid; large objects from the lower-resolution levels. FPN is now a standard component in most modern detection architectures.</p> </li> <li> <p>One-stage detectors skip the proposal step entirely, predicting class labels and bounding boxes in a single pass. This is faster but was historically less accurate than two-stage detectors, until focal loss closed the gap.</p> </li> <li> <p>YOLO (You Only Look Once, Redmon et al., 2016) divides the image into an \\(S \\times S\\) grid. Each grid cell predicts \\(B\\) bounding boxes and \\(C\\) class probabilities. If the centre of an object falls in a grid cell, that cell is responsible for detecting it. YOLO is extremely fast because the entire detection is a single forward pass with no proposal stage.</p> </li> <li> <p>YOLOv2 added anchor boxes, batch normalisation, and multi-scale training. YOLOv3 used a Feature Pyramid Network and predicted at three scales. YOLOv4-v8 continued improving with better backbones, path aggregation networks, and mosaic data augmentation (stitching four images together during training to increase context diversity).</p> </li> <li> <p>SSD (Single Shot MultiBox Detector, Liu et al., 2016) predicts at multiple feature map scales within the backbone, using anchor boxes at each scale. Early (high-resolution) feature maps detect small objects; later (low-resolution) maps detect large objects. SSD is faster than Faster R-CNN with competitive accuracy.</p> </li> <li> <p>RetinaNet (Lin et al., 2017) identified the core problem with one-stage detectors: class imbalance. The vast majority of anchor boxes correspond to background, which generates easy negatives that dominate the loss and overwhelm the gradients from the rare positive examples.</p> </li> <li> <p>Focal loss solves this by down-weighting easy examples:</p> </li> </ul> \\[\\text{FL}(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t)\\] <ul> <li> <p>where \\(p_t\\) is the predicted probability for the correct class. When the model is confident and correct (\\(p_t\\) is high), \\((1 - p_t)^\\gamma\\) is small, reducing the loss contribution from easy negatives. The hyperparameter \\(\\gamma\\) (typically 2) controls the strength of the down-weighting. With \\(\\gamma = 0\\), focal loss reduces to standard cross-entropy. With focal loss, RetinaNet achieved accuracy comparable to two-stage detectors at one-stage speed.</p> </li> <li> <p>Anchor-free detection eliminates anchor boxes entirely, reducing hyperparameter tuning and simplifying the pipeline.</p> </li> <li> <p>FCOS (Fully Convolutional One-Stage, Tian et al., 2019) predicts, at every spatial position of the feature map, the distances from that position to the four sides of the nearest bounding box (left, top, right, bottom) plus a class label. A centerness score down-weights predictions far from the object centre, improving quality. FCOS uses FPN to handle multiple scales.</p> </li> <li> <p>CenterNet (Zhou et al., 2019) detects objects as points: it predicts a heatmap where peaks correspond to object centres, then regresses the width and height at each peak. Detection becomes keypoint estimation. This is elegant and anchor-free, but requires careful heatmap post-processing.</p> </li> <li> <p>CornerNet detects objects as pairs of corners (top-left and bottom-right). It predicts two heatmaps (one for each corner type) and uses an associative embedding to match corresponding corners into bounding boxes. This avoids the need for anchors and handles objects of arbitrary shape.</p> </li> <li> <p>Semantic segmentation assigns a class label to every pixel in the image. Unlike detection (which outputs boxes), segmentation produces a dense pixel-level map. A street scene might label every pixel as road, sidewalk, car, pedestrian, building, sky, etc.</p> </li> </ul> <p></p> <ul> <li> <p>Fully Convolutional Networks (FCN) (Long et al., 2015) adapted classification CNNs for segmentation by replacing fully connected layers with convolutional layers, allowing the network to output a spatial map rather than a single class. Upsampling (via transposed convolutions or bilinear interpolation) restores the output to the input resolution. Skip connections from earlier layers add back spatial detail lost during downsampling.</p> </li> <li> <p>Transposed convolution (sometimes called \"deconvolution\") is the upsampling counterpart of convolution. Where strided convolution reduces spatial dimensions, transposed convolution increases them. It inserts zeros between input elements and then applies a standard convolution, effectively learning how to upsample.</p> </li> <li> <p>U-Net (Ronneberger et al., 2015) introduced a symmetric encoder-decoder architecture with skip connections at every level. The encoder (contracting path) reduces spatial resolution while increasing channels, exactly like a classification CNN. The decoder (expanding path) upsamples back to full resolution. Skip connections concatenate encoder feature maps with decoder feature maps at each level, providing fine spatial detail to the decoder. This combination of high-level semantics and low-level detail produces sharp, accurate segmentation boundaries.</p> </li> </ul> <p></p> <ul> <li> <p>U-Net was originally designed for biomedical image segmentation (where training data is scarce) and its architecture has become the foundation for many subsequent models, including the U-Net in latent diffusion models (file 04).</p> </li> <li> <p>DeepLab (Chen et al., 2014-2018) introduced two key innovations for segmentation:</p> <ul> <li> <p>Atrous (dilated) convolution: standard convolution with gaps inserted between filter elements, controlled by a dilation rate \\(r\\). A 3x3 filter with dilation \\(r\\) has a receptive field of \\((2r + 1) \\times (2r + 1)\\) while using only 9 parameters. This captures context at multiple scales without downsampling, preserving spatial resolution.</p> </li> <li> <p>Atrous Spatial Pyramid Pooling (ASPP): applies multiple atrous convolutions with different dilation rates in parallel (e.g., rates 1, 6, 12, 18), concatenates the results, and fuses with a 1x1 convolution. ASPP captures context at multiple scales simultaneously, similar in spirit to the Inception module (file 02) but using dilation instead of different kernel sizes.</p> </li> </ul> </li> <li> <p>DeepLab also used a Conditional Random Field (CRF) (chapter 05) as a post-processing step to refine segmentation boundaries by encouraging spatially nearby pixels with similar colours to share the same label.</p> </li> <li> <p>Instance segmentation combines detection and segmentation: it identifies each individual object instance and produces a pixel-level mask for each. Two cars in a scene get two separate masks, not just \"car\" for both.</p> </li> <li> <p>Mask R-CNN (He et al., 2017) extends Faster R-CNN by adding a small segmentation head that predicts a binary mask for each detected object. The architecture is Faster R-CNN + a mask branch: the mask branch takes the RoI-pooled features and outputs a \\(m \\times m\\) binary mask per class. It uses RoIAlign instead of RoI pooling: bilinear interpolation at precisely sampled points rather than quantised grid cells, which avoids the spatial misalignment that quantisation causes. This small change significantly improves mask quality.</p> </li> <li> <p>Mask R-CNN is trained with a multi-task loss: classification loss + box regression loss + mask loss (per-pixel binary cross-entropy). The mask branch predicts a mask for every class independently; only the mask corresponding to the predicted class is used, which decouples mask prediction from classification and improves both.</p> </li> <li> <p>Panoptic segmentation unifies semantic and instance segmentation into a single task. Every pixel gets both a class label (semantic) and an instance ID (instance, for \"thing\" classes like cars and people). \"Stuff\" classes (sky, road, grass) get only semantic labels because they are amorphous regions without countable instances.</p> </li> <li> <p>The panoptic quality (PQ) metric evaluates this by decomposing into a segmentation quality (average IoU of matched segments) and a recognition quality (F1 score of matched segments):</p> </li> </ul> \\[\\text{PQ} = \\underbrace{\\frac{\\sum_{(p,g) \\in \\text{TP}} \\text{IoU}(p,g)}{|\\text{TP}|}}_{\\text{SQ}} \\times \\underbrace{\\frac{|\\text{TP}|}{|\\text{TP}| + \\frac{1}{2}|\\text{FP}| + \\frac{1}{2}|\\text{FN}|}}_{\\text{RQ}}\\] <ul> <li> <p>Real-time segmentation is critical for applications like autonomous driving and augmented reality, where latency budgets are tight (often under 30 milliseconds per frame).</p> </li> <li> <p>BiSeNet (Bilateral Segmentation Network, Yu et al., 2018) uses two parallel paths: a spatial path with wide, shallow layers that preserves spatial detail, and a context path with deep, narrow layers that captures semantics. The outputs are fused, giving both speed and accuracy.</p> </li> <li> <p>DDRNet (Deep Dual-Resolution Network, Hong et al., 2021) maintains two branches at different resolutions throughout the network, with repeated information exchange between them. The high-resolution branch preserves spatial detail while the low-resolution branch captures global context. Multiple bilateral fusion modules merge information in both directions.</p> </li> <li> <p>The general trend in real-time segmentation is to avoid the heavy encoder-decoder pattern and instead maintain sufficient spatial resolution throughout the network, trading some accuracy for dramatically lower latency.</p> </li> </ul>"},{"location":"chapter%2008%3A%20computer%20vision/03.%20object%20detection%20and%20segmentation/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement IoU computation and Non-Maximum Suppression from scratch. Apply NMS to a set of overlapping bounding boxes and visualise the result. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef compute_iou(box1, box2):\n    \"\"\"Compute IoU between two boxes [x1, y1, x2, y2].\"\"\"\n    x1 = jnp.maximum(box1[0], box2[0])\n    y1 = jnp.maximum(box1[1], box2[1])\n    x2 = jnp.minimum(box1[2], box2[2])\n    y2 = jnp.minimum(box1[3], box2[3])\n\n    intersection = jnp.maximum(0, x2 - x1) * jnp.maximum(0, y2 - y1)\n    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n    union = area1 + area2 - intersection\n\n    return intersection / (union + 1e-6)\n\ndef nms(boxes, scores, iou_threshold=0.5):\n    \"\"\"Non-Maximum Suppression.\"\"\"\n    order = jnp.argsort(-scores)  # sort by descending confidence\n    keep = []\n\n    remaining = list(range(len(scores)))\n    order_list = order.tolist()\n\n    while order_list:\n        idx = order_list[0]\n        keep.append(idx)\n        order_list = order_list[1:]\n\n        new_order = []\n        for j in order_list:\n            iou = compute_iou(boxes[idx], boxes[j])\n            if iou &lt; iou_threshold:\n                new_order.append(j)\n        order_list = new_order\n\n    return keep\n\n# Example: overlapping detections of the same object\nboxes = jnp.array([\n    [50, 60, 150, 160],   # high confidence\n    [55, 65, 155, 165],   # overlapping duplicate\n    [52, 58, 148, 158],   # overlapping duplicate\n    [200, 100, 300, 200], # different object\n    [205, 105, 305, 205], # overlapping duplicate\n])\nscores = jnp.array([0.95, 0.80, 0.70, 0.90, 0.60])\n\nkeep = nms(boxes, scores, iou_threshold=0.5)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\ncolors = ['#3498db', '#e74c3c', '#27ae60', '#9b59b6', '#f39c12']\n\nfor ax, title, indices in zip(axes, ['Before NMS', 'After NMS'],\n                               [range(len(boxes)), keep]):\n    ax.set_xlim(0, 400); ax.set_ylim(0, 300)\n    ax.set_aspect('equal'); ax.invert_yaxis()\n    ax.set_title(title)\n    for i in indices:\n        b = boxes[i]\n        rect = patches.Rectangle((b[0], b[1]), b[2]-b[0], b[3]-b[1],\n                                  linewidth=2, edgecolor=colors[i],\n                                  facecolor='none')\n        ax.add_patch(rect)\n        ax.text(b[0], b[1]-5, f'{scores[i]:.2f}', color=colors[i], fontsize=10)\n\nplt.tight_layout(); plt.show()\nprint(f\"Kept {len(keep)} of {len(boxes)} boxes after NMS\")\n</code></pre></p> </li> <li> <p>Implement a simplified Region Proposal Network (RPN). Given a feature map, generate anchor boxes at multiple scales and aspect ratios, and predict objectness scores and box offsets. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\ndef generate_anchors(feature_h, feature_w, stride, scales, ratios):\n    \"\"\"Generate anchor boxes for each position on the feature map.\"\"\"\n    anchors = []\n    for y in range(feature_h):\n        for x in range(feature_w):\n            cx = (x + 0.5) * stride\n            cy = (y + 0.5) * stride\n            for s in scales:\n                for r in ratios:\n                    w = s * jnp.sqrt(r)\n                    h = s / jnp.sqrt(r)\n                    anchors.append([cx - w/2, cy - h/2, cx + w/2, cy + h/2])\n    return jnp.array(anchors)\n\ndef rpn_forward(feature_map, params):\n    \"\"\"Simplified RPN: predicts objectness and box offsets per anchor.\"\"\"\n    H, W, C = feature_map.shape\n    n_anchors = params['cls_w'].shape[1]\n\n    # Slide a 1x1 conv over the feature map (simplified)\n    cls_scores = feature_map.reshape(-1, C) @ params['cls_w']  # (H*W, n_anchors)\n    box_offsets = feature_map.reshape(-1, C) @ params['reg_w']  # (H*W, n_anchors*4)\n\n    cls_scores = jax.nn.sigmoid(cls_scores)\n    return cls_scores.ravel(), box_offsets.reshape(-1, 4)\n\n# Setup\nfeature_h, feature_w, channels = 4, 4, 16\nstride = 16  # each feature map cell covers 16x16 pixels\nscales = [32, 64, 128]\nratios = [0.5, 1.0, 2.0]\nn_anchors_per_pos = len(scales) * len(ratios)\n\nkey = jax.random.PRNGKey(42)\nk1, k2, k3 = jax.random.split(key, 3)\n\nfeature_map = jax.random.normal(k1, (feature_h, feature_w, channels))\nparams = {\n    'cls_w': jax.random.normal(k2, (channels, n_anchors_per_pos)) * 0.01,\n    'reg_w': jax.random.normal(k3, (channels, n_anchors_per_pos * 4)) * 0.01,\n}\n\nanchors = generate_anchors(feature_h, feature_w, stride, scales, ratios)\nscores, offsets = rpn_forward(feature_map, params)\n\nprint(f\"Feature map: {feature_h}x{feature_w}, stride={stride}\")\nprint(f\"Anchors per position: {n_anchors_per_pos}\")\nprint(f\"Total anchors: {len(anchors)}\")\nprint(f\"Objectness scores shape: {scores.shape}\")\nprint(f\"Box offsets shape: {offsets.shape}\")\n\n# Visualise anchors for one position\nfig, ax = plt.subplots(figsize=(6, 6))\nimg_size = feature_h * stride\nax.set_xlim(0, img_size); ax.set_ylim(0, img_size)\nax.invert_yaxis(); ax.set_aspect('equal')\n\npos_idx = feature_h // 2 * feature_w + feature_w // 2  # centre position\ncolors = ['#3498db', '#e74c3c', '#27ae60']\nfor i, s in enumerate(scales):\n    for j, r in enumerate(ratios):\n        idx = pos_idx * n_anchors_per_pos + i * len(ratios) + j\n        a = anchors[idx]\n        rect = patches.Rectangle((a[0], a[1]), a[2]-a[0], a[3]-a[1],\n                                  linewidth=1.5, edgecolor=colors[i],\n                                  facecolor='none', linestyle=['--', '-', ':'][j])\n        ax.add_patch(rect)\n\nax.scatter([img_size/2], [img_size/2], c='red', s=50, zorder=5)\nax.set_title(f'Anchors at centre position\\n3 scales \u00d7 3 ratios = {n_anchors_per_pos}')\nax.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Implement a simplified U-Net encoder-decoder with skip connections for 1D segmentation (binary labelling of a 1D signal). <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef conv1d_same(x, kernel):\n    \"\"\"1D convolution with same padding.\"\"\"\n    k = len(kernel)\n    pad = k // 2\n    x_pad = jnp.pad(x, pad, mode='edge')\n    n = len(x)\n    out = jnp.zeros(n)\n    for i in range(n):\n        out = out.at[i].set(jnp.sum(x_pad[i:i+k] * kernel))\n    return out\n\ndef downsample(x):\n    return x[::2]\n\ndef upsample(x, target_len):\n    return jnp.interp(jnp.linspace(0, 1, target_len), jnp.linspace(0, 1, len(x)), x)\n\ndef unet_1d(x, params):\n    \"\"\"Simplified 1D U-Net with 2 encoder/decoder levels.\"\"\"\n    # Encoder\n    e1 = jnp.maximum(0, conv1d_same(x, params['enc1']))\n    e1_down = downsample(e1)\n\n    e2 = jnp.maximum(0, conv1d_same(e1_down, params['enc2']))\n    e2_down = downsample(e2)\n\n    # Bottleneck\n    bottleneck = jnp.maximum(0, conv1d_same(e2_down, params['bottleneck']))\n\n    # Decoder with skip connections\n    d2_up = upsample(bottleneck, len(e2))\n    d2 = jnp.maximum(0, conv1d_same(d2_up + e2, params['dec2']))  # skip connection\n\n    d1_up = upsample(d2, len(e1))\n    d1 = conv1d_same(d1_up + e1, params['dec1'])  # skip connection\n\n    return jax.nn.sigmoid(d1)\n\n# Create signal with labelled regions\nn = 128\nt = jnp.linspace(0, 4 * jnp.pi, n)\nsignal = jnp.sin(t) + 0.5 * jnp.sin(3 * t)\nlabels = (signal &gt; 0.5).astype(jnp.float32)  # binary segmentation target\n\nkey = jax.random.PRNGKey(42)\nkeys = jax.random.split(key, 5)\nparams = {\n    'enc1': jax.random.normal(keys[0], (5,)) * 0.3,\n    'enc2': jax.random.normal(keys[1], (5,)) * 0.3,\n    'bottleneck': jax.random.normal(keys[2], (3,)) * 0.3,\n    'dec2': jax.random.normal(keys[3], (5,)) * 0.3,\n    'dec1': jax.random.normal(keys[4], (5,)) * 0.3,\n}\n\ndef loss_fn(params, signal, labels):\n    pred = unet_1d(signal, params)\n    return -jnp.mean(labels * jnp.log(pred + 1e-7) + (1 - labels) * jnp.log(1 - pred + 1e-7))\n\ngrad_fn = jax.jit(jax.grad(loss_fn))\nlr = 0.05\n\nfor step in range(500):\n    grads = grad_fn(params, signal, labels)\n    params = {k: params[k] - lr * grads[k] for k in params}\n\npred = unet_1d(signal, params)\n\nfig, axes = plt.subplots(3, 1, figsize=(12, 7), sharex=True)\naxes[0].plot(t, signal, color='#3498db', linewidth=1.5)\naxes[0].set_title('Input Signal'); axes[0].set_ylabel('Value')\n\naxes[1].fill_between(t, 0, labels, alpha=0.3, color='#27ae60')\naxes[1].set_title('Ground Truth Labels'); axes[1].set_ylabel('Label')\n\naxes[2].plot(t, pred, color='#e74c3c', linewidth=1.5)\naxes[2].fill_between(t, 0, (pred &gt; 0.5).astype(float), alpha=0.2, color='#e74c3c')\naxes[2].set_title('U-Net Prediction'); axes[2].set_ylabel('Probability')\naxes[2].set_xlabel('t')\n\nplt.tight_layout(); plt.show()\nprint(f\"Final loss: {loss_fn(params, signal, labels):.4f}\")\nprint(f\"Pixel accuracy: {jnp.mean((pred &gt; 0.5) == labels):.2%}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2008%3A%20computer%20vision/04.%20vision%20transformers%20and%20generation/","title":"Vision Transformers and Generation","text":"<p>Vision transformers apply self-attention to image patches, challenging CNN dominance with data-driven spatial learning. This file covers ViT, DeiT, Swin Transformer, image generation with GANs (StyleGAN), VAEs, and diffusion models (DDPM, Stable Diffusion), plus super-resolution and neural style transfer.</p> <ul> <li> <p>CNNs (file 02) build in strong spatial inductive biases: local connectivity, weight sharing, and translation equivariance. Vision Transformers (ViTs) ask a provocative question: what if we drop these biases entirely and let the model learn spatial structure from data, using only the attention mechanism from chapter 06?</p> </li> <li> <p>The Vision Transformer (ViT) (Dosovitskiy et al., 2021) applies the standard Transformer encoder directly to images. The key idea is to treat an image as a sequence of patches, just as NLP treats text as a sequence of tokens.</p> </li> <li> <p>The process works as follows:</p> <ol> <li>Split the image (height \\(H\\), width \\(W\\), channels \\(C\\)) into a grid of non-overlapping patches of size \\(P \\times P\\). This produces \\(N = HW / P^2\\) patches.</li> <li>Flatten each patch into a vector of length \\(P^2 \\cdot C\\) and project it to the model dimension \\(D\\) via a learned linear embedding (a single matrix multiply, chapter 02).</li> <li>Prepend a learnable [CLS] token embedding (analogous to BERT's [CLS], chapter 07). This token attends to all patches and its final representation is used for classification.</li> <li>Add position embeddings (one learnable vector per position) to provide spatial information, since attention is permutation-equivariant.</li> <li>Pass the sequence of \\((N + 1)\\) token embeddings through a standard Transformer encoder (multi-head self-attention + FFN, chapter 06).</li> <li>The [CLS] token's final representation is passed through a classification head (a small MLP).</li> </ol> </li> </ul> <p></p> <ul> <li> <p>The patch embedding is equivalent to a convolution with kernel size \\(P\\) and stride \\(P\\) (non-overlapping). ViT literally converts the 2D image into a 1D sequence, then processes it with the same architecture used for language.</p> </li> <li> <p>ViT has less inductive bias than CNNs: it does not enforce local connectivity or translation equivariance. This means it needs more training data to learn spatial structure from scratch. On small datasets, CNNs outperform ViT. But when trained on very large datasets (JFT-300M, 300 million images), ViT matches or exceeds the best CNNs, suggesting that the inductive biases of CNNs are helpful for data efficiency but not necessary for ultimate performance.</p> </li> <li> <p>ViT self-attention is \\(O(N^2)\\) in the number of patches. For a 224x224 image with 16x16 patches, \\(N = 196\\), which is manageable. But for higher-resolution images or smaller patches, the quadratic cost becomes prohibitive.</p> </li> <li> <p>DeiT (Data-efficient Image Transformer, Touvron et al., 2021) showed that ViT can be trained effectively on ImageNet alone (without the massive JFT dataset) using strong data augmentation, regularisation (stochastic depth, label smoothing, dropout), and knowledge distillation: a pre-trained CNN teacher provides soft labels that the ViT student learns to match. DeiT added a distillation token alongside the [CLS] token, trained to predict the teacher's output.</p> </li> <li> <p>Swin Transformer (Liu et al., 2021) addresses ViT's two main limitations: its quadratic cost with image size and its lack of hierarchical feature maps (which detection and segmentation need).</p> </li> <li> <p>Swin introduces shifted windows: instead of global self-attention over all patches, attention is computed within local windows (e.g., 7x7 patches). This makes the cost linear in image size: \\(O(N)\\) rather than \\(O(N^2)\\). But local windows alone would prevent information flow between regions.</p> </li> <li> <p>Window shifting solves this: in alternating layers, the window partition is shifted by half the window size. This creates cross-window connections, allowing information to flow between all parts of the image across layers without the cost of global attention.</p> </li> </ul> <p></p> <ul> <li> <p>Swin also builds a hierarchical representation by merging patches across stages. After each stage, neighbouring 2x2 patches are concatenated and projected to double the channel dimension and halve the spatial resolution. This produces multi-scale feature maps analogous to those in CNNs and FPN (file 03), making Swin directly compatible with detection heads like Faster R-CNN and segmentation heads like U-Net.</p> </li> <li> <p>PVT (Pyramid Vision Transformer) takes a similar hierarchical approach with spatial-reduction attention: at each stage, the keys and values are spatially downsampled before computing attention, reducing the quadratic cost while maintaining the global receptive field.</p> </li> <li> <p>Self-supervised visual learning trains representations from unlabelled images. Labels are expensive to collect, but images are abundant. The goal is to learn features that transfer well to downstream tasks without any human annotation.</p> </li> <li> <p>Contrastive learning trains the model to recognise that two augmented views of the same image (a \"positive pair\") should have similar representations, while views of different images (\"negative pairs\") should have dissimilar representations.</p> </li> <li> <p>SimCLR (Chen et al., 2020) creates two augmented views of each image in a batch, encodes both with a shared backbone + projection head, and applies the NT-Xent loss (normalised temperature-scaled cross-entropy):</p> </li> </ul> \\[\\ell_{i,j} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j) / \\tau)}{\\sum_{k \\neq i} \\exp(\\text{sim}(z_i, z_k) / \\tau)}\\] <ul> <li> <p>where \\(\\text{sim}\\) is cosine similarity (chapter 01) and \\(\\tau\\) is a temperature parameter. The numerator pushes positive pairs together; the denominator pushes negative pairs apart. SimCLR requires large batch sizes (4,096+) to provide enough negatives.</p> </li> <li> <p>MoCo (Momentum Contrast, He et al., 2020) solves the large-batch requirement by maintaining a momentum-updated queue of negative embeddings. The query encoder is updated by gradient descent; the key encoder is updated as an exponential moving average (EMA, chapter 04) of the query encoder: \\(\\theta_k \\leftarrow m \\theta_k + (1 - m) \\theta_q\\), with \\(m = 0.999\\). The queue stores recent key embeddings, providing a large and consistent set of negatives without needing huge batches.</p> </li> <li> <p>BYOL (Bootstrap Your Own Latent, Grill et al., 2020) eliminates negative pairs entirely. It uses two networks: an \"online\" network and a \"target\" network (EMA of the online). The online network predicts the target network's representation of a different augmented view. Without negatives, BYOL avoids the collapse problem (where the model outputs the same vector for everything) through the asymmetry of the predictor head and the EMA target.</p> </li> <li> <p>DINO (Self-Distillation with No Labels, Caron et al., 2021) applies self-distillation to ViT. A student network predicts the output of a teacher network (EMA of the student) across different augmented views. The teacher uses larger crops; the student uses smaller crops. DINO produces features that contain explicit information about the scene layout: the self-attention maps of DINO-trained ViTs naturally segment objects without any segmentation supervision.</p> </li> <li> <p>Masked image modelling is the visual analogue of BERT's masked language modelling (chapter 07). A large fraction of input patches is masked, and the model learns to reconstruct them.</p> </li> <li> <p>MAE (Masked Autoencoders, He et al., 2022) masks 75% of patches and trains a ViT encoder-decoder to reconstruct the missing pixel values. Only the unmasked patches are processed by the encoder (saving 4x computation during pre-training), and the lightweight decoder reconstructs the full image from the encoded visible patches plus learnable mask tokens.</p> </li> <li> <p>BEiT (BERT Pre-training of Image Transformers, Bao et al., 2022) masks patches and predicts discrete visual tokens (obtained from a pre-trained dVAE tokeniser) rather than raw pixels. This parallels BERT's prediction of discrete word tokens and avoids the low-level detail of pixel reconstruction.</p> </li> <li> <p>Image generation aims to produce new, realistic images that do not exist in the training set. The core challenge is modelling the high-dimensional probability distribution of natural images.</p> </li> <li> <p>Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) use two competing networks: a generator \\(G\\) that creates fake images from random noise, and a discriminator \\(D\\) that tries to distinguish real images from fakes. They are trained adversarially: \\(G\\) tries to fool \\(D\\), and \\(D\\) tries to catch \\(G\\).</p> </li> </ul> \\[\\min_G \\max_D \\; \\mathbb{E}_{x \\sim p_{\\text{data}}}[\\log D(x)] + \\mathbb{E}_{z \\sim p(z)}[\\log(1 - D(G(z)))]\\] <ul> <li> <p>The generator takes a random latent vector \\(z\\) (sampled from a simple distribution like a Gaussian) and maps it through a series of transposed convolutions to produce an image. The discriminator is a standard CNN classifier. At equilibrium, \\(G\\) produces images indistinguishable from real data, and \\(D\\) outputs 0.5 for all inputs.</p> </li> <li> <p>Mode collapse is the main failure mode of GANs: the generator learns to produce only a few types of images that fool the discriminator, ignoring the diversity of the training data. The generator finds a small set of \"safe\" outputs rather than covering the full distribution.</p> </li> <li> <p>Training tricks that stabilise GANs include: spectral normalisation (constraining the Lipschitz constant of the discriminator), progressive growing (training at low resolution first, then gradually increasing), feature matching (matching the statistics of intermediate discriminator features rather than the final output), and using Wasserstein distance instead of the original JS divergence objective.</p> </li> <li> <p>StyleGAN (Karras et al., 2019) is the most influential GAN architecture for high-quality image synthesis. Its key innovation is the style-based generator: instead of feeding the latent vector \\(z\\) directly into the generator, it is first mapped through a mapping network (an 8-layer MLP) to produce a style vector \\(w\\). This style vector is injected into each layer of the generator via adaptive instance normalisation (AdaIN), which modulates the feature map statistics:</p> </li> </ul> \\[\\text{AdaIN}(x, y) = y_{s} \\cdot \\frac{x - \\mu(x)}{\\sigma(x)} + y_{b}\\] <ul> <li> <p>where \\(y_s\\) and \\(y_b\\) are the scale and bias derived from \\(w\\). Different layers control different aspects: early layers control coarse features (pose, face shape), middle layers control medium features (hair style, eyes), and late layers control fine details (freckles, hair texture). StyleGAN can generate photorealistic faces at 1024x1024 resolution.</p> </li> <li> <p>Variational Autoencoders (VAEs) (chapter 06) provide an alternative generative approach. Unlike GANs, VAEs have a principled probabilistic framework with a clear training objective (ELBO). They tend to produce blurrier images than GANs but offer a smoother, more structured latent space. VAEs are the encoder-decoder pair used in latent diffusion models for compressing images to and from latent space.</p> </li> <li> <p>Diffusion models have become the dominant paradigm for image generation, surpassing GANs in both quality and diversity. The idea is conceptually simple: gradually add noise to data until it becomes pure Gaussian noise (the forward process), then learn to reverse this process step by step (the reverse process).</p> </li> <li> <p>The forward process adds Gaussian noise over \\(T\\) timesteps:</p> </li> </ul> \\[q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1 - \\beta_t} \\, x_{t-1}, \\beta_t I)\\] <ul> <li>where \\(\\beta_t\\) is a noise schedule that increases over time. After enough steps, \\(x_T\\) is approximately pure Gaussian noise regardless of the original image \\(x_0\\). Using the reparametrisation trick (chapter 06) and setting \\(\\alpha_t = 1 - \\beta_t\\), \\(\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s\\), we can sample \\(x_t\\) directly from \\(x_0\\):</li> </ul> \\[x_t = \\sqrt{\\bar{\\alpha}_t} \\, x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\, \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I)\\] <ul> <li>The reverse process learns to denoise: starting from pure noise \\(x_T\\), the model predicts the noise \\(\\epsilon\\) added at each step and subtracts it to recover \\(x_{t-1}\\). This is parametrised by a neural network \\(\\epsilon_\\theta\\) (typically a U-Net, from file 03), trained with a simple MSE loss:</li> </ul> \\[\\mathcal{L} = \\mathbb{E}_{t, x_0, \\epsilon}\\left[\\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2\\right]\\] <p></p> <ul> <li> <p>DDPM (Denoising Diffusion Probabilistic Models, Ho et al., 2020) established this framework. Sampling requires iterating through all \\(T\\) steps (typically 1,000), which is slow. DDIM (Denoising Diffusion Implicit Models, Song et al., 2021) reformulates the sampling process as a deterministic mapping, allowing large step skips (e.g., 50 steps instead of 1,000) with minimal quality loss.</p> </li> <li> <p>Score-based models (Song and Ermon, 2019) provide an alternative perspective. Instead of predicting the noise \\(\\epsilon\\), the model estimates the score function \\(\\nabla_{x_t} \\log p(x_t)\\), the gradient of the log-probability with respect to the noisy image. This gradient points toward higher-probability (cleaner) regions of the data distribution. Sampling follows this gradient using Langevin dynamics. Score-based models and DDPM were unified in the framework of stochastic differential equations (SDEs): the forward process is an SDE that adds noise, and the reverse process is the time-reversed SDE.</p> </li> <li> <p>Classifier-free guidance (Ho and Salimans, 2022) controls the tradeoff between sample quality and diversity. The model is trained both conditionally (with a text prompt or class label) and unconditionally (with the condition dropped randomly). At sampling time, the prediction is a weighted combination:</p> </li> </ul> \\[\\hat{\\epsilon} = \\epsilon_\\theta(x_t, \\varnothing) + s \\cdot (\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\varnothing))\\] <ul> <li> <p>where \\(c\\) is the condition, \\(\\varnothing\\) is the null condition, and \\(s &gt; 1\\) is the guidance scale. Higher \\(s\\) produces images that match the condition more strongly but with less diversity. \\(s = 1\\) gives the unguided model; \\(s = 7.5\\) is a common default.</p> </li> <li> <p>Latent diffusion (Rombach et al., 2022; Stable Diffusion) moves the diffusion process from pixel space to a learned latent space. A pre-trained VAE encoder compresses images to a lower-dimensional latent representation (typically 4x or 8x spatial downsampling), diffusion operates in this compressed space, and the VAE decoder reconstructs pixels from the denoised latent. This is dramatically more efficient: diffusing a 512x512 image in pixel space means processing a \\(512 \\times 512 \\times 3\\) tensor, but in latent space only a \\(64 \\times 64 \\times 4\\) tensor.</p> </li> <li> <p>The denoising U-Net in latent diffusion receives the noisy latent, the timestep (encoded as a sinusoidal embedding, analogous to positional encoding in transformers), and a conditioning signal (text embedding from a frozen CLIP or T5 text encoder). The text condition enters via cross-attention layers within the U-Net: the text embeddings serve as keys and values, and the image features serve as queries. This lets the model attend to relevant parts of the text prompt at each spatial location.</p> </li> <li> <p>Flow matching is an emerging alternative to diffusion that learns a direct transport path between noise and data, rather than the iterative denoising of DDPM.</p> </li> <li> <p>A continuous normalising flow (CNF) defines a time-dependent velocity field \\(v_\\theta(x, t)\\) that pushes samples from a simple distribution \\(p_0\\) (noise) to the data distribution \\(p_1\\) along smooth trajectories. The transformation follows an ordinary differential equation (ODE):</p> </li> </ul> \\[\\frac{dx}{dt} = v_\\theta(x, t), \\quad t \\in [0, 1]\\] <ul> <li> <p>Starting from \\(x_0 \\sim \\mathcal{N}(0, I)\\), integrating the ODE forward to \\(t = 1\\) produces a sample from the data distribution. The velocity field is parametrised by a neural network and trained to match a target conditional flow.</p> </li> <li> <p>Optimal transport (OT) flow matching (Lipman et al., 2023) uses straight-line paths between noise and data as the target flow: the conditional path from noise sample \\(x_0\\) to data sample \\(x_1\\) is simply \\(x_t = (1 - t) x_0 + t x_1\\), and the target velocity is \\(v = x_1 - x_0\\). The training loss becomes:</p> </li> </ul> \\[\\mathcal{L} = \\mathbb{E}_{t, x_0, x_1} \\left[\\|v_\\theta(x_t, t) - (x_1 - x_0)\\|^2\\right]\\] <ul> <li> <p>Rectified flows (Liu et al., 2022) iteratively straighten the learned flow paths. After an initial training pass, the model is used to generate (noise, data) pairs by simulating the ODE. These pairs, which are more closely aligned than random pairings, are used to retrain the model. Repeating this process produces increasingly straight paths, which can be traversed in fewer ODE steps (even a single step), enabling extremely fast generation.</p> </li> <li> <p>Flow matching has several advantages over diffusion: the training objective is simpler (direct velocity regression, no noise schedule), the sampling ODE is smoother (requiring fewer integration steps), and the connection to optimal transport provides theoretical grounding. Stable Diffusion 3 and Flux use flow matching instead of traditional DDPM.</p> </li> </ul>"},{"location":"chapter%2008%3A%20computer%20vision/04.%20vision%20transformers%20and%20generation/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement the ViT patch embedding from scratch. Split an image into patches, flatten them, project to the model dimension, add position embeddings, and prepend a [CLS] token. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef create_patch_embedding(image, patch_size, d_model, params):\n    \"\"\"Convert an image into a sequence of patch embeddings.\"\"\"\n    H, W, C = image.shape\n    n_patches_h = H // patch_size\n    n_patches_w = W // patch_size\n    n_patches = n_patches_h * n_patches_w\n\n    # Extract patches\n    patches = []\n    for i in range(n_patches_h):\n        for j in range(n_patches_w):\n            patch = image[i*patch_size:(i+1)*patch_size,\n                          j*patch_size:(j+1)*patch_size, :]\n            patches.append(patch.ravel())\n    patches = jnp.stack(patches)  # (N, P*P*C)\n\n    # Linear projection to d_model\n    embeddings = patches @ params['proj_w'] + params['proj_b']  # (N, d_model)\n\n    # Prepend CLS token\n    cls_token = params['cls_token']  # (1, d_model)\n    embeddings = jnp.concatenate([cls_token, embeddings], axis=0)  # (N+1, d_model)\n\n    # Add position embeddings\n    embeddings = embeddings + params['pos_embed']  # (N+1, d_model)\n\n    return embeddings, patches\n\n# Setup\nH, W, C = 32, 32, 3\npatch_size = 8\nd_model = 64\nn_patches = (H // patch_size) * (W // patch_size)  # 16\n\nkey = jax.random.PRNGKey(42)\nkeys = jax.random.split(key, 5)\n\n# Create a synthetic image with distinct quadrants\nimage = jnp.zeros((H, W, C))\nimage = image.at[:16, :16, 0].set(1.0)   # red top-left\nimage = image.at[:16, 16:, 1].set(1.0)   # green top-right\nimage = image.at[16:, :16, 2].set(1.0)   # blue bottom-left\nimage = image.at[16:, 16:, :2].set(1.0)  # yellow bottom-right\n\nparams = {\n    'proj_w': jax.random.normal(keys[0], (patch_size**2 * C, d_model)) * 0.02,\n    'proj_b': jnp.zeros(d_model),\n    'cls_token': jax.random.normal(keys[1], (1, d_model)) * 0.02,\n    'pos_embed': jax.random.normal(keys[2], (n_patches + 1, d_model)) * 0.02,\n}\n\nembeddings, patches = create_patch_embedding(image, patch_size, d_model, params)\n\nprint(f\"Image shape: {image.shape}\")\nprint(f\"Patch size: {patch_size}x{patch_size}\")\nprint(f\"Number of patches: {n_patches}\")\nprint(f\"Patch vector length: {patch_size**2 * C}\")\nprint(f\"Embedding shape: {embeddings.shape}  (CLS + {n_patches} patches)\")\n\n# Visualise patches\nfig, axes = plt.subplots(2, 5, figsize=(14, 6))\naxes[0, 0].imshow(image); axes[0, 0].set_title('Full Image'); axes[0, 0].axis('off')\nfor idx in range(min(9, n_patches)):\n    ax = axes[(idx+1) // 5, (idx+1) % 5]\n    patch_img = patches[idx].reshape(patch_size, patch_size, C)\n    ax.imshow(patch_img); ax.set_title(f'Patch {idx}'); ax.axis('off')\nplt.suptitle('ViT Patch Decomposition')\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Implement a simple GAN training loop. Train a generator and discriminator on 2D data and visualise the generated distribution converging to the real distribution. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef generator(z, params):\n    h = jnp.tanh(z @ params['g_w1'] + params['g_b1'])\n    h = jnp.tanh(h @ params['g_w2'] + params['g_b2'])\n    return h @ params['g_w3'] + params['g_b3']\n\ndef discriminator(x, params):\n    h = jax.nn.leaky_relu(x @ params['d_w1'] + params['d_b1'], 0.2)\n    h = jax.nn.leaky_relu(h @ params['d_w2'] + params['d_b2'], 0.2)\n    return jax.nn.sigmoid(h @ params['d_w3'] + params['d_b3'])\n\ndef init_params(key):\n    keys = jax.random.split(key, 6)\n    z_dim, h_dim, data_dim = 2, 32, 2\n    scale = 0.1\n    return {\n        'g_w1': jax.random.normal(keys[0], (z_dim, h_dim)) * scale,\n        'g_b1': jnp.zeros(h_dim),\n        'g_w2': jax.random.normal(keys[1], (h_dim, h_dim)) * scale,\n        'g_b2': jnp.zeros(h_dim),\n        'g_w3': jax.random.normal(keys[2], (h_dim, data_dim)) * scale,\n        'g_b3': jnp.zeros(data_dim),\n        'd_w1': jax.random.normal(keys[3], (data_dim, h_dim)) * scale,\n        'd_b1': jnp.zeros(h_dim),\n        'd_w2': jax.random.normal(keys[4], (h_dim, h_dim)) * scale,\n        'd_b2': jnp.zeros(h_dim),\n        'd_w3': jax.random.normal(keys[5], (h_dim, 1)) * scale,\n        'd_b3': jnp.zeros(1),\n    }\n\ndef d_loss(params, real_data, fake_data):\n    real_score = discriminator(real_data, params)\n    fake_score = discriminator(fake_data, params)\n    return -jnp.mean(jnp.log(real_score + 1e-7) + jnp.log(1 - fake_score + 1e-7))\n\ndef g_loss(params, fake_data):\n    fake_score = discriminator(fake_data, params)\n    return -jnp.mean(jnp.log(fake_score + 1e-7))\n\n# Real data: ring distribution\nkey = jax.random.PRNGKey(42)\ntheta = jax.random.uniform(key, (512,)) * 2 * jnp.pi\nreal_data = jnp.stack([jnp.cos(theta), jnp.sin(theta)], axis=1)\nreal_data = real_data + jax.random.normal(key, real_data.shape) * 0.05\n\nparams = init_params(jax.random.PRNGKey(0))\nd_grad = jax.grad(d_loss)\ng_grad = jax.grad(g_loss)\nlr = 0.001\n\nsnapshots = []\nfor step in range(3000):\n    key, k1 = jax.random.split(key)\n    z = jax.random.normal(k1, (512, 2))\n    fake_data = generator(z, params)\n\n    # Update discriminator\n    grads = d_grad(params, real_data, fake_data)\n    for k in ['d_w1', 'd_b1', 'd_w2', 'd_b2', 'd_w3', 'd_b3']:\n        params[k] = params[k] - lr * grads[k]\n\n    # Update generator\n    fake_data = generator(z, params)\n    grads = g_grad(params, fake_data)\n    for k in ['g_w1', 'g_b1', 'g_w2', 'g_b2', 'g_w3', 'g_b3']:\n        params[k] = params[k] - lr * grads[k]\n\n    if step in [0, 500, 1500, 2999]:\n        snapshots.append((step, fake_data.copy()))\n\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nfor ax, (step, fake) in zip(axes, snapshots):\n    ax.scatter(real_data[:, 0], real_data[:, 1], s=5, alpha=0.3, c='#3498db', label='Real')\n    ax.scatter(fake[:, 0], fake[:, 1], s=5, alpha=0.3, c='#e74c3c', label='Generated')\n    ax.set_title(f'Step {step}'); ax.set_xlim(-2, 2); ax.set_ylim(-2, 2)\n    ax.set_aspect('equal'); ax.legend(markerscale=3)\nplt.suptitle('GAN Training: Generator Learns the Ring Distribution')\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Implement the diffusion forward process: add noise to an image at increasing timesteps and visualise the progressive corruption. Then implement a single denoising step. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef noise_schedule(T, beta_start=0.0001, beta_end=0.02):\n    \"\"\"Linear noise schedule.\"\"\"\n    betas = jnp.linspace(beta_start, beta_end, T)\n    alphas = 1.0 - betas\n    alpha_bars = jnp.cumprod(alphas)\n    return betas, alphas, alpha_bars\n\ndef forward_diffusion(x0, t, alpha_bars, key):\n    \"\"\"Add noise to x0 at timestep t.\"\"\"\n    alpha_bar_t = alpha_bars[t]\n    noise = jax.random.normal(key, x0.shape)\n    xt = jnp.sqrt(alpha_bar_t) * x0 + jnp.sqrt(1 - alpha_bar_t) * noise\n    return xt, noise\n\n# Create a simple 2D \"image\" (checkerboard)\nimg = jnp.zeros((32, 32))\nfor i in range(4):\n    for j in range(4):\n        if (i + j) % 2 == 0:\n            img = img.at[i*8:(i+1)*8, j*8:(j+1)*8].set(1.0)\n\nT = 1000\nbetas, alphas, alpha_bars = noise_schedule(T)\n\n# Visualise forward process\ntimesteps = [0, 50, 200, 500, 999]\nkey = jax.random.PRNGKey(42)\n\nfig, axes = plt.subplots(1, len(timesteps), figsize=(16, 3.5))\nfor ax, t in zip(axes, timesteps):\n    key, subkey = jax.random.split(key)\n    xt, noise = forward_diffusion(img, t, alpha_bars, subkey)\n    ax.imshow(xt, cmap='gray', vmin=-2, vmax=2)\n    ax.set_title(f't={t}\\n$\\\\bar{{\\\\alpha}}$={alpha_bars[t]:.3f}')\n    ax.axis('off')\nplt.suptitle('Diffusion Forward Process: Progressive Noise Addition')\nplt.tight_layout(); plt.show()\n\n# Simple denoising: train a tiny network to predict noise at t=200\nt_denoise = 200\nkey, k1 = jax.random.split(key)\nxt, true_noise = forward_diffusion(img, t_denoise, alpha_bars, k1)\n\n# Tiny \"denoiser\": just learn a constant noise estimate (for illustration)\nnoise_estimate = jnp.zeros_like(img)\nlr = 0.01\nfor step in range(100):\n    residual = noise_estimate - true_noise\n    noise_estimate = noise_estimate - lr * residual\n\n# Reverse one step\nalpha_bar_t = alpha_bars[t_denoise]\nx_denoised = (xt - jnp.sqrt(1 - alpha_bar_t) * noise_estimate) / jnp.sqrt(alpha_bar_t)\n\nfig, axes = plt.subplots(1, 3, figsize=(12, 4))\naxes[0].imshow(img, cmap='gray'); axes[0].set_title('Original $x_0$'); axes[0].axis('off')\naxes[1].imshow(xt, cmap='gray', vmin=-2, vmax=2)\naxes[1].set_title(f'Noisy $x_{{200}}$'); axes[1].axis('off')\naxes[2].imshow(x_denoised, cmap='gray')\naxes[2].set_title('Denoised (one step)'); axes[2].axis('off')\nplt.tight_layout(); plt.show()\n\nmse = jnp.mean((x_denoised - img)**2)\nprint(f\"Denoising MSE: {mse:.4f}\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2008%3A%20computer%20vision/05.%20video%20and%203D%20vision/","title":"Video and 3D Vision","text":"<p>Video and 3D vision extend image understanding into the temporal and spatial domains. This file covers optical flow, video classification (3D CNNs, TimeSformer), object tracking (SORT, DeepSORT), action recognition, depth estimation (monocular and stereo), point clouds, NeRFs, and 3D Gaussian splatting.</p> <ul> <li> <p>Files 01-04 treated images as isolated snapshots. But the visual world is continuous: objects move, scenes change, and depth exists. This file extends computer vision into the temporal domain (video) and the spatial domain (3D), covering how models understand motion, track objects, estimate depth, and reconstruct scenes.</p> </li> <li> <p>A video is a sequence of images (frames) captured over time. At 30 frames per second, a 10-second clip contains 300 frames. The key challenge is modelling the temporal dimension: how do objects move, how do scenes evolve, and how can we relate information across frames?</p> </li> <li> <p>Optical flow estimates the apparent motion of pixels between two consecutive frames. For each pixel in frame \\(t\\), optical flow produces a 2D displacement vector \\((u, v)\\) pointing to where that pixel moved in frame \\(t+1\\). The result is a dense motion field the same size as the image.</p> </li> </ul> <p></p> <ul> <li>Optical flow is computed under the brightness constancy assumption: a pixel's intensity does not change as it moves. If a pixel at position \\((x, y)\\) in frame \\(t\\) has intensity \\(I(x, y, t)\\) and moves by \\((u, v)\\) in a small time interval \\(\\delta t\\):</li> </ul> \\[I(x + u\\delta t, \\, y + v\\delta t, \\, t + \\delta t) = I(x, y, t)\\] <ul> <li>Taking a first-order Taylor expansion (chapter 03) and dividing by \\(\\delta t\\):</li> </ul> \\[I_x u + I_y v + I_t = 0\\] <ul> <li> <p>where \\(I_x, I_y\\) are the spatial gradients (Sobel, file 01) and \\(I_t\\) is the temporal gradient (difference between consecutive frames). This is the optical flow constraint equation. One equation, two unknowns (\\(u, v\\)): we need an additional constraint.</p> </li> <li> <p>Lucas-Kanade assumes the flow is constant within a small window (e.g., 5x5 pixels). This gives an overdetermined system (25 equations, 2 unknowns) solved by least squares (the normal equation from chapter 06):</p> </li> </ul> \\[ \\begin{bmatrix} u \\\\ v \\end{bmatrix} = \\begin{bmatrix} \\sum I_x^2 &amp; \\sum I_x I_y \\\\ \\sum I_x I_y &amp; \\sum I_y^2 \\end{bmatrix}^{-1} \\begin{bmatrix} -\\sum I_x I_t \\\\ -\\sum I_y I_t \\end{bmatrix} \\] <ul> <li> <p>The 2x2 matrix is the structure tensor from file 01 (the same matrix used in Harris corner detection). Lucas-Kanade works well for small motions but fails when objects move more than a few pixels between frames.</p> </li> <li> <p>Farneback's method fits a polynomial expansion to the neighbourhood of each pixel and estimates the displacement field that best explains the change between frames. It produces dense flow (a vector for every pixel) and handles larger motions than Lucas-Kanade.</p> </li> <li> <p>Modern deep learning optical flow methods (FlowNet, RAFT) learn to predict optical flow end-to-end from pairs of frames. RAFT (Recurrent All-Pairs Field Transforms, Teed and Deng, 2020) computes a 4D correlation volume between all pairs of pixels in the two frames and iteratively refines the flow estimate using a GRU-based update operator. RAFT achieves state-of-the-art accuracy and has become the standard flow backbone.</p> </li> <li> <p>Two-stream networks (Simonyan and Zisserman, 2014) were an early approach to video understanding. One stream processes a single RGB frame (appearance), the other processes a stack of optical flow frames (motion). The two streams are fused at the end (by averaging or concatenation). This architecture explicitly separates \"what things look like\" from \"how they move.\"</p> </li> <li> <p>3D Convolutional Networks extend 2D convolutions to the temporal dimension. A 3D convolution applies a filter of size \\(k \\times k \\times k_t\\) that spans both spatial and temporal dimensions, directly learning spatiotemporal features.</p> </li> <li> <p>C3D (Tran et al., 2015) stacked 3D convolutions with 3x3x3 filters, showing that temporal convolutions can learn motion features without explicit optical flow. The cost is high: 3D convolutions have \\(k_t\\) times more parameters and computation than their 2D counterparts.</p> </li> <li> <p>I3D (Inflated 3D, Carreira and Zisserman, 2017) took a more practical approach: start with a pre-trained 2D CNN (like Inception or ResNet) and \"inflate\" all 2D filters to 3D by repeating the weights along the temporal dimension and dividing by \\(k_t\\). This transfers ImageNet pre-training to video while adding temporal modelling. A 2D \\(k \\times k\\) filter becomes a \\(k \\times k \\times k_t\\) filter initialised as \\(W_{\\text{3D}}[:,:,j] = W_{\\text{2D}} / k_t\\) for all temporal positions \\(j\\).</p> </li> <li> <p>SlowFast Networks (Feichtenhofer et al., 2019) use two parallel pathways operating at different temporal resolutions:</p> <ul> <li>The Slow pathway processes frames at a low frame rate (e.g., every 16th frame) with high spatial resolution and many channels, capturing fine spatial detail.</li> <li>The Fast pathway processes frames at a high frame rate (every 2nd frame) with reduced spatial resolution and fewer channels (typically \\(1/8\\) of the Slow pathway), capturing rapid temporal changes.</li> <li>Lateral connections fuse information from Fast to Slow via strided convolutions.</li> </ul> </li> <li> <p>The insight is that spatial and temporal information have different bandwidth requirements: object appearance changes slowly, but motion can be rapid. SlowFast matches this asymmetry by design.</p> </li> <li> <p>TimeSformer (Bertasius et al., 2021) applies the Vision Transformer to video. It decomposes full spatiotemporal attention (which would be prohibitively expensive: \\(O((T \\times N)^2)\\) for \\(T\\) frames and \\(N\\) patches per frame) into divided attention: each block alternates between temporal attention (each patch attends across time at the same spatial position) and spatial attention (each patch attends across space within the same frame). This reduces the cost from \\(O(T^2 N^2)\\) to \\(O(T^2 + N^2)\\).</p> </li> <li> <p>VideoMAE (Tong et al., 2022) extends the masked autoencoder idea (file 04) to video. An extremely high masking ratio (90-95%) is used because video has high temporal redundancy: neighbouring frames look nearly identical, so masking most patches still leaves enough information for reconstruction. VideoMAE pre-trains a ViT backbone on unlabelled video and transfers to downstream tasks.</p> </li> <li> <p>Action recognition classifies a video clip into one of many action categories (e.g., \"running,\" \"cooking,\" \"playing guitar\"). It is the video analogue of image classification. Standard benchmarks include Kinetics-400 (400 action classes, ~300K clips), Something-Something (174 fine-grained actions requiring temporal reasoning), and ActivityNet (200 classes with long, untrimmed videos).</p> </li> <li> <p>Temporal action detection goes beyond classification: given a long untrimmed video, find the start time, end time, and class of each action. This is the temporal analogue of object detection. Methods like ActionFormer use a Transformer to process temporal features and predict action boundaries.</p> </li> <li> <p>Video object tracking follows a specific object across frames after it is identified in the first frame.</p> </li> <li> <p>SORT (Simple Online and Realtime Tracking, Bewley et al., 2016) combines a detection model (which detects objects in each frame independently) with the Kalman filter for motion prediction and the Hungarian algorithm for assignment.</p> </li> <li> <p>The Kalman filter maintains a state estimate (position, velocity, size) for each tracked object and predicts where it will be in the next frame using a linear motion model. When a new detection arrives, the Kalman filter updates its estimate by combining the prediction with the observation, weighted by their respective uncertainties. This is Bayesian updating (chapter 05) applied to tracking.</p> </li> <li> <p>The Hungarian algorithm solves the bilinear assignment problem: given \\(M\\) tracked objects and \\(N\\) new detections, find the optimal one-to-one matching that minimises total cost (using IoU distance from file 03). Unmatched detections start new tracks; unmatched tracks are terminated after a grace period.</p> </li> <li> <p>DeepSORT extends SORT by adding a deep appearance feature: each detected object is passed through a small CNN that produces an appearance embedding (a descriptor vector). The matching cost combines IoU distance with cosine distance (chapter 01) in embedding space. This handles occlusion and re-identification: even if an object disappears behind another for several frames, its appearance embedding allows re-matching when it reappears.</p> </li> <li> <p>ByteTrack (Zhang et al., 2022) improves tracking by using every detection, including low-confidence ones. Most trackers discard detections below a confidence threshold. ByteTrack first matches high-confidence detections to existing tracks, then matches the remaining low-confidence detections to unmatched tracks. This recovers objects that are temporarily occluded or blurry (and thus have low detection confidence).</p> </li> <li> <p>3D vision recovers the third spatial dimension that is lost in the 2D image projection (file 01).</p> </li> <li> <p>Depth estimation predicts the distance from the camera to each point in the scene.</p> </li> <li> <p>Stereo depth uses two cameras separated by a known baseline \\(b\\). The same point appears at different horizontal positions in the left and right images (this offset is called disparity \\(d\\)). Depth is inversely proportional to disparity:</p> </li> </ul> \\[Z = \\frac{f \\cdot b}{d}\\] <ul> <li> <p>where \\(f\\) is the focal length and \\(b\\) is the baseline distance. Computing disparity requires finding corresponding points between the two images (stereo matching), which is a 1D search along the horizontal scan line (because the cameras are horizontally aligned, a point at the same height in 3D projects to the same row in both images).</p> </li> <li> <p>Monocular depth estimation predicts depth from a single image, which is fundamentally ill-posed (infinitely many 3D scenes can produce the same 2D image). Yet humans do it effortlessly using cues like relative size, texture gradients, occlusion, and atmospheric haze. Deep networks learn these cues from training data.</p> </li> <li> <p>Models like MiDaS and Depth Anything predict relative depth maps (ranking which objects are closer) from single images. They are trained on diverse datasets with a scale-invariant loss and produce remarkably accurate results despite the theoretical ambiguity.</p> </li> <li> <p>Point clouds are sets of 3D points \\((x, y, z)\\), optionally with colour or other attributes, captured by LiDAR sensors or stereo reconstruction. Unlike images, point clouds are unordered and irregularly spaced.</p> </li> <li> <p>PointNet (Qi et al., 2017) processes point clouds directly by applying shared MLPs to each point independently, then aggregating with max pooling (which is permutation-invariant, solving the ordering problem). PointNet++ adds hierarchical grouping to capture local structure at multiple scales.</p> </li> <li> <p>Neural Radiance Fields (NeRFs) (Mildenhall et al., 2020) represent a 3D scene as a continuous function that maps a 3D position \\((x, y, z)\\) and viewing direction \\((\\theta, \\phi)\\) to a colour \\((r, g, b)\\) and density \\(\\sigma\\). This function is parameterised by an MLP:</p> </li> </ul> \\[F_\\theta: (x, y, z, \\theta, \\phi) \\to (r, g, b, \\sigma)\\] <ul> <li>To render a pixel, a ray is cast from the camera through that pixel into the scene. Points are sampled along the ray, and the MLP predicts colour and density at each point. The pixel colour is computed by volume rendering: integrating colour weighted by density along the ray:</li> </ul> \\[C(\\mathbf{r}) = \\int_{t_n}^{t_f} T(t) \\cdot \\sigma(\\mathbf{r}(t)) \\cdot \\mathbf{c}(\\mathbf{r}(t), \\mathbf{d}) \\, dt\\] <ul> <li>where \\(T(t) = \\exp(-\\int_{t_n}^{t} \\sigma(\\mathbf{r}(s)) \\, ds)\\) is the accumulated transmittance (how much light has been absorbed so far). In practice, this integral is approximated by sampling \\(N\\) points along the ray and summing:</li> </ul> \\[\\hat{C} = \\sum_{i=1}^{N} T_i \\cdot (1 - \\exp(-\\sigma_i \\delta_i)) \\cdot c_i\\] <ul> <li> <p>NeRF is trained by minimising the MSE between rendered pixels and ground truth pixels from a set of posed photographs. After training, the NeRF can render photorealistic novel views from any camera position. The limitation is speed: rendering requires evaluating the MLP millions of times (one per sample point per pixel), making real-time rendering difficult.</p> </li> <li> <p>3D Gaussian Splatting (Kerbl et al., 2023) addresses NeRF's speed limitation by representing the scene as a collection of 3D Gaussian primitives rather than a continuous volumetric function. Each Gaussian has a 3D position (mean), a 3D covariance matrix (controlling shape and orientation), opacity, and colour (represented as spherical harmonics for view-dependent effects).</p> </li> <li> <p>Rendering projects each 3D Gaussian onto the image plane (producing a 2D Gaussian \"splat\"), sorts by depth, and composites front-to-back using alpha blending. This is a rasterisation process that runs on the GPU in real time (100+ FPS), orders of magnitude faster than NeRF's ray marching. Gaussian splatting matches or exceeds NeRF quality while enabling real-time rendering.</p> </li> <li> <p>SLAM (Simultaneous Localisation and Mapping) is the problem of building a map of an unknown environment while simultaneously tracking the camera's position within it. It is fundamental to robotics, autonomous driving, and AR.</p> </li> <li> <p>Visual odometry estimates camera motion from frame to frame by tracking features across images. Feature points (SIFT, ORB from file 01) are matched between consecutive frames, and the camera's rotation and translation are estimated from the correspondences using the essential matrix (which encodes the geometric relationship between two views, derived from the intrinsic and extrinsic parameters of file 01).</p> </li> <li> <p>Feature-based SLAM extends visual odometry by maintaining a persistent map. ORB-SLAM (Mur-Artal et al., 2015) is the most widely used feature-based SLAM system. It has three parallel threads:</p> <ol> <li>Tracking: match ORB features in each new frame to the map, estimate camera pose using PnP (Perspective-n-Point) and RANSAC</li> <li>Local mapping: triangulate new map points from matched features, optimise their positions using bundle adjustment (minimising reprojection error across all views that see each point)</li> <li>Loop closure: detect when the camera revisits a previously mapped area (using bag-of-visual-words), then correct the accumulated drift by globally optimising the map</li> </ol> </li> <li> <p>LiDAR SLAM uses 3D point clouds from LiDAR sensors instead of (or in addition to) camera images. LiDAR provides direct depth measurements, making the geometry estimation more robust but at higher hardware cost. Methods like LOAM (LiDAR Odometry and Mapping) register point clouds between consecutive scans using iterative closest point (ICP) alignment.</p> </li> <li> <p>Visual-Inertial SLAM fuses camera data with measurements from an IMU (accelerometer + gyroscope). The IMU provides high-frequency rotation and acceleration estimates that bridge the gaps between camera frames and handle fast motion or temporary visual feature loss.</p> </li> <li> <p>VR/AR applications are among the most demanding consumers of computer vision.</p> </li> <li> <p>Pose estimation determines the position and orientation of the human body (or face, or hands) from images. Body pose is typically represented as a set of 2D or 3D keypoint locations (joints: shoulders, elbows, wrists, hips, knees, ankles). Models like OpenPose and MediaPipe predict these keypoints using heatmap regression: for each joint, the model outputs a heatmap where the peak indicates the joint's location.</p> </li> <li> <p>Top-down methods first detect people with a bounding box detector (file 03), then estimate the pose within each box. Bottom-up methods detect all keypoints in the image first, then group them into individuals using part affinity fields (vector fields that encode the association between connected joints).</p> </li> <li> <p>Scene reconstruction builds a 3D model of the environment from sensor data. In AR, this enables placing virtual objects on real surfaces, occluding virtual objects behind real ones, and casting virtual shadows. Real-time scene reconstruction methods (like depth sensor-based systems in ARKit and ARCore) build a sparse mesh of the environment that updates as the user moves.</p> </li> <li> <p>Real-time rendering constraints in VR are extreme: both eyes need separate renders at 90+ FPS (to avoid motion sickness), with latency under 20 milliseconds from head movement to display update. Techniques like foveated rendering (rendering at high resolution only where the user is looking, using eye tracking) and reprojection (warping the previous frame based on new head pose to fill the gap while the next frame renders) are essential for meeting these constraints.</p> </li> <li> <p>The convergence of real-time neural rendering (3D Gaussian splatting), robust tracking (visual-inertial SLAM), and efficient pose estimation is making photorealistic, interactive AR/VR experiences increasingly feasible.</p> </li> </ul>"},{"location":"chapter%2008%3A%20computer%20vision/05.%20video%20and%203D%20vision/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement the Lucas-Kanade optical flow algorithm from scratch. Compute flow between two synthetic frames where a square moves to the right. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef lucas_kanade(frame1, frame2, window_size=5):\n    \"\"\"Lucas-Kanade optical flow.\"\"\"\n    # Compute gradients\n    Ix = jnp.zeros_like(frame1)\n    Iy = jnp.zeros_like(frame1)\n    It = frame2 - frame1\n\n    # Sobel-like gradients\n    Ix = Ix.at[1:-1, :].set((frame1[2:, :] - frame1[:-2, :]) / 2)\n    Iy = Iy.at[:, 1:-1].set((frame1[:, 2:] - frame1[:, :-2]) / 2)\n\n    H, W = frame1.shape\n    half_w = window_size // 2\n    u = jnp.zeros_like(frame1)\n    v = jnp.zeros_like(frame1)\n\n    for i in range(half_w, H - half_w):\n        for j in range(half_w, W - half_w):\n            Ix_win = Ix[i-half_w:i+half_w+1, j-half_w:j+half_w+1].ravel()\n            Iy_win = Iy[i-half_w:i+half_w+1, j-half_w:j+half_w+1].ravel()\n            It_win = It[i-half_w:i+half_w+1, j-half_w:j+half_w+1].ravel()\n\n            A = jnp.stack([Ix_win, Iy_win], axis=1)\n            ATA = A.T @ A\n            ATb = -A.T @ It_win\n\n            # Check if the system is well-conditioned\n            det = ATA[0,0] * ATA[1,1] - ATA[0,1] * ATA[1,0]\n            if jnp.abs(det) &gt; 1e-6:\n                flow = jnp.linalg.solve(ATA, ATb)\n                u = u.at[i, j].set(flow[0])\n                v = v.at[i, j].set(flow[1])\n\n    return u, v\n\n# Create two frames: a white square that moves right\nframe1 = jnp.zeros((64, 64))\nframe1 = frame1.at[20:40, 15:35].set(1.0)\n\nframe2 = jnp.zeros((64, 64))\nframe2 = frame2.at[20:40, 20:40].set(1.0)  # shifted 5 pixels right\n\nu, v = lucas_kanade(frame1, frame2, window_size=7)\n\n# Visualise\nfig, axes = plt.subplots(1, 3, figsize=(14, 4))\naxes[0].imshow(frame1, cmap='gray'); axes[0].set_title('Frame 1'); axes[0].axis('off')\naxes[1].imshow(frame2, cmap='gray'); axes[1].set_title('Frame 2'); axes[1].axis('off')\n\n# Quiver plot of flow (subsample for clarity)\nstep = 4\nY, X = jnp.mgrid[0:64:step, 0:64:step]\naxes[2].imshow(frame1, cmap='gray', alpha=0.5)\naxes[2].quiver(X, Y, u[::step, ::step], v[::step, ::step],\n               color='#e74c3c', scale=50, width=0.005)\naxes[2].set_title('Optical Flow'); axes[2].axis('off')\n\nplt.tight_layout(); plt.show()\n\n# Check average flow in the moving region\nregion_u = u[20:40, 15:35]\nprint(f\"Average horizontal flow in object region: {region_u[region_u != 0].mean():.2f} pixels\")\n</code></pre></p> </li> <li> <p>Implement a simple Kalman filter for 2D object tracking. Simulate a noisy trajectory and show how the Kalman filter smooths the estimates. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef kalman_predict(x, P, F, Q):\n    \"\"\"Kalman filter prediction step.\"\"\"\n    x_pred = F @ x\n    P_pred = F @ P @ F.T + Q\n    return x_pred, P_pred\n\ndef kalman_update(x_pred, P_pred, z, H, R):\n    \"\"\"Kalman filter update step.\"\"\"\n    y = z - H @ x_pred                        # innovation\n    S = H @ P_pred @ H.T + R                  # innovation covariance\n    K = P_pred @ H.T @ jnp.linalg.inv(S)      # Kalman gain\n    x_updated = x_pred + K @ y\n    P_updated = (jnp.eye(len(x_pred)) - K @ H) @ P_pred\n    return x_updated, P_updated\n\n# State: [x, y, vx, vy]\ndt = 1.0\nF = jnp.array([[1, 0, dt, 0],    # state transition\n                [0, 1, 0, dt],\n                [0, 0, 1, 0],\n                [0, 0, 0, 1]])\nH = jnp.array([[1, 0, 0, 0],     # observation: we measure x, y\n                [0, 1, 0, 0]])\nQ = jnp.eye(4) * 0.01            # process noise\nR = jnp.eye(2) * 4.0             # measurement noise (noisy detector)\n\n# Simulate ground truth: circular motion\nn_steps = 50\nt = jnp.linspace(0, 2 * jnp.pi, n_steps)\ntrue_x = 10 * jnp.cos(t) + 20\ntrue_y = 10 * jnp.sin(t) + 20\n\n# Noisy observations\nkey = jax.random.PRNGKey(42)\nnoise = jax.random.normal(key, (n_steps, 2)) * 2.0\nobs_x = true_x + noise[:, 0]\nobs_y = true_y + noise[:, 1]\n\n# Run Kalman filter\nx = jnp.array([obs_x[0], obs_y[0], 0.0, 0.0])  # initial state\nP = jnp.eye(4) * 10.0                             # initial uncertainty\n\nkalman_x, kalman_y = [], []\nfor i in range(n_steps):\n    x, P = kalman_predict(x, P, F, Q)\n    z = jnp.array([obs_x[i], obs_y[i]])\n    x, P = kalman_update(x, P, z, H, R)\n    kalman_x.append(x[0])\n    kalman_y.append(x[1])\n\nkalman_x = jnp.array(kalman_x)\nkalman_y = jnp.array(kalman_y)\n\n# Visualise\nplt.figure(figsize=(8, 8))\nplt.plot(true_x, true_y, 'k-', linewidth=2, label='Ground Truth')\nplt.scatter(obs_x, obs_y, c='#e74c3c', s=20, alpha=0.5, label='Noisy Observations')\nplt.plot(kalman_x, kalman_y, '#3498db', linewidth=2, label='Kalman Filter')\nplt.legend(); plt.grid(alpha=0.3)\nplt.title('Kalman Filter Tracking')\nplt.xlabel('x'); plt.ylabel('y')\nplt.axis('equal'); plt.show()\n\nobs_error = jnp.mean(jnp.sqrt((obs_x - true_x)**2 + (obs_y - true_y)**2))\nkalman_error = jnp.mean(jnp.sqrt((kalman_x - true_x)**2 + (kalman_y - true_y)**2))\nprint(f\"Observation RMSE: {obs_error:.2f}\")\nprint(f\"Kalman filter RMSE: {kalman_error:.2f}\")\nprint(f\"Error reduction: {(1 - kalman_error/obs_error) * 100:.1f}%\")\n</code></pre></p> </li> <li> <p>Implement a simplified NeRF-style volume rendering pipeline. Cast rays through a simple 3D scene (spheres of known colour and density) and render an image by integrating along each ray. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef render_ray(origin, direction, spheres, n_samples=64, t_near=1.0, t_far=6.0):\n    \"\"\"Volume render a single ray through a scene of spheres.\"\"\"\n    t_vals = jnp.linspace(t_near, t_far, n_samples)\n    deltas = jnp.concatenate([jnp.diff(t_vals), jnp.array([1e-3])])\n\n    colour = jnp.zeros(3)\n    transmittance = 1.0\n\n    for i in range(n_samples):\n        point = origin + t_vals[i] * direction\n\n        # Compute density and colour at this point\n        density = 0.0\n        point_colour = jnp.zeros(3)\n\n        for center, radius, col, sigma in spheres:\n            dist = jnp.linalg.norm(point - center)\n            # Soft sphere: density falls off with distance from surface\n            d = jnp.exp(-jnp.maximum(0, dist - radius) * sigma) * sigma\n            density += d\n            point_colour += d * jnp.array(col)\n\n        # Normalise colour by total density\n        point_colour = jnp.where(density &gt; 1e-6, point_colour / density, point_colour)\n\n        # Volume rendering equation\n        alpha = 1.0 - jnp.exp(-density * deltas[i])\n        colour += transmittance * alpha * point_colour\n        transmittance *= (1.0 - alpha)\n\n    return colour\n\n# Scene: three coloured spheres\nspheres = [\n    (jnp.array([0.0, 0.0, 4.0]), 0.8, [1.0, 0.2, 0.2], 5.0),   # red\n    (jnp.array([1.5, 0.5, 5.0]), 0.6, [0.2, 1.0, 0.2], 5.0),   # green\n    (jnp.array([-1.0, -0.5, 3.5]), 0.5, [0.2, 0.2, 1.0], 5.0), # blue\n]\n\n# Camera setup\nimg_h, img_w = 64, 64\nfocal = 60.0\norigin = jnp.array([0.0, 0.0, 0.0])\n\nimage = jnp.zeros((img_h, img_w, 3))\nfor i in range(img_h):\n    for j in range(img_w):\n        # Compute ray direction\n        px = (j - img_w / 2) / focal\n        py = -(i - img_h / 2) / focal\n        direction = jnp.array([px, py, 1.0])\n        direction = direction / jnp.linalg.norm(direction)\n\n        colour = render_ray(origin, direction, spheres)\n        image = image.at[i, j].set(jnp.clip(colour, 0, 1))\n\nplt.figure(figsize=(6, 6))\nplt.imshow(image)\nplt.title('NeRF-style Volume Rendering\\n(3 spheres)')\nplt.axis('off')\nplt.tight_layout(); plt.show()\nprint(f\"Image shape: {image.shape}\")\nprint(f\"Rendered {img_h * img_w} rays with 64 samples each\")\n</code></pre></p> </li> </ol>"},{"location":"chapter%2009%3A%20audio%20and%20speech/01.%20digital%20signal%20processing/","title":"Digital Signal Processing","text":"<p>Digital signal processing converts raw audio waveforms into structured representations that ML models can learn from. This file covers sound physics, sampling and quantisation, the Fourier transform (DFT, FFT), spectrograms, mel filterbanks, MFCCs, and windowing -- the feature extraction pipeline for all speech and audio AI.</p> <ul> <li> <p>Sound is a pressure wave that propagates through a medium (air, water, solids). A vibrating object (vocal cord, guitar string, speaker cone) pushes and pulls on air molecules, creating alternating regions of high pressure (compression) and low pressure (rarefaction). </p> </li> <li> <p>These pressure variations travel outward at roughly 343 m/s in air and reach your ear, where they vibrate the eardrum and are transduced into neural signals.</p> </li> <li> <p>Think of sound like dropping a stone into a still pond: the stone is the vibrating source, the ripples are the pressure wave, and a cork bobbing on the surface is the microphone or eardrum responding to the wave's arrival. </p> </li> <li> <p>The height of the cork's bobbing is the amplitude, how often it bobs per second is the frequency, and whether it starts at the top or bottom of its bob when the wave arrives is the phase.</p> </li> <li> <p>A waveform is a plot of pressure (or voltage, after a microphone converts sound to an electrical signal) against time. The simplest waveform is a pure tone, a single sinusoid:</p> </li> </ul> \\[x(t) = A \\sin(2\\pi f t + \\phi)\\] <ul> <li> <p>where:</p> <ul> <li>\\(A\\) is the amplitude (peak deviation from zero, determining loudness), </li> <li>\\(f\\) is the frequency in Hz (cycles per second, determining pitch), </li> <li>\\(\\phi\\) is the phase in radians (time offset of the wave). </li> </ul> </li> <li> <p>The period is \\(T = 1/f\\), the duration of one complete cycle.</p> </li> </ul> <p></p> <ul> <li> <p>Amplitude determines the perceived loudness. Doubling the amplitude quadruples the power (since power is proportional to amplitude squared). </p> </li> <li> <p>Human hearing spans an enormous range of amplitudes, so we use a logarithmic scale: the decibel (dB). The sound pressure level is:</p> </li> </ul> \\[L = 20 \\log_{10}\\left(\\frac{A}{A_\\text{ref}}\\right) \\text{ dB}\\] <ul> <li> <p>where \\(A_\\text{ref}\\) is a reference amplitude (typically the threshold of hearing, \\(20 \\mu\\text{Pa}\\)). A whisper is about 30 dB, normal conversation 60 dB, a rock concert 110 dB. Each 6 dB increase roughly doubles the amplitude; each 10 dB increase roughly doubles perceived loudness. The logarithm here is the same function from chapter 03.</p> </li> <li> <p>Frequency determines pitch. Low frequencies (20-250 Hz) sound bass; high frequencies (2000-20000 Hz) sound treble. Human hearing spans roughly 20 Hz to 20 kHz. Concert A is 440 Hz. Doubling the frequency raises the pitch by one octave. </p> </li> <li> <p>Most natural sounds are not pure tones but complex mixtures of many frequencies, which is why a piano and a violin playing the same note sound different: they share the same fundamental frequency but differ in their harmonics (integer multiples of the fundamental) and their relative amplitudes (the timbre).</p> </li> <li> <p>Phase determines where in its cycle the wave starts. Two waves with identical amplitude and frequency but different phases can interfere constructively (phases aligned, amplitudes add) or destructively (phases opposite, amplitudes cancel). </p> </li> <li> <p>Phase is critical in stereo audio and beamforming but is largely discarded in many speech processing pipelines because human pitch and timbre perception is mostly phase-invariant.</p> </li> <li> <p>Real-world audio signals are continuous functions of time, but computers work with discrete numbers. Sampling converts a continuous signal into a discrete sequence by measuring the signal's value at regular intervals. </p> </li> <li> <p>The sample rate \\(f_s\\) is how many measurements per second. CD audio uses \\(f_s = 44{,}100\\) Hz; telephony uses 8000 Hz; modern speech models typically use 16000 Hz.</p> </li> <li> <p>The Nyquist-Shannon sampling theorem states that a continuous signal can be perfectly reconstructed from its samples if and only if the sample rate is at least twice the highest frequency present in the signal:</p> </li> </ul> \\[f_s \\geq 2 f_\\text{max}\\] <ul> <li> <p>The frequency \\(f_s / 2\\) is called the Nyquist frequency. If the signal contains frequencies above the Nyquist frequency, those frequencies fold back into the valid range and appear as false lower-frequency components. This phenomenon is called aliasing. Aliasing is irreversible: once it occurs, the original signal cannot be recovered from the samples.</p> </li> <li> <p>The everyday analogy for aliasing is the wagon-wheel effect in films: a wheel spinning at just above the frame rate appears to spin slowly backward because the camera undersamples the rotation. In audio, a 15 kHz tone sampled at 16 kHz (\\(f_\\text{Nyquist} = 8\\) kHz) aliases down to \\(16 - 15 = 1\\) kHz, a completely different pitch.</p> </li> </ul> <p></p> <ul> <li> <p>To prevent aliasing, an anti-aliasing filter (a low-pass filter) removes all frequencies above \\(f_s/2\\) before sampling. This is applied by the analog-to-digital converter (ADC) hardware before the signal is digitised.</p> </li> <li> <p>Quantisation maps each continuous-valued sample to the nearest value in a finite set of levels. An \\(n\\)-bit quantiser has \\(2^n\\) levels. CD audio uses 16-bit quantisation (\\(2^{16} = 65{,}536\\) levels); telephony often uses 8-bit with \\(\\mu\\)-law or A-law companding (a nonlinear mapping that devotes more levels to small amplitudes, matching human perception). Quantisation introduces quantisation noise, a form of rounding error with variance \\(\\Delta^2/12\\) where \\(\\Delta\\) is the step size between levels.</p> </li> <li> <p>Time-domain analysis extracts features directly from the waveform without transforming it to another domain. These features are simple, fast to compute, and capture basic signal properties.</p> </li> <li> <p>Energy of a frame of \\(N\\) samples measures the overall loudness:</p> </li> </ul> \\[E = \\sum_{n=0}^{N-1} x[n]^2\\] <ul> <li> <p>Speech segments have high energy; silence has low energy. Energy is the squared \\(\\ell_2\\) norm from chapter 01 applied to a signal vector.</p> </li> <li> <p>Zero-crossing rate (ZCR) counts how often the signal changes sign in a frame:</p> </li> </ul> \\[\\text{ZCR} = \\frac{1}{2(N-1)} \\sum_{n=1}^{N-1} |\\text{sign}(x[n]) - \\text{sign}(x[n-1])|\\] <ul> <li> <p>High ZCR indicates high-frequency content or noise; low ZCR indicates low-frequency or voiced speech (where the vocal cords vibrate periodically). ZCR is a crude frequency estimator: a pure tone at \\(f\\) Hz crosses zero \\(2f\\) times per second.</p> </li> <li> <p>Autocorrelation measures how similar a signal is to a delayed copy of itself:</p> </li> </ul> \\[R[k] = \\sum_{n=0}^{N-1-k} x[n] \\cdot x[n+k]\\] <ul> <li> <p>At lag \\(k = 0\\), autocorrelation equals the energy. For periodic signals, the autocorrelation has peaks at lags equal to the period and its multiples. This is the standard technique for pitch detection: find the first significant peak in \\(R[k]\\) after \\(k=0\\), and the pitch is \\(f_s / k_\\text{peak}\\). Autocorrelation is related to the dot product from chapter 01: \\(R[k]\\) is the dot product of the signal with its \\(k\\)-shifted version.</p> </li> <li> <p>Frequency-domain analysis reveals the spectral content of a signal, information invisible in the waveform. The key tool is the Discrete Fourier Transform (DFT), which decomposes a signal of \\(N\\) samples into \\(N\\) complex-valued frequency components:</p> </li> </ul> \\[X[k] = \\sum_{n=0}^{N-1} x[n] \\cdot e^{-j 2\\pi k n / N}, \\quad k = 0, 1, \\ldots, N-1\\] <ul> <li> <p>Each \\(X[k]\\) is a complex number whose magnitude \\(|X[k]|\\) gives the amplitude of the frequency component at \\(f_k = k \\cdot f_s / N\\) Hz, and whose phase \\(\\angle X[k]\\) gives the phase offset. The DFT is a change of basis from the time-domain basis (unit impulses) to the frequency-domain basis (complex exponentials), which is a direct application of the basis concept from chapter 02. The DFT can be written as a matrix multiplication \\(\\mathbf{X} = W \\mathbf{x}\\) where \\(W\\) is the \\(N \\times N\\) DFT matrix with entries \\(W_{kn} = e^{-j2\\pi kn/N}\\).</p> </li> <li> <p>The Fast Fourier Transform (FFT) is an algorithm that computes the DFT in \\(O(N \\log N)\\) operations instead of the naive \\(O(N^2)\\), by recursively splitting the problem into even-indexed and odd-indexed subproblems (the Cooley-Tukey algorithm). This speedup is what makes real-time spectral analysis practical. FFT is one of the most important algorithms in all of computing.</p> </li> <li> <p>The power spectrum \\(|X[k]|^2\\) shows the distribution of energy across frequencies. The magnitude spectrum \\(|X[k]|\\) shows amplitude. Plotting these reveals which frequencies dominate the signal: a vowel has strong harmonics at integer multiples of the fundamental frequency; a fricative (like \"s\") has broad high-frequency energy.</p> </li> <li> <p>A spectrogram is a visual representation of how the frequency content of a signal changes over time. It is computed by chopping the signal into short overlapping frames, computing the FFT of each frame, and stacking the resulting magnitude spectra side by side. The horizontal axis is time, the vertical axis is frequency, and the colour (or brightness) at each point represents the magnitude. The spectrogram is the single most important visualisation in audio processing.</p> </li> </ul> <p></p> <ul> <li>The mel scale is a perceptual frequency scale that reflects how humans perceive pitch. Humans perceive equal ratios of frequency as equal intervals of pitch (just as we perceive equal ratios of intensity as equal intervals of loudness). Below about 1000 Hz, the mel scale is approximately linear; above 1000 Hz, it becomes approximately logarithmic:</li> </ul> \\[m = 2595 \\log_{10}\\left(1 + \\frac{f}{700}\\right)\\] <ul> <li> <p>The inverse is \\(f = 700(10^{m/2595} - 1)\\). The mel scale is the reason that musical semitones are equally spaced on a log-frequency axis: A4 (440 Hz) to A5 (880 Hz) and A5 to A6 (1760 Hz) both sound like \"one octave up\" even though the Hz gaps are 440 and 880 respectively.</p> </li> <li> <p>A mel filterbank is a set of triangular bandpass filters spaced uniformly on the mel scale. Each filter covers a frequency band and sums the spectral energy within that band, producing a single number. Typical speech systems use 40-80 mel filters. Low-frequency filters are narrow (high frequency resolution where we are perceptually sensitive) and high-frequency filters are wide (low resolution where we are less sensitive). This mimics the frequency resolution of the human cochlea.</p> </li> </ul> <p></p> <ul> <li> <p>Mel-Frequency Cepstral Coefficients (MFCCs) are the classic feature representation for speech and audio. They compress the mel spectrum into a small number of decorrelated coefficients that capture the shape of the spectral envelope (which encodes vocal tract configuration and therefore phonetic identity) while discarding fine spectral detail (which encodes pitch and phase).</p> </li> <li> <p>The MFCC pipeline:</p> <ol> <li>Pre-emphasis: apply a first-order high-pass filter \\(y[n] = x[n] - \\alpha x[n-1]\\) (typically \\(\\alpha = 0.97\\)) to boost high frequencies that are attenuated by the vocal tract.</li> <li>Framing: chop the signal into overlapping frames (typically 25 ms long, with 10 ms hop).</li> <li>Windowing: multiply each frame by a window function (Hamming) to reduce spectral leakage (see below).</li> <li>FFT: compute the power spectrum of each windowed frame.</li> <li>Mel filterbank: apply the triangular mel filterbank to the power spectrum, producing mel-band energies.</li> <li>Log: take the logarithm of the mel-band energies. The log compresses the dynamic range and converts multiplication (of spectral components) to addition, matching human loudness perception.</li> <li>DCT: apply the Discrete Cosine Transform to the log-mel energies. The DCT decorrelates the mel bands (since adjacent bands are highly correlated) and compacts the energy into the first few coefficients. Keep the first 13 coefficients (MFCC-0 through MFCC-12).</li> </ol> </li> </ul> <p></p> <ul> <li> <p>The DCT in step 7 is essentially a \"Fourier transform of the spectrum\" (hence the name cepstrum = anagram of spectrum). Low-order cepstral coefficients capture the broad spectral shape (vocal tract resonances, called formants), while high-order coefficients capture fine spectral detail (pitch harmonics). By keeping only the first 13, we retain formant information and discard pitch detail.</p> </li> <li> <p>Delta and delta-delta MFCCs (first and second time derivatives of the MFCCs, computed by finite differences across adjacent frames) capture the dynamics of the spectral shape, adding temporal context. A full MFCC feature vector is often 39-dimensional: 13 static + 13 delta + 13 delta-delta.</p> </li> <li> <p>Modern neural network models (chapter 06) have largely replaced MFCCs with learned features: log-mel spectrograms (the output of step 6, skipping the DCT) are the standard input for deep learning ASR and audio classification. The model learns its own decorrelation. Still, MFCCs remain important for low-resource settings, classical ML pipelines, and understanding the signal processing foundations.</p> </li> <li> <p>Windowing is the process of multiplying a signal frame by a smooth window function before computing the FFT. Without windowing, the FFT assumes the frame repeats infinitely; the abrupt start and end of the frame create artificial discontinuities that spread energy across all frequencies, an artifact called spectral leakage.</p> </li> <li> <p>Rectangular window \\(w[n] = 1\\) for all \\(n\\): no tapering, maximum leakage, but widest main lobe (best frequency resolution for a given frame length). Rarely used in practice.</p> </li> <li> <p>Hamming window: \\(w[n] = 0.54 - 0.46 \\cos(2\\pi n / (N-1))\\). Tapers to near zero at the edges, greatly reducing leakage. The standard choice for speech processing.</p> </li> <li> <p>Hann window (also called Hanning): \\(w[n] = 0.5 - 0.5 \\cos(2\\pi n / (N-1))\\). Tapers to exactly zero at the edges. Very similar to Hamming but with slightly better sidelobe suppression.</p> </li> <li> <p>Blackman window: \\(w[n] = 0.42 - 0.5 \\cos(2\\pi n / (N-1)) + 0.08 \\cos(4\\pi n / (N-1))\\). Even better sidelobe suppression but wider main lobe (worse frequency resolution). Used when sidelobe artifacts are particularly problematic.</p> </li> <li> <p>There is a fundamental tradeoff: windows with less leakage have wider main lobes, meaning they cannot resolve two closely spaced frequencies. This is the spectral resolution vs. leakage tradeoff, a consequence of the uncertainty principle from chapter 03.</p> </li> <li> <p>Overlap-add (OLA) is a technique for reconstructing a signal from windowed, processed frames. Frames overlap (typically 50-75%), and after processing, the windowed outputs are summed. If the window and overlap are chosen correctly (e.g., Hann window with 50% overlap), the overlapping windows sum to a constant, giving perfect reconstruction. This is essential for any frame-based modification of audio (noise reduction, pitch shifting, time stretching).</p> </li> <li> <p>The Short-Time Fourier Transform (STFT) is the formal framework underlying spectrograms. It applies the DFT to each windowed frame of the signal:</p> </li> </ul> \\[ \\text{STFT}\\{x[n]\\}(m, k) = \\sum_{n=0}^{N-1} x[n + mH] \\cdot w[n] \\cdot e^{-j 2\\pi k n / N} \\] <ul> <li> <p>where \\(m\\) is the frame index, \\(H\\) is the hop size (number of samples between consecutive frames), \\(w[n]\\) is the window function, and \\(N\\) is the FFT size. The output is a 2D complex-valued matrix: the time-frequency representation of the signal.</p> </li> <li> <p>The STFT embodies a fundamental time-frequency tradeoff:</p> <ul> <li>Long frames (large \\(N\\)): high frequency resolution (can distinguish closely-spaced frequencies) but poor time resolution (cannot pinpoint when a frequency changes).</li> <li>Short frames (small \\(N\\)): high time resolution but poor frequency resolution.</li> <li>The product of time resolution and frequency resolution is bounded below: \\(\\Delta t \\cdot \\Delta f \\geq \\frac{1}{4\\pi}\\). This is the Gabor limit, the signal processing analogue of the Heisenberg uncertainty principle from physics.</li> </ul> </li> <li> <p>Typical speech STFT parameters: 25 ms frame length (\\(N = 400\\) at 16 kHz), 10 ms hop (\\(H = 160\\)), Hamming window, 512-point FFT (zero-padded from 400 for efficiency and smoother spectral interpolation).</p> </li> <li> <p>Filtering modifies the frequency content of a signal by amplifying some frequencies and attenuating others. A filter is a system that takes an input signal and produces an output signal. Filters are characterised by their frequency response \\(H(f)\\), which describes the gain and phase shift applied to each frequency.</p> </li> <li> <p>Low-pass filter: passes frequencies below a cutoff frequency \\(f_c\\) and attenuates frequencies above it. Removes high-frequency noise and detail. The anti-aliasing filter before sampling is a low-pass filter.</p> </li> <li> <p>High-pass filter: passes frequencies above \\(f_c\\) and attenuates below. Removes low-frequency rumble and DC offset. The pre-emphasis filter in MFCC extraction (\\(y[n] = x[n] - 0.97 x[n-1]\\)) is a simple high-pass filter.</p> </li> <li> <p>Band-pass filter: passes frequencies within a range \\([f_1, f_2]\\) and attenuates outside. Each triangle in the mel filterbank is a band-pass filter.</p> </li> <li> <p>Band-stop (notch) filter: attenuates a specific narrow frequency range. Used to remove specific interference (e.g., 50/60 Hz power line hum).</p> </li> <li> <p>A Finite Impulse Response (FIR) filter computes each output sample as a weighted sum of the current and past input samples:</p> </li> </ul> \\[y[n] = \\sum_{k=0}^{M} b_k \\cdot x[n-k]\\] <ul> <li> <p>The weights \\(b_k\\) are the filter coefficients (also called taps). The filter has order \\(M\\). FIR filters are always stable (the output never diverges) and can be designed to have perfectly linear phase (all frequencies are delayed by the same amount, preserving waveform shape). Their downside is that achieving a sharp cutoff requires many taps (high \\(M\\)), increasing computation. The output is a convolution of the input with the coefficient vector, exactly the 1D convolution operation from chapter 06.</p> </li> <li> <p>An Infinite Impulse Response (IIR) filter uses feedback: the output depends on both past inputs and past outputs:</p> </li> </ul> \\[ y[n] = \\sum_{k=0}^{M} b_k \\cdot x[n-k] - \\sum_{k=1}^{L} a_k \\cdot y[n-k] \\] <ul> <li> <p>The feedback terms \\(a_k\\) create a recursive structure whose impulse response is theoretically infinite in duration. IIR filters achieve sharp cutoffs with far fewer coefficients than FIR filters, but they can be unstable (output grows without bound if the poles of the transfer function lie outside the unit circle, a concept from the \\(z\\)-transform). They also have nonlinear phase, which can distort the waveform shape. Classical filter designs (Butterworth, Chebyshev, elliptic) are IIR.</p> </li> <li> <p>The transfer function of a discrete-time filter is obtained via the \\(z\\)-transform:</p> </li> </ul> \\[H(z) = \\frac{\\sum_{k=0}^{M} b_k z^{-k}}{1 + \\sum_{k=1}^{L} a_k z^{-k}}\\] <ul> <li> <p>The roots of the numerator are called zeros and the roots of the denominator are called poles. The pole-zero plot fully characterises the filter's behaviour. Poles near the unit circle amplify nearby frequencies; zeros near the unit circle attenuate them. FIR filters have only zeros (the denominator is 1). This connects to the eigenvalue and root-finding concepts from chapter 02 and chapter 03.</p> </li> <li> <p>Convolution theorem: convolution in the time domain equals element-wise multiplication in the frequency domain. This means filtering can be performed either by directly convolving the signal with the filter impulse response, or by multiplying their Fourier transforms and inverse-transforming the result. For long filters, the frequency-domain approach (using FFT) is faster: \\(O(N \\log N)\\) versus \\(O(NM)\\).</p> </li> <li> <p>The inverse STFT (iSTFT) reconstructs a time-domain signal from its STFT representation. This is essential for any system that modifies audio in the frequency domain (noise reduction, source separation, voice conversion). The reconstruction uses overlap-add:</p> </li> </ul> \\[ x[n] = \\frac{\\sum_{m} w[n - mH] \\cdot \\text{IDFT}\\{X(m, k)\\}[n - mH]}{\\sum_{m} w[n - mH]^2} \\] <ul> <li> <p>The denominator normalises for the window overlap, ensuring perfect reconstruction when the synthesis window matches the analysis window and the overlap is sufficient.</p> </li> <li> <p>Summary of the DSP chain for speech: raw audio is sampled at 16 kHz, pre-emphasised, chopped into 25 ms Hamming-windowed frames with 10 ms hop, each frame is FFT-transformed, passed through a mel filterbank, log-compressed, and either kept as log-mel features (for neural network models) or DCT-transformed to produce MFCCs (for classical models). This entire chain converts a 1D time-domain signal into a 2D time-frequency representation suitable for downstream machine learning, which will be the subject of file 02.</p> </li> </ul>"},{"location":"chapter%2009%3A%20audio%20and%20speech/01.%20digital%20signal%20processing/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Generate a sine wave, sample it at different rates, and demonstrate aliasing. Plot the continuous signal, properly-sampled version, and under-sampled (aliased) version. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Parameters\nf_signal = 5.0  # 5 Hz signal\nduration = 1.0  # 1 second\n\n# \"Continuous\" signal (very high sample rate)\nt_cont = jnp.linspace(0, duration, 10000)\nx_cont = jnp.sin(2 * jnp.pi * f_signal * t_cont)\n\n# Properly sampled (fs = 50 Hz, well above Nyquist = 10 Hz)\nfs_good = 50\nt_good = jnp.arange(0, duration, 1.0 / fs_good)\nx_good = jnp.sin(2 * jnp.pi * f_signal * t_good)\n\n# Under-sampled (fs = 7 Hz, below Nyquist = 10 Hz) -&gt; aliasing\nfs_bad = 7\nt_bad = jnp.arange(0, duration, 1.0 / fs_bad)\nx_bad = jnp.sin(2 * jnp.pi * f_signal * t_bad)\n\n# The aliased frequency: |f_signal - fs_bad| = |5 - 7| = 2 Hz\nf_alias = abs(f_signal - fs_bad)\nx_alias_cont = jnp.sin(2 * jnp.pi * f_alias * t_cont)\n\nfig, axes = plt.subplots(3, 1, figsize=(12, 9))\n\n# Plot 1: original signal\naxes[0].plot(t_cont, x_cont, color='#3498db', linewidth=1.5, label=f'Original {f_signal} Hz')\naxes[0].set_title(f'Original {f_signal} Hz Signal')\naxes[0].set_xlabel('Time (s)'); axes[0].set_ylabel('Amplitude')\naxes[0].legend(); axes[0].grid(True, alpha=0.3)\n\n# Plot 2: proper sampling\naxes[1].plot(t_cont, x_cont, color='#3498db', linewidth=1, alpha=0.4, label='Original')\naxes[1].stem(t_good, x_good, linefmt='#27ae60', markerfmt='o', basefmt='k-',\n             label=f'Sampled at {fs_good} Hz (above Nyquist)')\naxes[1].set_title(f'Proper Sampling: fs = {fs_good} Hz &gt; 2 x {f_signal} Hz')\naxes[1].set_xlabel('Time (s)'); axes[1].set_ylabel('Amplitude')\naxes[1].legend(); axes[1].grid(True, alpha=0.3)\n\n# Plot 3: aliased sampling\naxes[2].plot(t_cont, x_cont, color='#3498db', linewidth=1, alpha=0.4, label='Original')\naxes[2].stem(t_bad, x_bad, linefmt='#e74c3c', markerfmt='o', basefmt='k-',\n             label=f'Sampled at {fs_bad} Hz (below Nyquist)')\naxes[2].plot(t_cont, x_alias_cont, color='#f39c12', linewidth=1.5, linestyle='--',\n             label=f'Aliased signal appears as {f_alias} Hz')\naxes[2].set_title(f'Aliased Sampling: fs = {fs_bad} Hz &lt; 2 x {f_signal} Hz')\naxes[2].set_xlabel('Time (s)'); axes[2].set_ylabel('Amplitude')\naxes[2].legend(); axes[2].grid(True, alpha=0.3)\n\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Compute and visualise the FFT of a signal composed of multiple sinusoids. Show the magnitude spectrum and identify the constituent frequencies. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Create a composite signal: 220 Hz + 440 Hz + 880 Hz (A3 + A4 + A5)\nfs = 8000  # 8 kHz sample rate\nduration = 0.1  # 100 ms\nt = jnp.arange(0, duration, 1.0 / fs)\nn_samples = len(t)\n\n# Three frequency components with different amplitudes\nx = 1.0 * jnp.sin(2 * jnp.pi * 220 * t) + \\\n    0.6 * jnp.sin(2 * jnp.pi * 440 * t) + \\\n    0.3 * jnp.sin(2 * jnp.pi * 880 * t)\n\n# Compute FFT\nX = jnp.fft.fft(x)\nfreqs = jnp.fft.fftfreq(n_samples, d=1.0 / fs)\nmagnitude = jnp.abs(X) / n_samples  # normalise\n\n# Only plot positive frequencies\npos_mask = freqs &gt;= 0\nfreqs_pos = freqs[pos_mask]\nmag_pos = magnitude[pos_mask] * 2  # double to account for negative freq energy\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 7))\n\n# Time domain\naxes[0].plot(t * 1000, x, color='#3498db', linewidth=1)\naxes[0].set_title('Composite Signal: 220 Hz + 440 Hz + 880 Hz')\naxes[0].set_xlabel('Time (ms)'); axes[0].set_ylabel('Amplitude')\naxes[0].grid(True, alpha=0.3)\n\n# Frequency domain\naxes[1].plot(freqs_pos, mag_pos, color='#e74c3c', linewidth=1.5)\naxes[1].set_title('Magnitude Spectrum (FFT)')\naxes[1].set_xlabel('Frequency (Hz)'); axes[1].set_ylabel('Magnitude')\naxes[1].set_xlim(0, 1500)\n# Annotate peaks\nfor f_peak, amp in [(220, 1.0), (440, 0.6), (880, 0.3)]:\n    axes[1].annotate(f'{f_peak} Hz', xy=(f_peak, amp), fontsize=10,\n                     ha='center', va='bottom', color='#9b59b6',\n                     arrowprops=dict(arrowstyle='-&gt;', color='#9b59b6'))\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Build the full MFCC pipeline from scratch in JAX: pre-emphasis, framing, windowing, FFT, mel filterbank, log, DCT. Visualise the mel filterbank and the resulting MFCCs as a heatmap. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# --- Generate a synthetic speech-like signal ---\nkey = jax.random.PRNGKey(42)\nfs = 16000\nduration = 1.0\nt = jnp.arange(0, duration, 1.0 / fs)\n\n# Simulate voiced speech: fundamental + harmonics with amplitude decay\nf0 = 150.0  # fundamental frequency\nx = sum(jnp.sin(2 * jnp.pi * f0 * k * t) / k for k in range(1, 8))\n# Add some noise\nx = x + 0.1 * jax.random.normal(key, t.shape)\nx = x / jnp.max(jnp.abs(x))  # normalise\n\n# --- Step 1: Pre-emphasis ---\nalpha = 0.97\nx_pre = jnp.concatenate([x[:1], x[1:] - alpha * x[:-1]])\n\n# --- Step 2: Framing ---\nframe_len = int(0.025 * fs)   # 25 ms = 400 samples\nhop_len = int(0.010 * fs)     # 10 ms = 160 samples\nn_frames = (len(x_pre) - frame_len) // hop_len + 1\nframes = jnp.stack([x_pre[i * hop_len : i * hop_len + frame_len]\n                     for i in range(n_frames)])\n\n# --- Step 3: Hamming window ---\nhamming = 0.54 - 0.46 * jnp.cos(2 * jnp.pi * jnp.arange(frame_len) / (frame_len - 1))\nwindowed = frames * hamming\n\n# --- Step 4: FFT ---\nn_fft = 512\nspectra = jnp.fft.rfft(windowed, n=n_fft)\npower_spectra = jnp.abs(spectra) ** 2 / n_fft\n\n# --- Step 5: Mel filterbank ---\nn_mels = 40\nf_min, f_max = 0.0, fs / 2.0\n\ndef hz_to_mel(f):\n    return 2595 * jnp.log10(1 + f / 700)\n\ndef mel_to_hz(m):\n    return 700 * (10 ** (m / 2595) - 1)\n\nmel_min = hz_to_mel(f_min)\nmel_max = hz_to_mel(f_max)\nmel_points = jnp.linspace(mel_min, mel_max, n_mels + 2)\nhz_points = mel_to_hz(mel_points)\n\nfreq_bins = jnp.floor((n_fft + 1) * hz_points / fs).astype(jnp.int32)\nn_freqs = n_fft // 2 + 1\nfilterbank = jnp.zeros((n_mels, n_freqs))\n\nfor m in range(n_mels):\n    f_left = freq_bins[m]\n    f_center = freq_bins[m + 1]\n    f_right = freq_bins[m + 2]\n    # Rising slope\n    for k in range(int(f_left), int(f_center)):\n        if f_center != f_left:\n            filterbank = filterbank.at[m, k].set((k - f_left) / (f_center - f_left))\n    # Falling slope\n    for k in range(int(f_center), int(f_right)):\n        if f_right != f_center:\n            filterbank = filterbank.at[m, k].set((f_right - k) / (f_right - f_center))\n\n# Apply filterbank\nmel_spectra = jnp.dot(power_spectra, filterbank.T)\n\n# --- Step 6: Log ---\nlog_mel = jnp.log(mel_spectra + 1e-10)\n\n# --- Step 7: DCT (type-II) ---\nn_mfcc = 13\nn_mel_channels = log_mel.shape[1]\ndct_matrix = jnp.zeros((n_mfcc, n_mel_channels))\nfor i in range(n_mfcc):\n    for j in range(n_mel_channels):\n        dct_matrix = dct_matrix.at[i, j].set(\n            jnp.cos(jnp.pi * i * (j + 0.5) / n_mel_channels)\n        )\nmfccs = jnp.dot(log_mel, dct_matrix.T)\n\n# --- Visualisation ---\nfig, axes = plt.subplots(3, 1, figsize=(14, 11))\n\n# Mel filterbank\nfreq_axis = jnp.linspace(0, fs / 2, n_freqs)\nfor m in range(n_mels):\n    color = '#3498db' if m % 2 == 0 else '#e74c3c'\n    axes[0].plot(freq_axis, filterbank[m], color=color, alpha=0.6, linewidth=0.8)\naxes[0].set_title(f'Mel Filterbank ({n_mels} filters)')\naxes[0].set_xlabel('Frequency (Hz)'); axes[0].set_ylabel('Weight')\naxes[0].grid(True, alpha=0.3)\n\n# Log-mel spectrogram\nim1 = axes[1].imshow(log_mel.T, aspect='auto', origin='lower',\n                      extent=[0, duration, 0, n_mels], cmap='viridis')\naxes[1].set_title('Log-Mel Spectrogram')\naxes[1].set_xlabel('Time (s)'); axes[1].set_ylabel('Mel Band')\nplt.colorbar(im1, ax=axes[1], label='Log Energy')\n\n# MFCCs\nim2 = axes[2].imshow(mfccs.T, aspect='auto', origin='lower',\n                      extent=[0, duration, 0, n_mfcc], cmap='coolwarm')\naxes[2].set_title(f'MFCCs (first {n_mfcc} coefficients)')\naxes[2].set_xlabel('Time (s)'); axes[2].set_ylabel('MFCC Index')\nplt.colorbar(im2, ax=axes[2], label='Coefficient Value')\n\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Implement FIR low-pass and high-pass filters and visualise their effect on a signal containing both low and high frequency components. Show both time-domain and frequency-domain views. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Create a signal with low (100 Hz) and high (2000 Hz) components\nfs = 8000\nduration = 0.05  # 50 ms for clear visualisation\nt = jnp.arange(0, duration, 1.0 / fs)\n\nx_low = jnp.sin(2 * jnp.pi * 100 * t)\nx_high = 0.5 * jnp.sin(2 * jnp.pi * 2000 * t)\nx = x_low + x_high\n\n# Design a simple FIR low-pass filter using windowed sinc\ndef fir_lowpass(cutoff_hz, fs, n_taps=51):\n    \"\"\"Design FIR low-pass filter using windowed sinc method.\"\"\"\n    fc = cutoff_hz / fs  # normalised cutoff\n    n = jnp.arange(n_taps)\n    mid = (n_taps - 1) / 2.0\n    # Sinc function (ideal low-pass impulse response)\n    h = jnp.where(n == mid, 2 * fc,\n                  jnp.sin(2 * jnp.pi * fc * (n - mid)) / (jnp.pi * (n - mid)))\n    # Apply Hamming window\n    window = 0.54 - 0.46 * jnp.cos(2 * jnp.pi * n / (n_taps - 1))\n    h = h * window\n    h = h / jnp.sum(h)  # normalise to unity gain at DC\n    return h\n\ndef apply_filter(x, h):\n    \"\"\"Apply FIR filter via convolution.\"\"\"\n    return jnp.convolve(x, h, mode='same')\n\n# Low-pass filter at 500 Hz (passes 100 Hz, blocks 2000 Hz)\nh_lp = fir_lowpass(500, fs, n_taps=51)\nx_lp = apply_filter(x, h_lp)\n\n# High-pass = delta - low-pass (spectral inversion)\ndelta = jnp.zeros(51)\ndelta = delta.at[25].set(1.0)\nh_hp = delta - h_lp\nx_hp = apply_filter(x, h_hp)\n\n# Compute spectra for all signals\ndef compute_spectrum(signal, fs):\n    X = jnp.fft.rfft(signal)\n    freqs = jnp.fft.rfftfreq(len(signal), d=1.0 / fs)\n    mag = jnp.abs(X) / len(signal) * 2\n    return freqs, mag\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 10))\n\n# Time domain plots\nfor i, (sig, title, color) in enumerate([\n    (x, 'Original (100 Hz + 2000 Hz)', '#3498db'),\n    (x_lp, 'Low-pass filtered (&lt; 500 Hz)', '#27ae60'),\n    (x_hp, 'High-pass filtered (&gt; 500 Hz)', '#e74c3c')\n]):\n    axes[i, 0].plot(t * 1000, sig[:len(t)], color=color, linewidth=1)\n    axes[i, 0].set_title(f'Time Domain: {title}')\n    axes[i, 0].set_xlabel('Time (ms)'); axes[i, 0].set_ylabel('Amplitude')\n    axes[i, 0].grid(True, alpha=0.3)\n\n# Frequency domain plots\nfor i, (sig, title, color) in enumerate([\n    (x, 'Original', '#3498db'),\n    (x_lp, 'Low-pass', '#27ae60'),\n    (x_hp, 'High-pass', '#e74c3c')\n]):\n    freqs, mag = compute_spectrum(sig, fs)\n    axes[i, 1].plot(freqs, mag, color=color, linewidth=1.5)\n    axes[i, 1].set_title(f'Spectrum: {title}')\n    axes[i, 1].set_xlabel('Frequency (Hz)'); axes[i, 1].set_ylabel('Magnitude')\n    axes[i, 1].set_xlim(0, 3000)\n    axes[i, 1].axvline(x=500, color='#f39c12', linestyle='--', alpha=0.7,\n                        label='Cutoff (500 Hz)')\n    axes[i, 1].legend(); axes[i, 1].grid(True, alpha=0.3)\n\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2009%3A%20audio%20and%20speech/02.%20automatic%20speech%20recognition/","title":"Automatic Speech Recognition","text":"<p>Automatic speech recognition converts spoken audio into written text, bridging the gap between human speech and machine-readable language. This file covers GMM-HMMs, CTC loss, the RNN-Transducer, attention-based encoder-decoder models (LAS), Whisper, and end-to-end ASR -- from classical pipelines to modern neural architectures.</p> <ul> <li> <p>Automatic speech recognition (ASR) is the task of converting spoken audio into written text. It is one of the oldest problems in AI (first systems in the 1950s recognised single digits) and one of the most commercially deployed (voice assistants, transcription services, subtitling). </p> </li> <li> <p>The difficulty comes from the enormous variability of speech: different speakers, accents, speaking rates, background noise, microphone characteristics, and the fundamental ambiguity of mapping a continuous acoustic signal to discrete words.</p> </li> <li> <p>Think of ASR like a courtroom stenographer. The stenographer hears a continuous stream of sound, mentally segments it into words, resolves ambiguities using context (\"they're\" vs \"their\" vs \"there\"), and types the result. An ASR system does the same thing, but in stages that can be made explicit and optimised independently or jointly.</p> </li> <li> <p>The classical ASR pipeline processes audio in a chain of distinct stages: raw audio is transformed into features (MFCCs or log-mel spectrograms, from file 01), an acoustic model scores how well each feature frame matches each phonetic unit, a pronunciation model (lexicon) maps phonetic units to words, a language model scores how likely word sequences are, and a decoder searches for the word sequence that maximises the combined score. Each component is trained and tuned separately.</p> </li> </ul> <p></p> <ul> <li> <p>Phonemes are the smallest units of sound that distinguish words in a language. English has roughly 39-44 phonemes (the exact number depends on the dialect and the phoneme inventory used). For example, \"bat\" and \"pat\" differ in one phoneme (/b/ vs /p/). Most ASR systems model context-dependent phonemes called triphones: a phoneme defined by its left and right neighbours (e.g., \"a\" in the context of \"b_t\" is a different unit from \"a\" in \"c_t\"), because the acoustic realisation of a phoneme is heavily influenced by its neighbours (this is called coarticulation).</p> </li> <li> <p>The number of possible triphones is enormous (40 phonemes cubed = 64,000), so decision tree clustering groups acoustically similar triphones into senones (typically 2000-10,000 classes). Each senone gets its own acoustic model. This clustering is a form of the decision tree algorithms from chapter 06.</p> </li> <li> <p>GMM-HMM (Gaussian Mixture Model - Hidden Markov Model) was the dominant acoustic modelling approach from the 1980s to the early 2010s. The HMM (from chapter 05) models the temporal structure of speech: each phoneme is a left-to-right HMM with 3-5 states, where each state represents a sub-phonetic segment (onset, middle, offset). The state-to-state transitions model duration implicitly.</p> </li> <li> <p>At each HMM state, the emission probability (how likely a particular feature vector is given that state) is modelled by a Gaussian mixture model (GMM): a weighted sum of multivariate Gaussian distributions (from chapter 05):</p> </li> </ul> \\[ p(\\mathbf{x} | s) = \\sum_{m=1}^{M} w_m \\cdot \\mathcal{N}(\\mathbf{x} ; \\boldsymbol{\\mu}_m, \\boldsymbol{\\Sigma}_m) \\] <ul> <li> <p>where \\(\\mathbf{x}\\) is the feature vector (e.g., 39-dimensional MFCCs), \\(s\\) is the HMM state, \\(M\\) is the number of mixture components (typically 8-64), \\(w_m\\) are mixture weights, and \\(\\boldsymbol{\\mu}_m\\), \\(\\boldsymbol{\\Sigma}_m\\) are the mean and covariance of each Gaussian component. The covariance matrices are usually diagonal for computational efficiency (assuming feature dimensions are independent, which is approximately true for MFCCs due to the DCT decorrelation).</p> </li> <li> <p>Training uses the Baum-Welch algorithm (a special case of EM, from chapter 05) to iteratively estimate the GMM parameters and HMM transition probabilities from transcribed speech data. Decoding (finding the most likely state sequence) uses the Viterbi algorithm (dynamic programming, from chapter 05):</p> </li> </ul> \\[ \\delta_t(j) = \\max_{i} \\left[ \\delta_{t-1}(i) \\cdot a_{ij} \\right] \\cdot b_j(\\mathbf{x}_t) \\] <ul> <li> <p>where \\(\\delta_t(j)\\) is the probability of the best path ending in state \\(j\\) at time \\(t\\), \\(a_{ij}\\) is the transition probability from state \\(i\\) to state \\(j\\), and \\(b_j(\\mathbf{x}_t)\\) is the emission probability of feature \\(\\mathbf{x}_t\\) in state \\(j\\).</p> </li> <li> <p>DNN-HMM (Hinton et al., 2012) replaced the GMM emission model with a deep neural network (DNN, from chapter 06) that predicts senone posterior probabilities \\(p(s | \\mathbf{x})\\) from a window of feature frames. The HMM still handles temporal structure and sequencing, but the neural network provides far more discriminative emission scores. This hybrid approach reduced word error rates by 20-30% relative to GMMs and was the dominant paradigm from 2012-2016.</p> </li> <li> <p>WFST decoding (Weighted Finite-State Transducer) is the standard decoding framework for traditional ASR. Each component (HMM topology H, context-dependency C, lexicon L, grammar/language model G) is represented as a weighted finite-state transducer, and they are composed into a single search graph \\(H \\circ C \\circ L \\circ G\\). The Viterbi search then finds the lowest-cost path through this composed graph. WFSTs allow modular combination of knowledge sources and efficient dynamic programming search. The mathematical framework is from finite automata theory (related to the state machines in chapter 05).</p> </li> <li> <p>End-to-end ASR eliminates the separate components (pronunciation model, phoneme inventory, WFST decoder) and trains a single neural network that maps directly from audio features to characters or word pieces. The key challenge is the alignment problem: the input (hundreds of feature frames per second) and output (a few characters per second) have very different lengths, and the alignment between them is unknown during training.</p> </li> <li> <p>Connectionist Temporal Classification (CTC) (Graves et al., 2006) solves the alignment problem by introducing a special blank token and allowing the network to output any sequence of characters and blanks, as long as collapsing consecutive duplicates and removing blanks yields the correct transcript. For example, the transcript \"cat\" could be produced by the output sequence \"--cc-aa-t--\" (where \"-\" is blank).</p> </li> <li> <p>Formally, CTC defines a many-to-one mapping \\(\\mathcal{B}\\) from the set of all length-\\(T\\) output sequences (over the alphabet plus blank) to label sequences. The probability of a label sequence \\(\\mathbf{y}\\) is the sum over all alignments that collapse to it:</p> </li> </ul> \\[P(\\mathbf{y} | \\mathbf{x}) = \\sum_{\\boldsymbol{\\pi} \\in \\mathcal{B}^{-1}(\\mathbf{y})} \\prod_{t=1}^{T} p(\\pi_t | \\mathbf{x})\\] <p></p> <ul> <li> <p>Computing this sum naively would require enumerating exponentially many alignments, but the CTC forward-backward algorithm computes it efficiently in \\(O(T \\cdot |\\mathbf{y}|)\\) using dynamic programming, analogous to the HMM forward-backward algorithm from chapter 05.</p> </li> <li> <p>CTC makes a conditional independence assumption: the output at each time step is independent of all other outputs given the input. This means CTC cannot model output dependencies (e.g., it cannot learn that \"q\" is almost always followed by \"u\"). An external language model must be used to handle such dependencies.</p> </li> <li> <p>CTC decoding options:</p> <ul> <li>Greedy decoding: take the most probable token at each time step, then collapse. Fast but suboptimal.</li> <li>Beam search: maintain the top-\\(k\\) partial hypotheses at each step, merging hypotheses that collapse to the same prefix. Can incorporate a language model score.</li> <li>Prefix beam search: a modified beam search that correctly handles the CTC blank merging, ensuring hypotheses are compared after collapsing.</li> </ul> </li> <li> <p>RNN-Transducer (RNN-T) (Graves, 2012) extends CTC by adding an explicit prediction network (a language model-like RNN) that conditions each output on the previous outputs, removing the conditional independence assumption. RNN-T has three components:</p> <ul> <li>Encoder: processes the audio features to produce hidden representations \\(\\mathbf{h}_t^\\text{enc}\\) (typically a stack of LSTMs or Conformer layers).</li> <li>Prediction network: an autoregressive RNN that produces hidden representations \\(\\mathbf{h}_u^\\text{pred}\\) from the previously emitted labels.</li> <li>Joint network: combines the encoder and prediction network outputs at each (time, label) position and produces a distribution over the next token (including blank):</li> </ul> </li> </ul> \\[p(y | t, u) = \\text{softmax}(W \\cdot \\text{tanh}(W_\\text{enc} \\mathbf{h}_t^\\text{enc} + W_\\text{pred} \\mathbf{h}_u^\\text{pred} + b))\\] <ul> <li> <p>RNN-T can emit zero or more labels per time step (by emitting non-blank tokens before advancing to the next time step, or emitting blank to advance without output). Training uses a forward-backward algorithm over the 2D (time, label) lattice, with complexity \\(O(T \\cdot U)\\) where \\(U\\) is the output length. RNN-T is the dominant architecture for on-device streaming ASR (used in Google's Pixel phones and similar products) because it naturally supports streaming: the encoder processes audio left-to-right and the prediction network generates output incrementally.</p> </li> <li> <p>Listen, Attend and Spell (LAS) (Chan et al., 2016) is an attention-based encoder-decoder model (the sequence-to-sequence architecture from chapter 06). It has three components:</p> <ul> <li>Listener (encoder): a pyramidal bidirectional LSTM that processes the full input sequence and downsamples by a factor of 8 (by concatenating pairs of consecutive hidden states at each layer), producing a shorter sequence of encoder hidden states.</li> <li>Attention: at each decoder step, computes attention weights over all encoder states to form a context vector (the same attention mechanism from chapter 07).</li> <li>Speller (decoder): an autoregressive LSTM that generates the output transcript one character at a time, conditioned on the context vector and previously generated characters.</li> </ul> </li> <li> <p>LAS achieves strong results but requires the full utterance to be available before decoding (because the attention attends to all encoder states), making it unsuitable for streaming applications. It also struggles with very long utterances because attention over long sequences becomes diffuse.</p> </li> <li> <p>The Conformer (Gulati et al., 2020) combines the local pattern-capturing ability of convolutions with the global dependency modelling of self-attention. Each Conformer block has four modules in a sandwich structure:</p> <ol> <li>Feed-forward module (half-step): a feed-forward network with residual connection, using half the residual weight.</li> <li>Multi-head self-attention module: standard transformer self-attention (from chapter 07) with relative positional encoding.</li> <li>Convolution module: a pointwise convolution, a gated linear unit (GLU), a 1D depthwise convolution, batch normalisation, a Swish activation, and another pointwise convolution. The depthwise convolution captures local context (like an n-gram over the feature sequence).</li> <li>Feed-forward module (half-step): identical to module 1.</li> </ol> </li> <li> <p>The output is: \\(\\mathbf{y} = \\text{LayerNorm}(\\mathbf{x} + \\frac{1}{2}\\text{FFN}_1 + \\text{MHSA} + \\text{Conv} + \\frac{1}{2}\\text{FFN}_2)\\). The macaron-like structure (FFN-Attention-Conv-FFN) with half-step residuals was found empirically to outperform other orderings. Conformers have become the default encoder for both CTC and RNN-T systems, outperforming pure transformer and pure LSTM encoders.</p> </li> </ul> <p></p> <ul> <li> <p>Whisper (Radford et al., 2023) is OpenAI's large-scale attention-based ASR model. It uses a standard encoder-decoder transformer architecture (from chapter 07) trained on 680,000 hours of weakly supervised data scraped from the internet (audio paired with approximate transcripts). Key design choices:</p> <ul> <li>Input: 80-channel log-mel spectrogram (from file 01) with 25 ms windows and 10 ms hops, normalised to zero mean and unit variance.</li> <li>Encoder: standard transformer encoder with sinusoidal positional embeddings and pre-activation layer normalisation.</li> <li>Decoder: transformer decoder that autoregressively generates tokens using a byte-level BPE tokenizer (from chapter 07).</li> <li>Multitask: a single model handles transcription, translation, language identification, and timestamp prediction, conditioned on special task tokens in the decoder prompt.</li> <li>The scale of the training data (rather than architectural novelty) is the primary driver of Whisper's strong generalisation across domains, accents, and languages.</li> </ul> </li> <li> <p>wav2vec 2.0 (Baevski et al., 2020) is a self-supervised pre-training framework for speech representations. The core idea is to learn speech representations from large amounts of unlabelled audio, then fine-tune with a small amount of labelled data. This follows the same self-supervised paradigm as BERT (from chapter 07) but adapted for continuous audio signals.</p> </li> <li> <p>The wav2vec 2.0 architecture has three parts:</p> <ul> <li>Feature encoder: a multi-layer 1D CNN that processes raw waveform samples and produces latent representations \\(\\mathbf{z}_t\\) at a 20 ms frame rate (one vector every 320 samples at 16 kHz).</li> <li>Quantisation module: discretises the latent representations into a finite codebook using product quantisation (dividing the vector into groups and quantising each group independently, choosing from \\(G\\) codebooks of \\(V\\) entries each). This produces targets \\(\\mathbf{q}_t\\) for the contrastive learning objective.</li> <li>Context network: a transformer encoder that takes the (partially masked) latent representations and produces contextualised representations \\(\\mathbf{c}_t\\).</li> </ul> </li> </ul> <p></p> <ul> <li>During pre-training, random spans of latent representations are masked (replaced with a learned mask embedding), and the model must identify the true quantised representation of the masked position from a set of distractors (negatives sampled from other positions in the same utterance). The contrastive loss is:</li> </ul> \\[\\mathcal{L} = -\\log \\frac{\\exp(\\text{sim}(\\mathbf{c}_t, \\mathbf{q}_t) / \\kappa)}{\\sum_{\\tilde{\\mathbf{q}} \\in Q_t} \\exp(\\text{sim}(\\mathbf{c}_t, \\tilde{\\mathbf{q}}) / \\kappa)}\\] <ul> <li> <p>where \\(\\text{sim}\\) is cosine similarity, \\(\\kappa\\) is a temperature parameter, and \\(Q_t\\) includes the true quantised target plus distractors. An additional diversity loss encourages equal use of all codebook entries. This loss is essentially the InfoNCE contrastive loss, the same family of contrastive objectives used in visual self-supervised learning.</p> </li> <li> <p>After pre-training, a linear projection and CTC head are added on top, and the model is fine-tuned on labelled data. wav2vec 2.0 achieved near state-of-the-art results with only 10 minutes of labelled data (using 53,000 hours of unlabelled audio for pre-training), demonstrating the power of self-supervised learning for low-resource speech recognition.</p> </li> <li> <p>HuBERT (Hsu et al., 2021) is another self-supervised approach that replaces the contrastive objective with a masked prediction objective (predicting discrete cluster assignments of the masked frames). The targets are produced by an offline clustering step (k-means on MFCCs in the first iteration, then k-means on HuBERT features in subsequent iterations). HuBERT simplifies the training pipeline compared to wav2vec 2.0 (no quantisation module or contrastive sampling needed) and achieves comparable or better results.</p> </li> <li> <p>Fast Conformer (Rekesh et al., 2023, NVIDIA NeMo) replaces the quadratic self-attention in the standard Conformer with a down-sampled attention mechanism: the input sequence is compressed (typically 8\u00d7 via strided convolution) before computing attention, then expanded back. This reduces the attention cost from \\(O(T^2)\\) to \\(O(T^2/64)\\) while retaining global context, enabling training on very long utterances (up to several minutes) without memory issues. Fast Conformer is the default encoder in NVIDIA's NeMo toolkit and forms the backbone of their production-grade models.</p> </li> <li> <p>Parakeet (NVIDIA, 2024) is a family of high-accuracy English ASR models built on the Fast Conformer encoder with CTC and RNN-T decoders, trained on 64,000 hours of English speech. Parakeet models (0.6B and 1.1B parameters) achieved the lowest word error rates on standard benchmarks at the time of release, surpassing Whisper large-v3 on most English test sets. The key ingredients are the efficient Fast Conformer architecture, aggressive data augmentation (SpecAugment, speed perturbation, noise mixing), and large-scale supervised training data \u2014 demonstrating that careful engineering of known components can still push the state of the art.</p> </li> <li> <p>Canary (NVIDIA, 2024) extends the NeMo framework to multilingual and multitask ASR. It uses a Fast Conformer encoder with an attention-based decoder (rather than CTC or RNN-T) and handles transcription plus translation across multiple languages in a single model (similar to Whisper's multitask design but with the more efficient Fast Conformer backbone). Canary models support English, German, Spanish, and French with competitive accuracy.</p> </li> <li> <p>Moonshine (Useful Sensors, 2024) is a family of ASR models specifically optimised for on-device and edge deployment. The encoder uses a hybrid architecture that replaces the initial transformer/conformer layers with a small CNN followed by a few transformer layers, dramatically reducing the model size (the base model is under 30M parameters). Moonshine targets real-time streaming on CPUs and low-power devices where Whisper would be too large and slow, trading some accuracy for 5-10\u00d7 lower latency and memory footprint.</p> </li> <li> <p>Distil-Whisper (Gandhi et al., 2023) applies knowledge distillation (chapter 06) to compress Whisper into a smaller, faster model. The student model uses only 2 decoder layers (compared to Whisper's 32) while keeping the full encoder, and is trained to match Whisper's output distributions. Distil-Whisper achieves within 1% WER of the teacher while being 6\u00d7 faster, making it practical for real-time applications where the full Whisper model is too slow.</p> </li> <li> <p>Universal Speech Model (USM) (Zhang et al., 2023, Google) scales self-supervised pre-training to 12 million hours of unlabelled audio across 300+ languages, followed by supervised fine-tuning. USM demonstrates that the wav2vec 2.0 / self-supervised paradigm scales to truly massive data regimes, achieving strong performance on low-resource languages with very limited labelled data.</p> </li> <li> <p>Massively Multilingual Speech (MMS) (Pratap et al., 2023, Meta) extends wav2vec 2.0 pre-training to over 1,100 languages using religious recordings and other sources of multilingual audio. MMS covers far more languages than any previous ASR system, enabling speech recognition for many under-resourced languages for the first time.</p> </li> <li> <p>The landscape of modern ASR is converging on a few dominant patterns: (1) Conformer-family encoders with CTC or RNN-T for streaming, (2) encoder-decoder transformers for offline/multitask, (3) self-supervised pre-training for low-resource settings, and (4) scale \u2014 more data and larger models consistently improve accuracy. The choice between these depends on the deployment constraints: latency budget, compute available, number of languages, and whether the application is streaming or batch.</p> </li> <li> <p>Language model integration improves ASR by incorporating linguistic knowledge beyond what the acoustic model captures. The basic idea is to combine the acoustic model score \\(p(\\mathbf{x} | \\mathbf{y})\\) (how well the audio matches the transcript) with a language model score \\(p(\\mathbf{y})\\) (how likely the transcript is as a sentence) during decoding.</p> </li> <li> <p>Shallow fusion combines the scores at beam search time:</p> </li> </ul> \\[\\hat{\\mathbf{y}} = \\arg\\max_\\mathbf{y} \\left[ \\log p_\\text{AM}(\\mathbf{y} | \\mathbf{x}) + \\lambda \\log p_\\text{LM}(\\mathbf{y}) \\right]\\] <ul> <li> <p>where \\(\\lambda\\) is a tunable weight and \\(p_\\text{LM}\\) is an external language model (typically an n-gram or neural LM from chapter 07). This is simple and effective but requires the LM to operate on the same token vocabulary as the ASR model.</p> </li> <li> <p>Deep fusion (Gulcehre et al., 2015) integrates the language model inside the decoder network: the LM hidden state is concatenated with the decoder hidden state and passed through a gating mechanism before the output projection. The entire system (including the pre-trained LM) is fine-tuned jointly. This allows deeper integration but is more complex to train.</p> </li> <li> <p>Cold fusion (Sriram et al., 2018) is similar to deep fusion but trains the ASR decoder from scratch with the language model integrated, rather than fine-tuning a pre-trained decoder. This forces the acoustic model to learn complementary information rather than duplicating what the LM already knows.</p> </li> <li> <p>Rescoring (N-best rescoring) is a two-pass approach: first generate \\(N\\) candidate transcripts using beam search, then re-rank them using a more powerful language model (e.g., a large transformer LM). This is simple to implement and allows using very large LMs that would be too slow for first-pass decoding.</p> </li> <li> <p>Internal language model estimation (ILME) addresses a subtle problem: end-to-end models implicitly learn an internal LM from the training transcripts, which can conflict with the external LM during shallow fusion (essentially double-counting the linguistic prior). ILME estimates the internal LM and subtracts its score during fusion:</p> </li> </ul> \\[\\hat{\\mathbf{y}} = \\arg\\max_\\mathbf{y} \\left[ \\log p_\\text{E2E}(\\mathbf{y} | \\mathbf{x}) - \\beta \\log p_\\text{ILM}(\\mathbf{y}) + \\lambda \\log p_\\text{LM}(\\mathbf{y}) \\right]\\] <ul> <li> <p>Streaming vs. offline ASR is a fundamental architectural choice. Offline (or batch) ASR processes the entire utterance before producing any output. Streaming ASR produces output incrementally as audio arrives, with bounded latency.</p> </li> <li> <p>Streaming is essential for real-time applications: live captioning, voice assistants (the user expects a response before they finish speaking), telephone call transcription. The challenge is that some future context is helpful for recognition (knowing that the next word is \"York\" disambiguates \"New\"), but streaming systems cannot wait for arbitrarily long future context.</p> </li> <li> <p>Unidirectional encoders (left-to-right LSTMs, causal convolutions, causal transformers) naturally support streaming because each output depends only on past and present input. Bidirectional encoders (which look at future context) do not support streaming directly.</p> </li> <li> <p>Chunked attention (also called blockwise or segmental attention) divides the input into fixed-length chunks and applies self-attention only within each chunk (and optionally to a few preceding chunks). This limits the latency to the chunk size plus processing time, while still allowing some local bidirectional context within each chunk. The tradeoff is that accuracy degrades as the chunk size decreases.</p> </li> <li> <p>Lookahead allows a streaming encoder to peek at a small number of future frames (e.g., 300-900 ms) before producing output for the current frame. This is implemented by adding a small right-context to the unidirectional computation. The lookahead window adds latency but significantly improves accuracy.</p> </li> <li> <p>Latency in streaming ASR has several components:</p> <ul> <li>Algorithmic latency: the delay from when audio arrives to when the model can process it (determined by chunk size, lookahead, and feature extraction).</li> <li>Computational latency: the time to run the model's forward pass.</li> <li>Endpointer latency: the delay in detecting that the user has finished speaking.</li> <li>First-token latency: how quickly the first word appears. Finalization latency: how quickly the final output is confirmed (streaming systems often produce provisional output that gets corrected as more audio arrives).</li> </ul> </li> <li> <p>Evaluation metrics for ASR:</p> </li> <li> <p>Word Error Rate (WER) is the primary metric. It is computed by aligning the hypothesis (system output) to the reference (ground truth transcript) using edit distance (minimum number of substitutions, insertions, and deletions to transform one into the other), then:</p> </li> </ul> \\[\\text{WER} = \\frac{S + D + I}{N}\\] <ul> <li> <p>where \\(S\\) is substitutions, \\(D\\) is deletions, \\(I\\) is insertions, and \\(N\\) is the total number of words in the reference. WER can exceed 100% if there are many insertions. A WER of 5% is considered roughly human-level for clean read speech; conversational or noisy speech is much harder (10-20%+).</p> </li> <li> <p>Character Error Rate (CER) is the same formula applied at the character level rather than the word level. CER is more informative for languages without clear word boundaries (Chinese, Japanese) and for evaluating how close near-misses are (\"cat\" vs \"bat\" is 100% WER but 33% CER).</p> </li> <li> <p>Word Information Lost (WIL) and Word Information Preserved (WIP) are information-theoretic alternatives that account for the correlation between reference and hypothesis more precisely than WER, but they are less commonly reported.</p> </li> <li> <p>Real-Time Factor (RTF) measures computational efficiency: the ratio of processing time to audio duration. RTF &lt; 1 means the system runs faster than real time; RTF &gt; 1 means it cannot keep up with live audio. Streaming systems must maintain RTF &lt; 1.</p> </li> <li> <p>Data augmentation is critical for robust ASR. Common techniques:</p> <ul> <li>Speed perturbation: resampling audio at 0.9x and 1.1x speed (changing pitch and duration).</li> <li>SpecAugment (Park et al., 2019): masking random frequency bands and time steps in the spectrogram. This is the audio analogue of dropout and is one of the most effective regularisation techniques for ASR. It requires no additional data.</li> <li>Noise augmentation: mixing clean speech with recorded noise at various signal-to-noise ratios.</li> <li>Room impulse response simulation: convolving clean speech with simulated room acoustics to simulate reverberant environments.</li> </ul> </li> <li> <p>Tokenisation for ASR determines the output vocabulary of the model. Options include:</p> <ul> <li>Characters: simple, small vocabulary (~30 for English), but long output sequences and no implicit language modelling.</li> <li>Word pieces / BPE (from chapter 07): subword units that balance vocabulary size and sequence length. The standard for modern systems (Whisper uses byte-level BPE with ~50,000 tokens).</li> <li>Words: large vocabulary (50,000+), short output sequences, but cannot handle out-of-vocabulary words.</li> <li>Phonemes: linguistically motivated, compact, but requires a pronunciation lexicon.</li> </ul> </li> <li> <p>The evolution of ASR can be summarised as a progression from heavily engineered modular systems (GMM-HMM + WFST decoding, 1990s-2010s) to hybrid systems (DNN-HMM, 2012-2016) to end-to-end systems that absorb more and more of the pipeline into a single neural network (CTC, RNN-T, LAS, 2016-2020) to large-scale pre-trained models that leverage vast amounts of unlabelled or weakly labelled data (wav2vec 2.0, Whisper, 2020-present). Each transition simplified the engineering while improving accuracy, following the broader trend in machine learning toward learning representations from data rather than hand-designing them (the same story told in chapter 06 for image features replaced by CNNs, and in chapter 07 for NLP features replaced by transformers).</p> </li> </ul>"},{"location":"chapter%2009%3A%20audio%20and%20speech/02.%20automatic%20speech%20recognition/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement CTC loss from scratch in JAX. Create a toy example with a short sequence of logits and a target label, compute the CTC forward algorithm to get the total probability, and compute the negative log-likelihood loss. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef ctc_forward(log_probs, targets):\n    \"\"\"\n    CTC forward algorithm (log-domain for numerical stability).\n    log_probs: (T, V) log probabilities over vocabulary (index 0 = blank)\n    targets: (U,) target label indices (no blanks)\n    Returns: log probability of the target sequence under CTC.\n    \"\"\"\n    T, V = log_probs.shape\n    U = len(targets)\n\n    # Build the extended label sequence with blanks: [blank, y1, blank, y2, ..., yU, blank]\n    S = 2 * U + 1\n    labels = jnp.zeros(S, dtype=jnp.int32)  # all blanks\n    for i in range(U):\n        labels = labels.at[2 * i + 1].set(targets[i])\n\n    # Initialise alpha (log domain)\n    NEG_INF = -1e30\n    alpha = jnp.full((T, S), NEG_INF)\n    alpha = alpha.at[0, 0].set(log_probs[0, labels[0]])        # start with blank\n    alpha = alpha.at[0, 1].set(log_probs[0, labels[1]])        # or first label\n\n    # Fill forward\n    for t in range(1, T):\n        for s in range(S):\n            # Same state\n            a = alpha[t - 1, s]\n            # From previous state\n            if s &gt; 0:\n                a = jnp.logaddexp(a, alpha[t - 1, s - 1])\n            # Skip blank (if current and two-back labels are different)\n            if s &gt; 1 and labels[s] != 0 and labels[s] != labels[s - 2]:\n                a = jnp.logaddexp(a, alpha[t - 1, s - 2])\n            alpha = alpha.at[t, s].set(a + log_probs[t, labels[s]])\n\n    # Total log probability: sum of last two states at final time step\n    log_prob = jnp.logaddexp(alpha[T - 1, S - 1], alpha[T - 1, S - 2])\n    return log_prob, alpha\n\n# --- Toy example ---\nT = 12   # input length (time steps)\nV = 5    # vocab size (0=blank, 1='c', 2='a', 3='t', 4='x')\ntargets = jnp.array([1, 2, 3])  # \"c\", \"a\", \"t\"\n\n# Create random logits and convert to log-probabilities\nkey = jax.random.PRNGKey(42)\nlogits = jax.random.normal(key, (T, V))\nlog_probs = jax.nn.log_softmax(logits, axis=-1)\n\nlog_prob, alpha = ctc_forward(log_probs, targets)\nctc_loss = -log_prob\n\nprint(f\"Target sequence: {targets.tolist()} ('c', 'a', 't')\")\nprint(f\"Input length T={T}, Vocab size V={V}\")\nprint(f\"CTC log-probability: {log_prob:.4f}\")\nprint(f\"CTC loss (neg log-prob): {ctc_loss:.4f}\")\n\n# Visualise the forward variable (alpha) lattice\nfig, ax = plt.subplots(figsize=(12, 5))\n# Convert from log to linear for visualisation\nalpha_linear = jnp.exp(alpha - jnp.max(alpha))  # normalise for visibility\nim = ax.imshow(alpha_linear.T, aspect='auto', origin='lower', cmap='viridis')\nax.set_xlabel('Time step (t)')\nax.set_ylabel('Extended label index (s)')\n\nlabel_names = ['_', 'c', '_', 'a', '_', 't', '_']  # _ = blank\nax.set_yticks(range(len(label_names)))\nax.set_yticklabels(label_names)\nax.set_title(f'CTC Forward Variable (alpha lattice) | Loss = {ctc_loss:.2f}')\nplt.colorbar(im, ax=ax, label='Normalised probability')\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Build a simple encoder-decoder attention-based ASR model (a minimal LAS-like architecture) in JAX. Use a 1D convolution encoder and a single-layer decoder with dot-product attention. Run it on synthetic data and visualise the attention weights. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# --- Minimal attention-based encoder-decoder for ASR ---\n\ndef init_params(key, input_dim, hidden_dim, vocab_size):\n    \"\"\"Initialise parameters for a tiny LAS-like model.\"\"\"\n    keys = jax.random.split(key, 8)\n    scale = 0.1\n    params = {\n        # Encoder: simple linear projection (simulating conv output)\n        'enc_w': jax.random.normal(keys[0], (input_dim, hidden_dim)) * scale,\n        'enc_b': jnp.zeros(hidden_dim),\n        # Attention: query, key, value projections\n        'attn_q': jax.random.normal(keys[1], (hidden_dim, hidden_dim)) * scale,\n        'attn_k': jax.random.normal(keys[2], (hidden_dim, hidden_dim)) * scale,\n        'attn_v': jax.random.normal(keys[3], (hidden_dim, hidden_dim)) * scale,\n        # Decoder RNN (simple Elman RNN for illustration)\n        'dec_wh': jax.random.normal(keys[4], (hidden_dim, hidden_dim)) * scale,\n        'dec_wx': jax.random.normal(keys[5], (vocab_size, hidden_dim)) * scale,\n        'dec_wc': jax.random.normal(keys[6], (hidden_dim, hidden_dim)) * scale,\n        'dec_b': jnp.zeros(hidden_dim),\n        # Output projection\n        'out_w': jax.random.normal(keys[7], (hidden_dim, vocab_size)) * scale,\n        'out_b': jnp.zeros(vocab_size),\n    }\n    return params\n\ndef encode(params, x):\n    \"\"\"Encoder: linear projection (placeholder for conv/LSTM stack).\"\"\"\n    return jnp.tanh(x @ params['enc_w'] + params['enc_b'])\n\ndef attend(params, query, enc_out):\n    \"\"\"Dot-product attention over encoder outputs.\"\"\"\n    q = query @ params['attn_q']                   # (hidden,)\n    k = enc_out @ params['attn_k']                 # (T_enc, hidden)\n    v = enc_out @ params['attn_v']                 # (T_enc, hidden)\n    d_k = q.shape[-1]\n    scores = (k @ q) / jnp.sqrt(d_k)              # (T_enc,)\n    weights = jax.nn.softmax(scores)               # (T_enc,)\n    context = weights @ v                          # (hidden,)\n    return context, weights\n\ndef decode_step(params, h_prev, y_prev_onehot, enc_out):\n    \"\"\"Single decoder step: RNN + attention.\"\"\"\n    # Embed previous token\n    y_emb = y_prev_onehot @ params['dec_wx']       # (hidden,)\n    # Attend to encoder\n    context, attn_w = attend(params, h_prev, enc_out)\n    # RNN update\n    h = jnp.tanh(h_prev @ params['dec_wh'] + y_emb + context @ params['dec_wc']\n                  + params['dec_b'])\n    # Output logits\n    logits = h @ params['out_w'] + params['out_b']\n    return h, logits, attn_w\n\n# --- Setup ---\nkey = jax.random.PRNGKey(0)\ninput_dim = 40       # e.g., 40 mel bands\nhidden_dim = 64\nvocab_size = 10      # small vocab for demo\nT_enc = 30           # encoder time steps\nT_dec = 8            # decoder steps\n\nparams = init_params(key, input_dim, hidden_dim, vocab_size)\n\n# Synthetic input: random mel-like features\nkey, subkey = jax.random.split(key)\nx = jax.random.normal(subkey, (T_enc, input_dim))\n\n# Encode\nenc_out = encode(params, x)\n\n# Decode (teacher forcing with random targets)\nkey, subkey = jax.random.split(key)\ntargets = jax.random.randint(subkey, (T_dec,), 0, vocab_size)\n\nh = jnp.zeros(hidden_dim)\nall_logits = []\nall_attn = []\n\nfor t in range(T_dec):\n    y_prev = jax.nn.one_hot(targets[t] if t &gt; 0 else 0, vocab_size)\n    h, logits, attn_w = decode_step(params, h, y_prev, enc_out)\n    all_logits.append(logits)\n    all_attn.append(attn_w)\n\nall_attn = jnp.stack(all_attn)  # (T_dec, T_enc)\nall_logits = jnp.stack(all_logits)  # (T_dec, vocab_size)\n\n# --- Visualise attention weights ---\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nim = axes[0].imshow(all_attn, aspect='auto', cmap='Blues', origin='lower')\naxes[0].set_xlabel('Encoder time step')\naxes[0].set_ylabel('Decoder step')\naxes[0].set_title('Attention Weights (decoder -&gt; encoder)')\nplt.colorbar(im, ax=axes[0])\n\n# Show predicted token distribution for each decoder step\nim2 = axes[1].imshow(jax.nn.softmax(all_logits, axis=-1), aspect='auto',\n                      cmap='Oranges', origin='lower')\naxes[1].set_xlabel('Vocabulary index')\naxes[1].set_ylabel('Decoder step')\naxes[1].set_title('Output Token Probabilities')\nplt.colorbar(im2, ax=axes[1])\n\nplt.suptitle('Minimal Attention-based ASR Model (untrained)')\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Compute Word Error Rate (WER) from scratch using dynamic programming (edit distance), and evaluate multiple hypotheses against a reference. Visualise the edit distance matrix. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef compute_wer(reference, hypothesis):\n    \"\"\"\n    Compute WER using dynamic programming (Levenshtein distance on words).\n    Returns WER, number of substitutions, deletions, insertions, and the DP matrix.\n    \"\"\"\n    ref_words = reference.split()\n    hyp_words = hypothesis.split()\n    N = len(ref_words)\n    M = len(hyp_words)\n\n    # DP matrix: d[i][j] = edit distance between ref[:i] and hyp[:j]\n    d = np.zeros((N + 1, M + 1), dtype=np.int32)\n    # Backtrack matrix to count S, D, I\n    ops = np.zeros((N + 1, M + 1, 3), dtype=np.int32)  # [sub, del, ins]\n\n    for i in range(N + 1):\n        d[i][0] = i  # all deletions\n    for j in range(M + 1):\n        d[0][j] = j  # all insertions\n\n    for i in range(1, N + 1):\n        for j in range(1, M + 1):\n            if ref_words[i - 1] == hyp_words[j - 1]:\n                sub_cost = d[i - 1][j - 1]  # match, no edit\n            else:\n                sub_cost = d[i - 1][j - 1] + 1  # substitution\n            del_cost = d[i - 1][j] + 1      # deletion\n            ins_cost = d[i][j - 1] + 1      # insertion\n\n            d[i][j] = min(sub_cost, del_cost, ins_cost)\n\n    # Backtrack to count operations\n    i, j = N, M\n    S, D, I = 0, 0, 0\n    while i &gt; 0 or j &gt; 0:\n        if i &gt; 0 and j &gt; 0 and d[i][j] == d[i-1][j-1] and ref_words[i-1] == hyp_words[j-1]:\n            i -= 1; j -= 1  # correct\n        elif i &gt; 0 and j &gt; 0 and d[i][j] == d[i-1][j-1] + 1:\n            S += 1; i -= 1; j -= 1  # substitution\n        elif i &gt; 0 and d[i][j] == d[i-1][j] + 1:\n            D += 1; i -= 1  # deletion\n        elif j &gt; 0 and d[i][j] == d[i][j-1] + 1:\n            I += 1; j -= 1  # insertion\n        else:\n            break\n\n    wer = (S + D + I) / N if N &gt; 0 else 0.0\n    return wer, S, D, I, d\n\n# --- Test cases ---\nreference = \"the cat sat on the mat\"\nhypotheses = [\n    \"the cat sat on the mat\",          # perfect\n    \"the cat sit on the mat\",          # 1 substitution\n    \"the cat on the mat\",              # 1 deletion\n    \"the big cat sat on the mat\",      # 1 insertion\n    \"a dog sat in a rug\",              # multiple errors\n]\n\nprint(f\"Reference: '{reference}'\\n\")\nprint(f\"{'Hypothesis':&lt;40s} {'WER':&gt;6s} {'S':&gt;3s} {'D':&gt;3s} {'I':&gt;3s}\")\nprint(\"-\" * 60)\nresults = []\nfor hyp in hypotheses:\n    wer, S, D, I, dp = compute_wer(reference, hyp)\n    results.append((hyp, wer, S, D, I, dp))\n    print(f\"'{hyp}':&lt;40s} {wer:&gt;6.1%} {S:&gt;3d} {D:&gt;3d} {I:&gt;3d}\")\n\n# Visualise the DP matrix for the worst case\nworst = results[-1]\nhyp_words = worst[0].split()\nref_words = reference.split()\ndp_matrix = worst[5]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# DP matrix\nim = axes[0].imshow(dp_matrix, cmap='YlOrRd', origin='upper')\naxes[0].set_xticks(range(len(hyp_words) + 1))\naxes[0].set_xticklabels([''] + hyp_words, rotation=45, ha='right', fontsize=9)\naxes[0].set_yticks(range(len(ref_words) + 1))\naxes[0].set_yticklabels([''] + ref_words, fontsize=9)\naxes[0].set_xlabel('Hypothesis words')\naxes[0].set_ylabel('Reference words')\naxes[0].set_title(f'Edit Distance Matrix\\nWER = {worst[1]:.1%}')\nfor i in range(dp_matrix.shape[0]):\n    for j in range(dp_matrix.shape[1]):\n        axes[0].text(j, i, str(dp_matrix[i, j]), ha='center', va='center', fontsize=8)\nplt.colorbar(im, ax=axes[0])\n\n# WER comparison bar chart\nnames = [f'Hyp {i+1}' for i in range(len(results))]\nwers = [r[1] * 100 for r in results]\ncolors = ['#27ae60' if w == 0 else '#f39c12' if w &lt; 30 else '#e74c3c' for w in wers]\naxes[1].barh(names, wers, color=colors)\naxes[1].set_xlabel('WER (%)')\naxes[1].set_title('Word Error Rate Comparison')\nfor i, (w, r) in enumerate(zip(wers, results)):\n    axes[1].text(w + 1, i, f'{w:.0f}% (S={r[2]}, D={r[3]}, I={r[4]})',\n                 va='center', fontsize=9)\naxes[1].set_xlim(0, max(wers) * 1.4)\n\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> <li> <p>Implement SpecAugment (frequency masking and time masking) on a log-mel spectrogram and visualise the original vs. augmented versions. Generate the spectrogram from a synthetic signal. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# --- Generate synthetic log-mel spectrogram ---\nkey = jax.random.PRNGKey(42)\nfs = 16000\nduration = 2.0\nt = jnp.arange(0, duration, 1.0 / fs)\n\n# Simulate speech: chirp signal with harmonics\nf0 = 120.0\nx = sum(jnp.sin(2 * jnp.pi * f0 * k * t * (1 + 0.1 * t)) / k for k in range(1, 10))\nkey, subkey = jax.random.split(key)\nx = x + 0.05 * jax.random.normal(subkey, t.shape)\n\n# Compute log-mel spectrogram (simplified)\nframe_len = 400  # 25 ms\nhop_len = 160    # 10 ms\nn_fft = 512\nn_mels = 80\n\nn_frames = (len(x) - frame_len) // hop_len + 1\nhamming = 0.54 - 0.46 * jnp.cos(2 * jnp.pi * jnp.arange(frame_len) / (frame_len - 1))\n\nframes = jnp.stack([x[i * hop_len : i * hop_len + frame_len] for i in range(n_frames)])\nwindowed = frames * hamming\nspectra = jnp.abs(jnp.fft.rfft(windowed, n=n_fft)) ** 2\n\n# Simple mel filterbank\ndef hz_to_mel(f): return 2595 * jnp.log10(1 + f / 700)\ndef mel_to_hz(m): return 700 * (10 ** (m / 2595) - 1)\n\nmel_points = jnp.linspace(hz_to_mel(0), hz_to_mel(fs / 2), n_mels + 2)\nhz_pts = mel_to_hz(mel_points)\nbins = jnp.floor((n_fft + 1) * hz_pts / fs).astype(jnp.int32)\n\nn_freqs = n_fft // 2 + 1\nfb = jnp.zeros((n_mels, n_freqs))\nfor m in range(n_mels):\n    lo, mid, hi = int(bins[m]), int(bins[m+1]), int(bins[m+2])\n    for k in range(lo, mid):\n        if mid != lo:\n            fb = fb.at[m, k].set((k - lo) / (mid - lo))\n    for k in range(mid, hi):\n        if hi != mid:\n            fb = fb.at[m, k].set((hi - k) / (hi - mid))\n\nlog_mel = jnp.log(spectra @ fb.T + 1e-10)\n\n# --- SpecAugment ---\ndef spec_augment(spec, key, n_freq_masks=2, freq_mask_width=15,\n                 n_time_masks=2, time_mask_width=25):\n    \"\"\"Apply SpecAugment: frequency and time masking.\"\"\"\n    augmented = spec.copy()\n    T, F = spec.shape\n\n    # Frequency masking\n    for _ in range(n_freq_masks):\n        key, k1, k2 = jax.random.split(key, 3)\n        f_width = jax.random.randint(k1, (), 1, freq_mask_width + 1)\n        f_start = jax.random.randint(k2, (), 0, max(1, F - freq_mask_width))\n        mask = (jnp.arange(F) &gt;= f_start) &amp; (jnp.arange(F) &lt; f_start + f_width)\n        augmented = jnp.where(mask[None, :], 0.0, augmented)\n\n    # Time masking\n    for _ in range(n_time_masks):\n        key, k1, k2 = jax.random.split(key, 3)\n        t_width = jax.random.randint(k1, (), 1, time_mask_width + 1)\n        t_start = jax.random.randint(k2, (), 0, max(1, T - time_mask_width))\n        mask = (jnp.arange(T) &gt;= t_start) &amp; (jnp.arange(T) &lt; t_start + t_width)\n        augmented = jnp.where(mask[:, None], 0.0, augmented)\n\n    return augmented\n\nkey, subkey = jax.random.split(key)\nlog_mel_aug = spec_augment(log_mel, subkey)\n\n# --- Visualise ---\nfig, axes = plt.subplots(2, 1, figsize=(14, 8))\n\nim0 = axes[0].imshow(log_mel.T, aspect='auto', origin='lower', cmap='inferno',\n                       extent=[0, duration, 0, n_mels])\naxes[0].set_title('Original Log-Mel Spectrogram')\naxes[0].set_xlabel('Time (s)'); axes[0].set_ylabel('Mel Band')\nplt.colorbar(im0, ax=axes[0], label='Log Energy')\n\nim1 = axes[1].imshow(log_mel_aug.T, aspect='auto', origin='lower', cmap='inferno',\n                       extent=[0, duration, 0, n_mels])\naxes[1].set_title('After SpecAugment (frequency + time masking)')\naxes[1].set_xlabel('Time (s)'); axes[1].set_ylabel('Mel Band')\nplt.colorbar(im1, ax=axes[1], label='Log Energy')\n\nplt.tight_layout(); plt.show()\n</code></pre></p> </li> </ol>"},{"location":"chapter%2009%3A%20audio%20and%20speech/03.%20text%20to%20speech%20and%20voice/","title":"Text to Speech and Voice","text":"<p>Text-to-speech synthesis reverses the ASR pipeline, generating natural-sounding audio from written text. This file covers the TTS pipeline (text normalisation, G2P, acoustic models, vocoders), Tacotron, WaveNet, HiFi-GAN, voice cloning, voice conversion, and voice activity detection (VAD).</p> <ul> <li> <p>In file 01, we built the signal-processing toolkit: waveforms, spectrograms, mel filterbanks, and MFCCs. In file 02, we turned speech into text. Now we reverse the arrow: given text, synthesise natural-sounding speech. This is text-to-speech (TTS), a problem that also opens the door to voice conversion, voice cloning, and voice activity detection.</p> </li> <li> <p>Think of TTS like a stage performance. The script is the text input. A director (the acoustic model) decides how each line should sound, its pitch, timing, emphasis. The orchestra (the vocoder) then performs the score, producing the actual sound waves the audience hears. Modern neural TTS replaces the stiff, robotic delivery of rule-based systems with performances that rival human speakers.</p> </li> </ul> <p></p> <ul> <li> <p>Text-to-speech pipeline the standard TTS pipeline has four stages: (1) text normalisation, (2) phoneme conversion, (3) acoustic model, and (4) vocoder. Some modern systems collapse stages 3 and 4 into a single end-to-end model, but the conceptual decomposition remains useful.</p> </li> <li> <p>Text normalisation converts raw text into a pronounceable form. Abbreviations expand (\"Dr.\" to \"Doctor\"), numbers become words (\"1984\" to \"nineteen eighty-four\"), currency symbols are verbalised (\"$5\" to \"five dollars\"), and URLs or special characters are handled. This stage is often rule-based with language-specific grammars, though neural normalisation models exist. Errors here propagate to every downstream stage: if \"St.\" is read as \"saint\" instead of \"street\", the entire utterance is wrong.</p> </li> <li> <p>Grapheme-to-phoneme (G2P) conversion maps normalised text to a phoneme sequence. English is notoriously irregular (\"though\", \"through\", \"tough\" all use \"ough\" differently), so dictionary lookup (the CMU Pronouncing Dictionary) handles common words while a neural sequence-to-sequence model (chapter 06's encoder-decoder or chapter 07's transformer) handles out-of-vocabulary words. Languages with shallow orthographies (Spanish, Finnish) need simpler G2P. The output is typically an IPA (International Phonetic Alphabet) sequence or an equivalent internal phoneme set.</p> </li> <li> <p>Acoustic models consume the phoneme sequence and produce an intermediate acoustic representation, almost always a mel spectrogram (file 01). The mel spectrogram captures the spectral envelope at each time frame, which encodes the perceptually relevant information a vocoder needs to reconstruct the waveform. The acoustic model must decide timing (how long each phoneme lasts), pitch (fundamental frequency \\(F_0\\)), and energy (loudness).</p> </li> <li> <p>Vocoders take the mel spectrogram and produce the raw audio waveform. This is an ill-posed inversion problem: many waveforms can produce the same spectrogram because phase information was discarded. Classical vocoders (Griffin-Lim, WORLD) use iterative or signal-model approaches, but neural vocoders now dominate in quality.</p> </li> <li> <p>Vocoders: WaveNet (van den Oord et al., 2016) was the first neural vocoder to produce speech nearly indistinguishable from human recordings. It models the waveform autoregressively, predicting each sample \\(x_t\\) conditioned on all previous samples:</p> </li> </ul> \\[P(x) = \\prod_{t=1}^{T} P(x_t \\mid x_1, \\ldots, x_{t-1}, c)\\] <ul> <li> <p>where \\(c\\) is the conditioning signal (mel spectrogram). Each sample is 16-bit, so a naive softmax over 65536 values is impractical. WaveNet uses mu-law companding to reduce to 256 quantisation levels, or later variants use a mixture of logistics distribution.</p> </li> <li> <p>WaveNet's core building block is the dilated causal convolution. Causal means filter weights only look at past samples (no future leakage). Dilated means the filter skips samples with exponentially increasing gaps: dilation factors \\(1, 2, 4, 8, \\ldots, 512\\). This gives an exponentially large receptive field while keeping the parameter count linear.</p> </li> <li> <p>The gated activation for each layer is:</p> </li> </ul> \\[z = \\tanh(W_{f} \\ast x) \\odot \\sigma(W_{g} \\ast x)\\] <ul> <li> <p>where \\(W_f\\) and \\(W_g\\) are filter and gate convolution weights, \\(\\ast\\) denotes dilated causal convolution, and \\(\\odot\\) is element-wise multiplication. This gating mechanism (from chapter 06's LSTMs) allows the network to control information flow.</p> </li> <li> <p>WaveNet produces exceptional quality but is painfully slow at inference: generating one second of 24 kHz audio requires 24000 sequential forward passes. This motivated all subsequent vocoder research.</p> </li> <li> <p>WaveRNN (Kalchbrenner et al., 2018) replaces WaveNet's deep convolutional stack with a single-layer recurrent network. It splits each 16-bit sample into coarse (upper 8 bits) and fine (lower 8 bits) components, predicting each with a GRU (chapter 06). This dual softmax approach reduces computation significantly while maintaining high quality. WaveRNN is fast enough for real-time on mobile CPUs with careful kernel optimisation.</p> </li> <li> <p>WaveGlow (Prenger et al., 2019) is a flow-based vocoder that avoids autoregressive generation entirely. It uses a sequence of invertible transformations (affine coupling layers, chapter 06's normalising flows) to map a simple Gaussian distribution to the waveform distribution. Training maximises the exact log-likelihood using the change-of-variables formula:</p> </li> </ul> \\[\\log P(x) = \\log P(z) + \\sum_{i} \\log \\left| \\det \\frac{\\partial f_i}{\\partial f_{i-1}} \\right|\\] <ul> <li> <p>where \\(z = f(x)\\) is the latent variable obtained by passing \\(x\\) through the flow. At inference, a sample \\(z \\sim \\mathcal{N}(0, I)\\) is drawn and pushed through the inverted flow in a single parallel pass. WaveGlow trades model size (large networks for the coupling layers) for generation speed.</p> </li> <li> <p>HiFi-GAN (Kong et al., 2020) uses a generative adversarial network to synthesise waveforms from mel spectrograms. The generator upsamples the mel spectrogram through a series of transposed convolutions, each followed by a multi-receptive field fusion (MRF) module. The MRF module applies multiple residual blocks with different kernel sizes and dilation rates in parallel, then sums their outputs. This allows the generator to capture patterns at multiple time scales simultaneously.</p> </li> </ul> <p></p> <ul> <li> <p>HiFi-GAN uses two discriminator types. The multi-period discriminator (MPD) reshapes the 1D waveform into 2D by folding it at different periods (2, 3, 5, 7, 11), then applies 2D convolutions. This captures periodic structures at different fundamental frequencies. The multi-scale discriminator (MSD) operates on the raw waveform, 2x downsampled, and 4x downsampled versions, capturing patterns at different temporal resolutions.</p> </li> <li> <p>The training objective combines adversarial loss, mel spectrogram reconstruction loss (L1 distance between the mel spectrogram of synthesised and ground truth audio), and feature matching loss (L1 distance between intermediate discriminator features):</p> </li> </ul> \\[\\mathcal{L}_G = \\mathcal{L}_{\\text{adv}}(G) + \\lambda_{\\text{mel}} \\mathcal{L}_{\\text{mel}}(G) + \\lambda_{\\text{fm}} \\mathcal{L}_{\\text{fm}}(G)\\] <ul> <li> <p>HiFi-GAN achieves synthesis quality comparable to WaveNet while being over 1000x faster, enabling real-time generation on a single GPU.</p> </li> <li> <p>Neural source-filter (NSF) models combine traditional signal processing with neural networks. In the classical source-filter model, voiced speech is produced by a source excitation (periodic pulse train at the fundamental frequency \\(F_0\\)) passed through a vocal tract filter (the spectral envelope). NSF models replace the handcrafted filter with a neural network while keeping the explicit source signal. The input \\(F_0\\) contour provides fine pitch control that purely data-driven vocoders sometimes struggle with.</p> </li> <li> <p>Acoustic models: Tacotron (Wang et al., 2017) was the first end-to-end neural TTS system that directly converted character sequences to mel spectrograms. It uses an encoder-decoder architecture with attention (chapter 07). The encoder processes the character/phoneme sequence with a convolution bank, highway network, and bidirectional GRU. The decoder is an autoregressive GRU that predicts mel frames one at a time, using the previous frame and the attention context as input.</p> </li> <li> <p>Tacotron 2 (Shen et al., 2018) refines the architecture significantly. The encoder is a 3-layer 1D convolution stack followed by a bidirectional LSTM (chapter 06). The decoder is a 2-layer LSTM with location-sensitive attention, which conditions the attention mechanism not only on the encoder outputs and decoder state but also on the cumulative attention weights from previous steps. This prevents the common failure mode of attention skipping or repeating words.</p> </li> </ul> <p></p> <ul> <li>The location-sensitive attention energy for encoder position \\(j\\) at decoder step \\(i\\) is:</li> </ul> \\[e_{i,j} = w^T \\tanh(W_s s_{i-1} + W_h h_j + W_f f_{i,j} + b)\\] <ul> <li> <p>where \\(s_{i-1}\\) is the previous decoder state, \\(h_j\\) is the encoder output at position \\(j\\), and \\(f_{i,j}\\) is the location feature obtained by convolving the cumulative attention weights \\(\\sum_{k&lt;i} \\alpha_{k,j}\\) with a 1D convolution filter. The attention weights are \\(\\alpha_{i,j} = \\text{softmax}(e_{i,j})\\).</p> </li> <li> <p>Tacotron 2's decoder also predicts a stop token probability at each step, indicating when the mel spectrogram is complete. The output mel spectrogram is then passed to a vocoder (originally WaveNet, later replaced by HiFi-GAN or similar).</p> </li> <li> <p>The autoregressive nature of Tacotron 2 means synthesis speed is limited by the number of mel frames. For a typical 80-frame-per-second mel spectrogram, a 5-second utterance requires 400 sequential decoder steps.</p> </li> <li> <p>FastSpeech (Ren et al., 2019) solves the speed problem with a non-autoregressive acoustic model. Instead of generating mel frames sequentially, FastSpeech generates all frames in parallel. The key challenge is determining how many mel frames each phoneme should produce, which FastSpeech handles with a duration predictor.</p> </li> <li> <p>The duration predictor is a small convolutional network that predicts the integer duration (number of mel frames) for each phoneme. During training, ground-truth durations are extracted from a pre-trained autoregressive teacher model (Tacotron 2) using its attention alignments. During inference, the predicted durations are used to expand the phoneme-level hidden sequence to the frame level using a length regulator that simply repeats each phoneme's hidden representation for the predicted number of frames.</p> </li> <li> <p>FastSpeech 2 (Ren et al., 2021) improves on FastSpeech by removing the teacher-student distillation. It extracts ground-truth durations directly using forced alignment (from file 02's acoustic model frameworks) and adds explicit variance adaptors for pitch (\\(F_0\\)) and energy in addition to duration. Each adaptor is a small convolutional predictor whose output conditions the decoder:</p> </li> </ul> \\[ \\begin{aligned} \\hat{d}_i &amp;= \\text{DurationPredictor}(h_i) \\\\ \\hat{p}_i &amp;= \\text{PitchPredictor}(h_i) \\\\ \\hat{e}_i &amp;= \\text{EnergyPredictor}(h_i) \\end{aligned} \\] <ul> <li> <p>where \\(h_i\\) is the encoder hidden state for phoneme \\(i\\). At training time, ground-truth values are used; at inference, the predicted values give explicit control over prosody. This controllability is a major advantage of FastSpeech 2: adjusting pitch, speed, or energy is as simple as scaling the predictor outputs.</p> </li> <li> <p>FastSpeech 2 is typically 10-20x faster than Tacotron 2 at inference and avoids common autoregressive failure modes like word skipping, repetition, and attention collapse.</p> </li> <li> <p>VITS (Kim et al., 2021) is an end-to-end TTS model that directly generates waveforms from text, eliminating the separate vocoder stage. VITS combines a conditional variational autoencoder (chapter 06) with normalising flows and adversarial training. The posterior encoder maps ground-truth mel spectrograms to a latent space, the prior encoder maps phonemes (through a transformer-based text encoder and duration predictor) to the same latent space, and the decoder (HiFi-GAN-based) generates waveforms from latent samples.</p> </li> <li> <p>The training objective for VITS combines:</p> <ul> <li>Reconstruction loss: the VAE forces the latent distribution to encode acoustic information</li> <li>KL divergence: aligns the text-conditioned prior with the audio-conditioned posterior</li> <li>Adversarial loss: discriminators ensure waveform quality</li> <li>Duration loss: trains the stochastic duration predictor</li> </ul> </li> <li> <p>VITS produces higher quality than two-stage systems (FastSpeech 2 + HiFi-GAN) because the acoustic model and vocoder are jointly optimised, avoiding the mismatch between predicted and ground-truth mel spectrograms that degrades two-stage systems.</p> </li> <li> <p>VALL-E (Wang et al., 2023) radically reframes TTS as a language modelling problem over discrete audio tokens. It uses a neural audio codec (EnCodec) to represent speech as a sequence of discrete codes from multiple codebook levels. Given a text prompt and a 3-second enrollment utterance (also encoded as discrete tokens), VALL-E uses a transformer language model to predict the audio tokens autoregressively.</p> </li> <li> <p>VALL-E uses two models: an autoregressive (AR) model that generates the first codebook level token-by-token, and a non-autoregressive (NAR) model that predicts the remaining codebook levels in parallel, conditioned on the first level and each other. This codec language model approach enables remarkable zero-shot voice cloning: a 3-second sample is enough to reproduce a speaker's voice, timbre, and even emotional tone.</p> </li> <li> <p>StyleTTS (Li et al., 2022) and StyleTTS 2 disentangle speech into content and style components. A style encoder extracts a style vector from reference audio, capturing speaker identity, prosody, and recording conditions. During inference, style can be sampled from a learned prior distribution or transferred from a reference utterance. StyleTTS 2 uses diffusion models (chapter 08) for the style prior, generating diverse and natural prosody.</p> </li> <li> <p>Kokoro (2024) is a lightweight, high-quality open-source TTS model notable for its small size (~82M parameters) and impressive naturalness. It uses a StyleTTS 2-inspired architecture with a diffusion-based style prior and a fine-tuned ISTFTNet vocoder that directly predicts STFT coefficients (from file 01) rather than raw waveform samples. Despite being a fraction of the size of models like VALL-E, Kokoro achieves near-human naturalness for English, Japanese, French, Korean, and Chinese, demonstrating that carefully curated training data and efficient architecture design can compete with brute-force scale. Kokoro's small footprint makes it practical for local and edge deployment.</p> </li> <li> <p>Orpheus (Canopy Labs, 2025) is a family of open-source TTS models (1B and 3B parameters) built on the codec language model paradigm pioneered by VALL-E. Orpheus takes the idea further with an LLM backbone (fine-tuned Llama 3) that generates SNAC audio codec tokens directly. Its standout feature is human-like emotional expressiveness: it handles laughter, sighs, hesitations, and affective prosody with remarkable naturalness. Orpheus can be prompted with tags like <code>[laugh]</code> or <code>[sigh]</code> in the input text, giving fine-grained control over paralinguistic expression.</p> </li> <li> <p>Dia (Nari Labs, 2025) is an open-source dialogue TTS model that generates realistic multi-speaker conversations from a single text transcript. Built on a 1.6B-parameter encoder-decoder transformer, Dia handles turn-taking, speaker-specific voices, and non-verbal cues (laughter, pauses) within a conversation. It also supports voice cloning from a short audio prompt, enabling zero-shot speaker generation in dialogue context.</p> </li> <li> <p>Sesame CSM (Conversational Speech Model, 2025) focuses on natural multi-turn conversational speech. Rather than optimising for reading-style TTS, Sesame models the dynamics of real conversation: backchannels (\"uh huh\"), interruptions, rhythm changes between speakers, and emotional responsiveness. The model uses a transformer backbone conditioned on conversational context (both text and audio history), producing speech that adapts its style to the flow of the dialogue.</p> </li> <li> <p>Fish Speech (Fish Audio, 2024) is an open-source TTS system that uses a dual autoregressive architecture: a large language model generates semantic tokens from text, and a smaller model converts these to VQGAN acoustic tokens, which are decoded into waveforms by a vocoder. Fish Speech supports zero-shot voice cloning from a 10-15 second reference and achieves low latency suitable for real-time applications. Its modular design allows swapping components (e.g., different vocoders) independently.</p> </li> <li> <p>ChatTTS (2024) is an open-source conversational TTS model designed for dialogue applications like chatbots and virtual assistants. It generates natural, conversational-sounding speech with fine-grained control over prosodic features (laughter, pauses, filler words) using special tokens embedded in the text input. ChatTTS supports mixed Chinese-English synthesis and multi-speaker generation.</p> </li> <li> <p>Bark (Suno, 2023) is a transformer-based open-source model that generates speech, music, and sound effects from text prompts. It uses a three-stage pipeline of transformer models (text \u2192 semantic tokens \u2192 coarse acoustic tokens \u2192 fine acoustic tokens) and supports voice cloning, multilingual synthesis, and non-speech audio like music and ambient sounds. Bark's generality comes at the cost of controllability \u2014 it is less precise than dedicated TTS systems but more flexible.</p> </li> <li> <p>Parler-TTS (Hugging Face, 2024) takes a natural language description approach to voice control: instead of requiring a reference audio clip for style, the user provides a text description like \"a female speaker with a warm, expressive voice in a quiet room.\" Parler-TTS is trained on annotated speech data where each utterance is paired with a natural language description of the speaking style, enabling intuitive control without any reference audio.</p> </li> <li> <p>Neuphonic is an API-based TTS platform optimised for ultra-low-latency speech synthesis, targeting real-time voice agents and conversational AI applications. It achieves time-to-first-audio under 100 ms through a streaming architecture that begins generating audio before the full input text is available. Neuphonic focuses on the deployment and latency optimisation layer rather than novel model architecture, providing production-grade infrastructure around modern neural TTS.</p> </li> <li> <p>KittenTTS is a compact, fast TTS model designed for efficiency and low-resource deployment. It prioritises minimal latency and small model size for edge and embedded applications, trading some naturalness for real-time performance on CPUs and mobile devices.</p> </li> <li> <p>The modern TTS landscape is bifurcating into two paradigms: (1) codec language models (VALL-E, Orpheus, Fish Speech) that treat speech generation as next-token prediction over discrete audio codes, leveraging the scaling laws of LLMs; and (2) flow/diffusion-based models (VITS, StyleTTS 2, Kokoro) that generate continuous mel spectrograms or waveforms through iterative refinement. Codec LMs excel at zero-shot cloning and expressiveness; flow/diffusion models tend to be smaller and faster. Both are rapidly converging toward human-level naturalness.</p> </li> <li> <p>Prosody modelling controls the \"music\" of speech: pitch, duration, energy, rhythm, and intonation. Without good prosody, synthesised speech sounds flat and robotic even if individual phonemes are clear. Think of prosody as the difference between a monotone GPS voice and an expressive audiobook narrator.</p> </li> <li> <p>Pitch (fundamental frequency \\(F_0\\)) is the perceived highness or lowness of speech. It rises at the end of questions, falls at the end of statements, and varies continuously during emotional speech. \\(F_0\\) is extracted from audio using algorithms like CREPE (a neural pitch tracker) or YIN (autocorrelation-based, from file 01). In TTS, pitch is either predicted by the acoustic model (FastSpeech 2's pitch predictor) or implicitly learned (Tacotron 2).</p> </li> <li> <p>Duration determines the speaking rate and rhythm. Stressed syllables are longer, function words are shortened, and pauses mark phrase boundaries. Duration modelling is explicit in non-autoregressive models (FastSpeech) and implicit in autoregressive models (Tacotron's attention alignment determines duration).</p> </li> <li> <p>Energy (loudness) carries emphasis. \"I didn't say HE stole it\" vs \"I didn't say he STOLE it\" have different meanings conveyed entirely through energy patterns.</p> </li> <li> <p>Style embeddings capture higher-level prosodic patterns. The Global Style Token (GST) framework (Wang et al., 2018) learns a bank of style tokens (soft attention over a learned set of embeddings) that capture speaking styles like \"excited\", \"sad\", or \"whispering\". The style embedding is extracted from a reference utterance and added to the encoder output, allowing style transfer at inference.</p> </li> <li> <p>Voice conversion (VC) changes the speaker identity of an utterance while preserving the linguistic content. Imagine recording yourself and having the output sound like a specific target speaker. VC requires disentangling speaker identity from content.</p> </li> </ul> <p></p> <ul> <li> <p>Speaker embeddings (detailed further in file 04) encode speaker identity as a fixed-dimensional vector. These can come from a pre-trained speaker verification model (x-vectors, ECAPA-TDNN). In VC, the source speech is encoded into a content representation that is speaker-independent, then decoded with the target speaker embedding.</p> </li> <li> <p>Disentangled representations separate speech into independent factors: content (phonemes), speaker identity, pitch, and rhythm. Approaches include:</p> <ul> <li>Information bottleneck: compress the content representation so tightly that speaker information is lost (AutoVC)</li> <li>Adversarial training: train a speaker classifier on the content representation and use gradient reversal to remove speaker information</li> <li>Vector quantisation: VQ-VAE forces the content through a discrete bottleneck, which naturally strips speaker identity (since codebook entries represent phonetic categories, not speaker traits)</li> </ul> </li> <li> <p>Voice cloning synthesises speech in a target speaker's voice. Multi-speaker TTS trains on data from many speakers, conditioning the model on a speaker embedding. At inference, a new speaker's embedding is extracted from enrollment audio and used to condition generation.</p> </li> <li> <p>Few-shot voice cloning adapts to a new speaker using a small amount of data (a few minutes). The speaker encoder extracts an embedding from the enrollment audio, and the TTS model generates speech conditioned on this embedding. This is the approach used in SV2TTS (Jia et al., 2018): a separately trained speaker encoder, a Tacotron 2 synthesiser conditioned on the speaker embedding, and a WaveRNN vocoder.</p> </li> <li> <p>Zero-shot voice cloning requires no adaptation at all: a single short utterance (3-30 seconds) is enough. VALL-E achieves this by treating the enrollment audio as a prompt for the language model. The model learns to continue generating in the same voice because it was trained on large-scale multi-speaker data where voice consistency within an utterance is the statistical norm.</p> </li> <li> <p>Voice activity detection (VAD) answers a simple binary question at each time frame: is someone speaking or not? Despite its simplicity, VAD is a critical preprocessing step for ASR (file 02), speaker diarisation (file 04), and noise reduction (file 05). A good VAD reduces computation by skipping silence and improves accuracy by preventing noise from being processed as speech.</p> </li> <li> <p>Classical VAD uses energy thresholding (speech is louder than silence), zero-crossing rate (speech has characteristic crossing patterns), and spectral features. These fail in noisy environments where the signal-to-noise ratio is low.</p> </li> <li> <p>Neural VAD models treat the problem as frame-level binary classification. A small RNN or CNN takes acoustic features (log mel energies from file 01) and predicts speech/non-speech probabilities.</p> </li> <li> <p>WebRTC VAD (Google) is a classic lightweight VAD using a GMM-based classifier on simple spectral features. It operates at four aggressiveness levels (0-3) and is extremely fast, but struggles with music, non-speech vocalisations, and low-SNR environments. It remains widely used as a baseline due to its zero-dependency simplicity.</p> </li> <li> <p>Silero VAD (Silero Team, 2021) is the de facto standard neural VAD for production use. Its architecture is a small stack of depthwise separable 1D convolutions (chapter 08's MobileNet idea applied to audio) followed by a single LSTM layer for temporal context, with a final linear head producing a speech probability per frame. The entire model is under 2MB (~1M parameters) and processes audio in 30-100 ms chunks.</p> <ul> <li>Input: raw 16 kHz audio (no manual feature extraction \u2014 the convolutional front-end learns its own features from the waveform directly).</li> <li>Windowed stateful inference: the LSTM hidden state carries over between chunks, so the model handles streaming audio without reprocessing the full history. Each call processes a 30, 60, or 100 ms chunk and returns a speech probability in \\([0, 1]\\).</li> <li>Adaptive thresholding: rather than a single fixed threshold, Silero VAD uses separate start and end thresholds with a minimum speech/silence duration, preventing rapid toggling on noisy boundaries. A speech segment must exceed the start threshold for a minimum duration before being confirmed, and silence must persist below the end threshold before the segment is closed.</li> <li>Performance: Silero VAD runs at 1-2% real-time factor on CPU (processing 1 second of audio takes ~10-20 ms), making it suitable for edge devices, mobile phones, and real-time pipelines. It significantly outperforms WebRTC VAD on noisy and music-heavy audio while remaining small enough for on-device deployment.</li> <li>Silero VAD is commonly used as the front-end for Whisper (file 02) to segment long audio into utterance-level chunks before transcription, and for speaker diarisation pipelines (file 04) to identify speech regions before extracting speaker embeddings.</li> </ul> </li> <li> <p>Acoustic activity detection (AAD) generalises VAD to detect any acoustic activity, not just speech. This is useful in smart home devices, security systems, and wildlife monitoring. AAD models detect events like glass breaking, dogs barking, or alarms, often using the audio classification frameworks described in file 04.</p> </li> <li> <p>Evaluation metrics for TTS measure both objective quality and subjective naturalness:</p> <ul> <li>Mean Opinion Score (MOS): human listeners rate naturalness on a 1-5 scale. The gold standard, but expensive and slow.</li> <li>Mel cepstral distortion (MCD): measures the distance between synthesised and reference mel cepstra. Lower is better, but does not always correlate with perception.</li> <li>PESQ / POLQA: standardised perceptual evaluation metrics originally designed for telephony.</li> <li>Speaker similarity: cosine similarity between speaker embeddings of synthesised and reference audio (relevant for voice cloning).</li> <li>Intelligibility: measured by feeding synthesised audio through an ASR system (file 02) and computing WER.</li> </ul> </li> </ul>"},{"location":"chapter%2009%3A%20audio%20and%20speech/03.%20text%20to%20speech%20and%20voice/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ul> <li>Task 1: Griffin-Lim vocoder from mel spectrogram. Implement the Griffin-Lim iterative phase reconstruction algorithm to convert a mel spectrogram back to a waveform. This demonstrates the vocoder problem and why neural vocoders are needed.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Generate a synthetic waveform (sum of harmonics simulating a vowel)\nsr = 16000\nduration = 1.0\nt = jnp.linspace(0, duration, int(sr * duration))\nf0 = 220.0  # fundamental frequency\nwaveform = (\n    0.6 * jnp.sin(2 * jnp.pi * f0 * t) +\n    0.3 * jnp.sin(2 * jnp.pi * 2 * f0 * t) +\n    0.1 * jnp.sin(2 * jnp.pi * 3 * f0 * t)\n)\n\n# Compute STFT\nn_fft = 1024\nhop_length = 256\nwindow = jnp.hanning(n_fft)\n\ndef stft(signal, n_fft, hop_length, window):\n    \"\"\"Compute Short-Time Fourier Transform.\"\"\"\n    n_frames = 1 + (len(signal) - n_fft) // hop_length\n    frames = jnp.stack([\n        signal[i * hop_length : i * hop_length + n_fft] * window\n        for i in range(n_frames)\n    ])\n    return jnp.fft.rfft(frames, n=n_fft)\n\ndef istft(stft_matrix, hop_length, window, length):\n    \"\"\"Compute inverse STFT with overlap-add.\"\"\"\n    n_fft = (stft_matrix.shape[1] - 1) * 2\n    n_frames = stft_matrix.shape[0]\n    frames = jnp.fft.irfft(stft_matrix, n=n_fft)\n    frames = frames * window[None, :]\n    output = jnp.zeros(length)\n    for i in range(n_frames):\n        start = i * hop_length\n        end = start + n_fft\n        if end &lt;= length:\n            output = output.at[start:end].add(frames[i])\n    return output\n\n# Forward STFT\nS = stft(waveform, n_fft, hop_length, window)\nmagnitude = jnp.abs(S)\n\n# Mel filterbank\nn_mels = 80\nmel_low = 0.0\nmel_high = 2595 * jnp.log10(1 + (sr / 2) / 700)\nmel_points = jnp.linspace(mel_low, mel_high, n_mels + 2)\nhz_points = 700 * (10 ** (mel_points / 2595) - 1)\nfreq_bins = jnp.floor((n_fft + 1) * hz_points / sr).astype(int)\n\nmel_filterbank = jnp.zeros((n_mels, n_fft // 2 + 1))\nfor m in range(n_mels):\n    f_left = freq_bins[m]\n    f_center = freq_bins[m + 1]\n    f_right = freq_bins[m + 2]\n    for k in range(f_left, f_center):\n        mel_filterbank = mel_filterbank.at[m, k].set(\n            (k - f_left) / max(f_center - f_left, 1)\n        )\n    for k in range(f_center, f_right):\n        mel_filterbank = mel_filterbank.at[m, k].set(\n            (f_right - k) / max(f_right - f_center, 1)\n        )\n\n# To mel and back (pseudo-inverse)\nmel_spec = magnitude @ mel_filterbank.T\nmagnitude_reconstructed = mel_spec @ jnp.linalg.pinv(mel_filterbank.T)\nmagnitude_reconstructed = jnp.maximum(magnitude_reconstructed, 1e-7)\n\n# Griffin-Lim algorithm\ndef griffin_lim(magnitude, n_iter, hop_length, window, signal_length):\n    \"\"\"Iterative phase reconstruction.\"\"\"\n    n_fft = (magnitude.shape[1] - 1) * 2\n    key = jax.random.PRNGKey(42)\n    phase = jax.random.uniform(key, magnitude.shape, minval=-jnp.pi, maxval=jnp.pi)\n\n    for _ in range(n_iter):\n        complex_spec = magnitude * jnp.exp(1j * phase)\n        signal = istft(complex_spec, hop_length, window, signal_length)\n        reanalysis = stft(signal, n_fft, hop_length, window)\n        phase = jnp.angle(reanalysis)\n\n    complex_spec = magnitude * jnp.exp(1j * phase)\n    return istft(complex_spec, hop_length, window, signal_length)\n\nreconstructed = griffin_lim(magnitude_reconstructed, n_iter=60, hop_length=hop_length,\n                            window=window, signal_length=len(waveform))\n\n# Plot comparison\nfig, axes = plt.subplots(3, 1, figsize=(12, 8))\n\naxes[0].plot(t[:1000], waveform[:1000], color='#3498db', linewidth=0.8)\naxes[0].set_title('Original Waveform')\naxes[0].set_ylabel('Amplitude')\n\naxes[1].imshow(jnp.log1p(mel_spec.T), aspect='auto', origin='lower', cmap='magma')\naxes[1].set_title('Mel Spectrogram (intermediate representation)')\naxes[1].set_ylabel('Mel bin')\n\naxes[2].plot(t[:1000], reconstructed[:1000], color='#e74c3c', linewidth=0.8)\naxes[2].set_title('Griffin-Lim Reconstructed Waveform (60 iterations)')\naxes[2].set_xlabel('Time (s)')\naxes[2].set_ylabel('Amplitude')\n\nplt.tight_layout()\nplt.show()\n\n# Measure reconstruction error\nmse = jnp.mean((waveform[:len(reconstructed)] - reconstructed[:len(waveform)]) ** 2)\nprint(f\"MSE between original and reconstructed: {mse:.6f}\")\nprint(\"Note: phase information loss through mel inversion causes artifacts.\")\n</code></pre> <ul> <li>Task 2: Duration predictor (FastSpeech-style). Train a small convolutional duration predictor that maps phoneme embeddings to durations. This is the core component enabling non-autoregressive TTS.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\n# Simulate phoneme sequences with ground-truth durations\n# In real TTS, durations come from forced alignment or teacher attention\ndef generate_synthetic_data(key, n_samples=200, max_phonemes=30, embed_dim=64):\n    \"\"\"Generate synthetic phoneme embeddings and durations.\"\"\"\n    keys = jr.split(key, 4)\n    lengths = jr.randint(keys[0], (n_samples,), 5, max_phonemes)\n\n    all_embeddings = []\n    all_durations = []\n    all_masks = []\n\n    for i in range(n_samples):\n        L = int(lengths[i])\n        emb = jr.normal(keys[1], (max_phonemes, embed_dim))\n        # Durations: vowels (even indices) are longer, consonants shorter\n        base_dur = jnp.where(jnp.arange(max_phonemes) % 2 == 0, 8.0, 4.0)\n        noise = jr.normal(jr.fold_in(keys[2], i), (max_phonemes,)) * 1.5\n        dur = jnp.clip(base_dur + noise, 1.0, 20.0).astype(jnp.float32)\n        mask = (jnp.arange(max_phonemes) &lt; L).astype(jnp.float32)\n\n        all_embeddings.append(emb)\n        all_durations.append(dur * mask)\n        all_masks.append(mask)\n\n    return (jnp.stack(all_embeddings), jnp.stack(all_durations),\n            jnp.stack(all_masks))\n\nkey = jr.PRNGKey(42)\nembeddings, durations, masks = generate_synthetic_data(key)\n\n# Duration predictor: 2-layer 1D convolution + linear projection\ndef init_duration_predictor(key, embed_dim=64, hidden_dim=128, kernel_size=3):\n    \"\"\"Initialise duration predictor weights.\"\"\"\n    keys = jr.split(key, 4)\n    scale1 = jnp.sqrt(2.0 / (embed_dim * kernel_size))\n    scale2 = jnp.sqrt(2.0 / (hidden_dim * kernel_size))\n    params = {\n        'conv1_w': jr.normal(keys[0], (kernel_size, embed_dim, hidden_dim)) * scale1,\n        'conv1_b': jnp.zeros(hidden_dim),\n        'conv2_w': jr.normal(keys[1], (kernel_size, hidden_dim, hidden_dim)) * scale2,\n        'conv2_b': jnp.zeros(hidden_dim),\n        'linear_w': jr.normal(keys[2], (hidden_dim, 1)) * jnp.sqrt(2.0 / hidden_dim),\n        'linear_b': jnp.zeros(1),\n    }\n    return params\n\ndef duration_predictor(params, x):\n    \"\"\"Predict log-durations from phoneme embeddings. x: (batch, seq, embed).\"\"\"\n    # Conv layer 1 with ReLU\n    h = jax.lax.conv_general_dilated(\n        x.transpose(0, 2, 1),  # (batch, embed, seq)\n        params['conv1_w'].transpose(2, 1, 0),  # (out, in, kernel)\n        window_strides=(1,), padding='SAME'\n    ).transpose(0, 2, 1) + params['conv1_b']  # back to (batch, seq, hidden)\n    h = jax.nn.relu(h)\n\n    # Conv layer 2 with ReLU\n    h = jax.lax.conv_general_dilated(\n        h.transpose(0, 2, 1),\n        params['conv2_w'].transpose(2, 1, 0),\n        window_strides=(1,), padding='SAME'\n    ).transpose(0, 2, 1) + params['conv2_b']\n    h = jax.nn.relu(h)\n\n    # Linear projection to scalar\n    log_dur = (h @ params['linear_w'] + params['linear_b']).squeeze(-1)\n    return log_dur\n\n# Loss: MSE on log-durations (standard in FastSpeech)\ndef loss_fn(params, embeddings, durations, masks):\n    log_dur_pred = duration_predictor(params, embeddings)\n    log_dur_true = jnp.log(jnp.clip(durations, 1.0, None))\n    sq_err = (log_dur_pred - log_dur_true) ** 2 * masks\n    return jnp.sum(sq_err) / jnp.sum(masks)\n\ngrad_fn = jax.jit(jax.value_and_grad(loss_fn))\n\n# Training loop\nparams = init_duration_predictor(jr.PRNGKey(0))\nlr = 1e-3\nlosses = []\n\nfor epoch in range(300):\n    loss_val, grads = grad_fn(params, embeddings, durations, masks)\n    params = jax.tree.map(lambda p, g: p - lr * g, params, grads)\n    losses.append(float(loss_val))\n\n# Evaluate on a sample\nlog_dur_pred = duration_predictor(params, embeddings[:1])\ndur_pred = jnp.exp(log_dur_pred[0])\ndur_true = durations[0]\nmask = masks[0]\nvalid_len = int(jnp.sum(mask))\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(losses, color='#3498db', linewidth=1.5)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('MSE Loss (log-duration)')\naxes[0].set_title('Duration Predictor Training')\naxes[0].set_yscale('log')\n\nx_pos = jnp.arange(valid_len)\nwidth = 0.35\naxes[1].bar(x_pos - width/2, dur_true[:valid_len], width, color='#27ae60',\n            label='Ground truth', alpha=0.8)\naxes[1].bar(x_pos + width/2, dur_pred[:valid_len], width, color='#e74c3c',\n            label='Predicted', alpha=0.8)\naxes[1].set_xlabel('Phoneme index')\naxes[1].set_ylabel('Duration (frames)')\naxes[1].set_title('Duration Prediction vs Ground Truth')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre> <ul> <li>Task 3: Simple neural vocoder with upsampling convolutions. Build a minimal HiFi-GAN-style generator that upsamples a mel spectrogram to a waveform using transposed convolutions and residual blocks.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\ndef init_residual_block(key, channels, kernel_size, dilation):\n    \"\"\"Initialise a dilated residual convolution block.\"\"\"\n    k1, k2 = jr.split(key)\n    scale = jnp.sqrt(2.0 / (channels * kernel_size))\n    return {\n        'conv1_w': jr.normal(k1, (kernel_size, channels, channels)) * scale,\n        'conv1_b': jnp.zeros(channels),\n        'conv2_w': jr.normal(k2, (kernel_size, channels, channels)) * scale,\n        'conv2_b': jnp.zeros(channels),\n        'dilation': dilation\n    }\n\ndef residual_block(params, x):\n    \"\"\"x: (batch, time, channels). Dilated conv residual block with LeakyReLU.\"\"\"\n    h = jax.nn.leaky_relu(x, negative_slope=0.1)\n    # Simplified: use standard conv (dilation handled conceptually)\n    h = jax.lax.conv_general_dilated(\n        h.transpose(0, 2, 1),\n        params['conv1_w'].transpose(2, 1, 0),\n        window_strides=(1,),\n        padding='SAME',\n        rhs_dilation=(params['dilation'],)\n    ).transpose(0, 2, 1) + params['conv1_b']\n    h = jax.nn.leaky_relu(h, negative_slope=0.1)\n    h = jax.lax.conv_general_dilated(\n        h.transpose(0, 2, 1),\n        params['conv2_w'].transpose(2, 1, 0),\n        window_strides=(1,),\n        padding='SAME'\n    ).transpose(0, 2, 1) + params['conv2_b']\n    return x + h\n\ndef init_generator(key, n_mels=80, upsample_rates=(8, 8, 4),\n                   channels=128):\n    \"\"\"Initialise a minimal HiFi-GAN-style generator.\"\"\"\n    keys = jr.split(key, 10)\n    params = {}\n\n    # Input projection: mel bins -&gt; channels\n    params['input_w'] = jr.normal(keys[0], (7, n_mels, channels)) * 0.02\n    params['input_b'] = jnp.zeros(channels)\n\n    # Upsample blocks (transposed convolutions)\n    in_ch = channels\n    for i, rate in enumerate(upsample_rates):\n        k_size = rate * 2\n        scale = jnp.sqrt(2.0 / (in_ch * k_size))\n        out_ch = in_ch // 2\n        params[f'up{i}_w'] = jr.normal(keys[i+1], (k_size, in_ch, out_ch)) * scale\n        params[f'up{i}_b'] = jnp.zeros(out_ch)\n        # Residual blocks at each scale\n        params[f'res{i}_0'] = init_residual_block(jr.fold_in(keys[i+4], 0),\n                                                    out_ch, 3, 1)\n        params[f'res{i}_1'] = init_residual_block(jr.fold_in(keys[i+4], 1),\n                                                    out_ch, 3, 3)\n        in_ch = out_ch\n\n    # Output projection to mono waveform\n    params['output_w'] = jr.normal(keys[8], (7, in_ch, 1)) * 0.02\n    params['output_b'] = jnp.zeros(1)\n    params['upsample_rates'] = upsample_rates\n\n    return params\n\ndef generator_forward(params, mel):\n    \"\"\"mel: (batch, time, n_mels) -&gt; waveform: (batch, time * prod(rates), 1).\"\"\"\n    # Input projection\n    h = jax.lax.conv_general_dilated(\n        mel.transpose(0, 2, 1),\n        params['input_w'].transpose(2, 1, 0),\n        window_strides=(1,), padding='SAME'\n    ).transpose(0, 2, 1) + params['input_b']\n\n    for i, rate in enumerate(params['upsample_rates']):\n        h = jax.nn.leaky_relu(h, negative_slope=0.1)\n        # Upsample via transposed convolution\n        k_size = rate * 2\n        h = jax.lax.conv_transpose(\n            h.transpose(0, 2, 1),\n            params[f'up{i}_w'].transpose(2, 1, 0),\n            strides=(rate,),\n            padding='SAME'\n        ).transpose(0, 2, 1) + params[f'up{i}_b']\n        # Residual blocks\n        h = residual_block(params[f'res{i}_0'], h)\n        h = residual_block(params[f'res{i}_1'], h)\n\n    h = jax.nn.leaky_relu(h, negative_slope=0.1)\n    out = jax.lax.conv_general_dilated(\n        h.transpose(0, 2, 1),\n        params['output_w'].transpose(2, 1, 0),\n        window_strides=(1,), padding='SAME'\n    ).transpose(0, 2, 1) + params['output_b']\n\n    return jnp.tanh(out)\n\n# Create a synthetic mel spectrogram (simulating a vowel)\nn_mels = 80\nn_frames = 50\nmel = jnp.zeros((1, n_frames, n_mels))\n# Add energy in low-frequency mel bins (simulating formants)\nmel = mel.at[:, :, 5:15].set(1.0)\nmel = mel.at[:, :, 20:25].set(0.6)\n\n# Initialise and run generator\nkey = jr.PRNGKey(42)\nparams = init_generator(key, n_mels=n_mels, upsample_rates=(8, 8, 4),\n                         channels=128)\nwaveform = generator_forward(params, mel)\n\nprint(f\"Input mel shape:  {mel.shape}\")\nprint(f\"Output waveform shape: {waveform.shape}\")\nprint(f\"Upsample factor: {8 * 8 * 4} = {8*8*4}x\")\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 6))\n\naxes[0].imshow(mel[0].T, aspect='auto', origin='lower', cmap='magma')\naxes[0].set_title('Input Mel Spectrogram')\naxes[0].set_ylabel('Mel bin')\naxes[0].set_xlabel('Frame')\n\nwaveform_np = waveform[0, :, 0]\naxes[1].plot(waveform_np[:2000], color='#9b59b6', linewidth=0.5)\naxes[1].set_title('Generator Output Waveform (untrained - random noise)')\naxes[1].set_ylabel('Amplitude')\naxes[1].set_xlabel('Sample')\n\nplt.tight_layout()\nplt.show()\nprint(\"Note: The output is noise because the generator is untrained.\")\nprint(\"In practice, adversarial + mel loss training shapes this into speech.\")\n</code></pre> <ul> <li>Task 4: Voice activity detection with a simple RNN. Train a small GRU-based VAD model on synthetic audio features to classify frames as speech or silence.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\n# Generate synthetic log-mel energy features with speech/silence labels\ndef generate_vad_data(key, n_sequences=100, n_frames=200, n_features=40):\n    \"\"\"Simulate log-mel features: speech regions are higher energy with structure.\"\"\"\n    keys = jr.split(key, 5)\n    all_features = []\n    all_labels = []\n\n    for i in range(n_sequences):\n        k = jr.fold_in(keys[0], i)\n        k1, k2, k3 = jr.split(k, 3)\n\n        # Random speech/silence pattern\n        label = jnp.zeros(n_frames)\n        n_segments = jr.randint(k1, (), 2, 6)\n        for seg in range(int(n_segments)):\n            start = jr.randint(jr.fold_in(k2, seg), (), 0, n_frames - 20)\n            length = jr.randint(jr.fold_in(k3, seg), (), 10, 50)\n            end = jnp.minimum(start + length, n_frames)\n            label = label.at[int(start):int(end)].set(1.0)\n\n        # Features: speech frames have higher energy + spectral structure\n        noise = jr.normal(jr.fold_in(keys[1], i), (n_frames, n_features)) * 0.3\n        speech_pattern = jnp.outer(label, jnp.exp(-jnp.arange(n_features) / 15.0))\n        features = speech_pattern * 2.0 + noise + 0.1\n\n        all_features.append(features)\n        all_labels.append(label)\n\n    return jnp.stack(all_features), jnp.stack(all_labels)\n\nkey = jr.PRNGKey(123)\nfeatures, labels = generate_vad_data(key)\ntrain_features, train_labels = features[:80], labels[:80]\ntest_features, test_labels = features[80:], labels[80:]\n\n# Simple GRU-based VAD model\ndef init_vad_model(key, input_dim=40, hidden_dim=64):\n    keys = jr.split(key, 6)\n    scale_ih = jnp.sqrt(2.0 / input_dim)\n    scale_hh = jnp.sqrt(2.0 / hidden_dim)\n    return {\n        'W_z': jr.normal(keys[0], (input_dim, hidden_dim)) * scale_ih,\n        'U_z': jr.normal(keys[1], (hidden_dim, hidden_dim)) * scale_hh,\n        'b_z': jnp.zeros(hidden_dim),\n        'W_r': jr.normal(keys[2], (input_dim, hidden_dim)) * scale_ih,\n        'U_r': jr.normal(keys[3], (hidden_dim, hidden_dim)) * scale_hh,\n        'b_r': jnp.zeros(hidden_dim),\n        'W_h': jr.normal(keys[4], (input_dim, hidden_dim)) * scale_ih,\n        'U_h': jr.normal(keys[5], (hidden_dim, hidden_dim)) * scale_hh,\n        'b_h': jnp.zeros(hidden_dim),\n        'W_out': jr.normal(jr.fold_in(keys[0], 99), (hidden_dim, 1)) * 0.1,\n        'b_out': jnp.zeros(1),\n    }\n\ndef gru_step(params, h, x):\n    \"\"\"Single GRU step.\"\"\"\n    z = jax.nn.sigmoid(x @ params['W_z'] + h @ params['U_z'] + params['b_z'])\n    r = jax.nn.sigmoid(x @ params['W_r'] + h @ params['U_r'] + params['b_r'])\n    h_tilde = jnp.tanh(x @ params['W_h'] + (r * h) @ params['U_h'] + params['b_h'])\n    h_new = (1 - z) * h + z * h_tilde\n    return h_new\n\ndef vad_forward(params, x):\n    \"\"\"x: (batch, time, features) -&gt; logits: (batch, time).\"\"\"\n    batch_size, n_frames, _ = x.shape\n    hidden_dim = params['W_z'].shape[1]\n    h = jnp.zeros((batch_size, hidden_dim))\n\n    outputs = []\n    for t in range(n_frames):\n        h = gru_step(params, h, x[:, t, :])\n        logit = (h @ params['W_out'] + params['b_out']).squeeze(-1)\n        outputs.append(logit)\n\n    return jnp.stack(outputs, axis=1)\n\ndef bce_loss(params, features, labels):\n    \"\"\"Binary cross-entropy loss for VAD.\"\"\"\n    logits = vad_forward(params, features)\n    probs = jax.nn.sigmoid(logits)\n    probs = jnp.clip(probs, 1e-7, 1 - 1e-7)\n    loss = -(labels * jnp.log(probs) + (1 - labels) * jnp.log(1 - probs))\n    return jnp.mean(loss)\n\ngrad_fn = jax.jit(jax.value_and_grad(bce_loss))\n\n# Training\nparams = init_vad_model(jr.PRNGKey(0))\nlr = 5e-3\nlosses = []\n\nfor epoch in range(200):\n    loss_val, grads = grad_fn(params, train_features, train_labels)\n    params = jax.tree.map(lambda p, g: p - lr * g, params, grads)\n    losses.append(float(loss_val))\n    if epoch % 50 == 0:\n        print(f\"Epoch {epoch}: loss = {loss_val:.4f}\")\n\n# Evaluate on test set\ntest_logits = vad_forward(params, test_features)\ntest_preds = (jax.nn.sigmoid(test_logits) &gt; 0.5).astype(jnp.float32)\naccuracy = jnp.mean(test_preds == test_labels)\nprint(f\"\\nTest accuracy: {accuracy:.4f}\")\n\n# Visualise a test example\nidx = 0\nfig, axes = plt.subplots(3, 1, figsize=(14, 7))\n\naxes[0].imshow(test_features[idx].T, aspect='auto', origin='lower', cmap='magma')\naxes[0].set_title('Log-Mel Energy Features')\naxes[0].set_ylabel('Mel bin')\n\naxes[1].fill_between(range(200), test_labels[idx], alpha=0.4, color='#27ae60',\n                     label='Ground truth')\naxes[1].plot(jax.nn.sigmoid(test_logits[idx]), color='#e74c3c',\n             linewidth=1.5, label='Predicted probability')\naxes[1].axhline(0.5, color='gray', linestyle='--', linewidth=0.8)\naxes[1].set_ylabel('Speech probability')\naxes[1].legend()\naxes[1].set_title('VAD Predictions')\n\naxes[2].fill_between(range(200), test_labels[idx], alpha=0.4, color='#27ae60',\n                     label='Ground truth')\naxes[2].fill_between(range(200), test_preds[idx], alpha=0.4, color='#f39c12',\n                     label='Predicted (threshold=0.5)')\naxes[2].set_ylabel('Speech / Silence')\naxes[2].set_xlabel('Frame')\naxes[2].legend()\naxes[2].set_title('VAD Binary Decision')\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"chapter%2009%3A%20audio%20and%20speech/04.%20speaker%20and%20audio%20analysis/","title":"Speaker and Audio Analysis","text":"<p>Speaker and audio analysis identifies who is speaking, when they speak, and what non-speech sounds are present. This file covers speaker verification and identification, i-vectors, d-vectors, x-vectors, speaker diarisation, audio event classification, music information retrieval, and emotion recognition from speech.</p> <ul> <li> <p>In file 01 we built the signal-processing foundations: spectrograms, MFCCs, and mel filterbanks. In file 02 we recognised what was said. Now we ask who said it, when they said it, and what else is happening in the audio. Speaker recognition, diarisation, audio classification, and music analysis all share a common thread: learning compact embeddings that capture the right invariances for the task at hand, echoing the embedding ideas from chapter 06.</p> </li> <li> <p>Think of identifying a speaker like recognising a friend's voice on the phone. You do not need to understand the words; something about the timbre, pacing, and vocal quality is unique to that person. Speaker recognition systems learn to extract exactly this \"voiceprint\" from raw audio, ignoring what is said and focusing on how it is said.</p> </li> <li> <p>Speaker recognition is the umbrella term for two related tasks:</p> <ul> <li>Speaker verification (SV): given a claimed identity and an audio clip, determine whether the speaker is who they claim to be. This is a binary decision (accept or reject) and is the technology behind voice-based authentication (\"Hey Siri, is this my voice?\").</li> <li>Speaker identification (SI): given an audio clip and a gallery of known speakers, determine which speaker produced the clip. This is a multi-class classification problem.</li> </ul> </li> </ul> <p></p> <ul> <li> <p>Both tasks share the same underlying representation: a fixed-dimensional speaker embedding that captures the speaker's identity regardless of what they say. The difference is only in the decision stage: verification compares two embeddings, identification finds the nearest embedding among candidates.</p> </li> <li> <p>Cosine similarity is the standard metric for comparing speaker embeddings. Given enrollment embedding \\(e\\) and test embedding \\(t\\):</p> </li> </ul> \\[s = \\frac{e \\cdot t}{\\|e\\| \\, \\|t\\|}\\] <ul> <li> <p>A threshold \\(\\theta\\) determines the accept/reject decision: if \\(s &gt; \\theta\\), accept. The threshold trades off between the false acceptance rate (FAR) and false rejection rate (FRR). The equal error rate (EER), where FAR = FRR, is the standard evaluation metric. Lower EER means better performance. State-of-the-art systems achieve EER below 1% on standard benchmarks (VoxCeleb).</p> </li> <li> <p>i-vectors (Dehak et al., 2010) were the dominant speaker embedding before deep learning. The idea comes from factor analysis (chapter 02's matrix factorisation and chapter 04's dimensionality reduction). A universal background model (UBM), a large GMM trained on diverse speakers, defines a supervector space. Each utterance's GMM supervector is projected into a low-dimensional total variability space:</p> </li> </ul> \\[M = m + Tw\\] <ul> <li> <p>where \\(M\\) is the utterance's GMM supervector, \\(m\\) is the UBM mean supervector, \\(T\\) is the total variability matrix (learned from data), and \\(w\\) is the i-vector, a low-dimensional (typically 400-600) representation capturing both speaker and channel variability.</p> </li> <li> <p>To remove channel variability from i-vectors, Probabilistic Linear Discriminant Analysis (PLDA) models the i-vector as a sum of speaker-specific and channel-specific latent variables. PLDA provides a principled log-likelihood ratio score for verification:</p> </li> </ul> \\[\\text{score}(w_1, w_2) = \\log \\frac{P(w_1, w_2 \\mid \\text{same speaker})}{P(w_1 \\mid \\text{speaker}_1) \\, P(w_2 \\mid \\text{speaker}_2)}\\] <ul> <li> <p>d-vectors (Variani et al., 2014) were the first neural speaker embeddings. A DNN trained for speaker classification on frame-level features extracts a fixed-dimensional representation by averaging the last hidden layer activations over all frames in an utterance. Simple but effective, d-vectors demonstrated that neural networks could learn speaker-discriminative features without the complex statistical machinery of i-vectors.</p> </li> <li> <p>x-vectors (Snyder et al., 2018) significantly advanced neural speaker embeddings using a Time Delay Neural Network (TDNN) architecture. TDNNs are 1D convolutions with specific context windows at each layer, related to the dilated convolutions from file 03's WaveNet but applied to frame-level features rather than raw waveform samples.</p> </li> </ul> <p></p> <ul> <li>The x-vector architecture has three stages:<ul> <li>Frame-level layers: a stack of TDNN layers processes MFCCs (from file 01) with progressively wider temporal context. Each layer sees a fixed context window (e.g., \\(\\{t-2, t-1, t, t+1, t+2\\}\\) for the first layer, wider for subsequent layers).</li> <li>Statistics pooling: after the frame-level layers, the mean and standard deviation of the frame-level outputs are computed over the entire utterance, producing a fixed-dimensional vector regardless of utterance length:</li> </ul> </li> </ul> \\[ \\begin{aligned} \\mu &amp;= \\frac{1}{T} \\sum_{t=1}^{T} h_t \\\\ \\sigma &amp;= \\sqrt{\\frac{1}{T} \\sum_{t=1}^{T} (h_t - \\mu)^2} \\end{aligned} \\] <ul> <li> <p>where \\(h_t\\) is the frame-level output at time \\(t\\). The concatenation \\([\\mu; \\sigma]\\) is the pooled representation.</p> <ul> <li>Segment-level layers: fully connected layers process the pooled representation. The output of the first segment-level layer (before the softmax) is the x-vector embedding.</li> </ul> </li> <li> <p>x-vectors are trained with a standard cross-entropy loss over speaker identities. Despite being trained for classification, the learned intermediate representation (the x-vector) generalises well to unseen speakers because the network learns to extract speaker-discriminative features rather than memorising specific speakers.</p> </li> <li> <p>ECAPA-TDNN (Desplanques et al., 2020) is the current state-of-the-art TDNN-based architecture for speaker recognition. It introduces three improvements over x-vectors:</p> <ul> <li>Squeeze-Excitation (SE) blocks: channel attention (from chapter 08's SENet) that re-weights feature channels based on global context, allowing the model to emphasise speaker-relevant channels.</li> <li>Res2Net-style multi-scale features: within each TDNN block, the channels are split into groups that are processed hierarchically, creating features at multiple temporal resolutions (analogous to chapter 08's multi-scale feature extraction).</li> <li>Attentive statistics pooling: instead of equal-weight averaging, an attention mechanism weights each frame's contribution to the pooled statistics. Frames with more speaker-discriminative content (e.g., vowels, which carry more speaker information) receive higher attention weights:</li> </ul> </li> </ul> \\[\\alpha_t = \\frac{\\exp(v^T f(h_t))}{\\sum_{\\tau} \\exp(v^T f(h_\\tau))}\\] <ul> <li> <p>where \\(f\\) is a small neural network and \\(v\\) is a learned attention vector. The attended mean and standard deviation become \\(\\tilde{\\mu} = \\sum_t \\alpha_t h_t\\) and \\(\\tilde{\\sigma} = \\sqrt{\\sum_t \\alpha_t (h_t - \\tilde{\\mu})^2}\\).</p> </li> <li> <p>ECAPA-TDNN is typically trained with AAM-Softmax (Additive Angular Margin Softmax), which adds an angular margin penalty to the classification loss, pushing embeddings of the same speaker closer together and different speakers further apart on the hypersphere:</p> </li> </ul> \\[L = -\\log \\frac{e^{s \\cos(\\theta_{y_i} + m)}}{e^{s \\cos(\\theta_{y_i} + m)} + \\sum_{j \\neq y_i} e^{s \\cos \\theta_j}}\\] <ul> <li> <p>where \\(\\theta_{y_i}\\) is the angle between the embedding and the weight vector of the true class, \\(m\\) is the margin (typically 0.2), and \\(s\\) is a scaling factor (typically 30). This loss comes from face recognition (chapter 08's ArcFace) and is highly effective for speaker verification.</p> </li> <li> <p>Speaker diarisation answers \"who spoke when\" in a multi-speaker recording. Think of it as colouring a timeline: each colour represents a different speaker, and the system must determine when each speaker is active, including overlapping speech.</p> </li> </ul> <p></p> <ul> <li> <p>Clustering-based diarisation is the traditional pipeline approach:</p> <ul> <li>Segmentation: divide the audio into short segments (typically 1-2 seconds) using a sliding window or speaker change detection.</li> <li>Embedding extraction: extract a speaker embedding (x-vector, ECAPA-TDNN) for each segment.</li> <li>Clustering: group segments by speaker. Agglomerative Hierarchical Clustering (AHC) is standard: start with each segment as its own cluster, then iteratively merge the two most similar clusters until a stopping criterion is met (based on a distance threshold or a target number of speakers).</li> <li>Re-segmentation: refine boundaries using a Viterbi-based re-alignment.</li> </ul> </li> <li> <p>The number of speakers is typically unknown a priori, which makes this problem harder than standard clustering. Spectral clustering with an eigenvalue-based threshold for determining \\(k\\) is another common approach.</p> </li> <li> <p>End-to-end neural diarisation (EEND) (Fujita et al., 2019) frames diarisation as a multi-label classification problem. A neural network (typically a self-attention-based model, chapter 07's transformer) takes the entire recording as input and outputs a binary activity label for each speaker at each frame. This directly handles overlapping speech, which is a major weakness of clustering-based methods.</p> </li> <li> <p>The EEND output for \\(S\\) speakers at frame \\(t\\) is:</p> </li> </ul> \\[\\hat{y}_{t,s} = \\sigma(f_s(h_t))\\] <ul> <li> <p>where \\(h_t\\) is the transformer output at frame \\(t\\) and \\(f_s\\) is a linear projection for speaker \\(s\\). The training loss is binary cross-entropy summed over speakers and frames. A key challenge is that the number of speakers must be fixed or handled with a variable-output architecture (EEND-EDA uses an encoder-decoder with attractors).</p> </li> <li> <p>Permutation invariant training (PIT) for diarisation handles the label ambiguity problem: since speakers have no inherent ordering, the loss is computed for all possible speaker-to-output assignments and the minimum is taken (this is the same PIT used in source separation, covered in file 05).</p> </li> <li> <p>Audio classification assigns a label to an entire audio clip. Unlike ASR (file 02), which transcribes speech, audio classification covers a broader range: environmental sounds (siren, rain, dog bark), music genres (rock, jazz, classical), and general audio events.</p> </li> <li> <p>The standard approach follows the image classification paradigm from chapter 08: represent the audio as a spectrogram (a 2D time-frequency image), then apply a CNN or transformer classifier. This spectral-image approach leverages decades of progress in computer vision.</p> </li> <li> <p>Environmental sound classification (ESC) uses datasets like ESC-50 (50 classes, 2000 clips) and UrbanSound8K. Typical architectures are CNNs (chapter 06) applied to log-mel spectrograms. Data augmentation is crucial: time stretching, pitch shifting, adding background noise, and SpecAugment (file 02's masking approach applied to spectrograms) all improve generalisation.</p> </li> <li> <p>Audio event detection (Sound Event Detection, SED) is the temporal analogue of classification: not just what events are present, but when they start and end. AudioSet (Gemmeke et al., 2017) is the large-scale benchmark with 527 event classes and over 2 million 10-second clips from YouTube, each weakly labelled (clip-level labels, not frame-level).</p> </li> <li> <p>Weakly-supervised SED must learn frame-level predictions from clip-level labels. The standard approach uses a CNN that produces frame-level class probabilities, then aggregates them to clip-level predictions via attention pooling:</p> </li> </ul> \\[\\hat{Y}_c = \\sigma\\left(\\sum_t \\alpha_{t,c} \\cdot f_{t,c}\\right)\\] <ul> <li> <p>where \\(f_{t,c}\\) is the frame-level logit for class \\(c\\) at time \\(t\\), and \\(\\alpha_{t,c}\\) is an attention weight. The clip-level prediction \\(\\hat{Y}_c\\) is trained against the clip-level label.</p> </li> <li> <p>Acoustic scene classification (ASC) categorises the overall environment: \"airport\", \"park\", \"metro station\", \"office\". This is a holistic task: the model must capture the general acoustic texture rather than specific events. The DCASE challenge series benchmarks ASC annually, with winning systems typically using ensembles of CNNs on multi-resolution spectrograms.</p> </li> <li> <p>Audio embeddings are general-purpose representations learned from large-scale audio data, analogous to word embeddings (chapter 07) or image features (chapter 08) that transfer to downstream tasks.</p> </li> <li> <p>VGGish (Hershey et al., 2017) adapts the VGG image classification network (chapter 08) to audio. It processes 0.96-second log-mel spectrogram patches through a VGG-like CNN pre-trained on AudioSet, producing a 128-dimensional embedding per patch. VGGish embeddings serve as general-purpose audio features for downstream tasks, similar to how ImageNet-pretrained CNNs provide visual features.</p> </li> <li> <p>PANNs (Pre-trained Audio Neural Networks, Kong et al., 2020) are a family of CNN architectures (CNN6, CNN10, CNN14) trained on the full AudioSet for audio tagging. CNN14, the most widely used, is a 14-layer CNN with \\(3 \\times 3\\) convolutions applied to log-mel spectrograms. PANNs produce 2048-dimensional embeddings that achieve state-of-the-art transfer learning on diverse audio tasks.</p> </li> <li> <p>Audio Spectrogram Transformer (AST) (Gong et al., 2021) applies the Vision Transformer (ViT, chapter 08) architecture directly to audio spectrograms. The spectrogram is split into \\(16 \\times 16\\) patches (just like ViT splits images), each patch is linearly projected to a token embedding, positional embeddings are added, and a standard transformer encoder (chapter 07) processes the sequence. A [CLS] token's output is used for classification.</p> </li> </ul> <p></p> <ul> <li> <p>AST benefits from ImageNet pre-training: since spectrograms are 2D images, AST initialises from a ViT pre-trained on ImageNet images, then fine-tunes on audio. This cross-modal transfer is surprisingly effective because both domains share low-level features (edges, textures) and the positional embeddings can be interpolated to handle different spectrogram sizes.</p> </li> <li> <p>HTS-AT (Chen et al., 2022) improves on AST with a hierarchical Swin Transformer architecture (chapter 08's shifted-window attention), reducing computational cost while improving performance through multi-scale feature extraction.</p> </li> <li> <p>BEATs (Chen et al., 2023) uses an audio-specific pre-training strategy: iterative masked prediction with a discrete tokeniser (similar to wav2vec 2.0's approach from file 02 but applied to general audio). The tokeniser is progressively refined, creating increasingly semantically meaningful discrete audio tokens.</p> </li> <li> <p>Speaker diarisation with embeddings combines speaker embeddings with temporal modelling. Modern systems like Pyannote.audio use a three-stage pipeline: (1) a neural segmentation model that detects speaker turns and overlapping speech, (2) an embedding extraction stage (ECAPA-TDNN) applied to each detected segment, and (3) clustering to assign speaker identities across the recording.</p> </li> <li> <p>Music information retrieval (MIR) applies audio analysis to music. The spectral representations from file 01 are particularly useful here because music has rich harmonic structure.</p> </li> <li> <p>Beat tracking detects the rhythmic pulse of music. The standard approach computes an onset strength envelope from the spectrogram (detecting energy increases that signal note onsets), then finds the tempo using autocorrelation or a tempogram, and finally tracks individual beat positions using dynamic programming to find the sequence of beat times that best matches the onset envelope while maintaining a consistent tempo.</p> </li> <li> <p>Chord recognition identifies the harmonic content over time. The input is typically a chromagram (also called a pitch class profile): a 12-dimensional representation that folds all octaves together, showing the energy in each of the 12 pitch classes (C, C#, D, ..., B). A CNN or RNN (chapter 06) classifies each time frame into one of the standard chord labels (C major, A minor, G7, etc.).</p> </li> <li> <p>The chromagram is computed from the STFT (file 01) by mapping each frequency bin to its pitch class:</p> </li> </ul> \\[\\text{chroma}(p) = \\sum_{k : \\text{pitch}(k) \\bmod 12 = p} |X(k)|^2\\] <ul> <li> <p>where \\(p \\in \\{0, 1, \\ldots, 11\\}\\) is the pitch class and \\(\\text{pitch}(k)\\) maps frequency bin \\(k\\) to its MIDI note number.</p> </li> <li> <p>Source separation basics (detailed further in file 05) separate a music recording into individual instruments (vocals, drums, bass, other). This is central to MIR applications like remixing, karaoke, and music transcription. Models like Demucs (file 05) achieve remarkably good separation quality on the standard MUSDB18 benchmark.</p> </li> <li> <p>Music tagging assigns labels to songs (genre, mood, instruments, era). It is essentially audio classification applied to music, using the same CNN-on-spectrogram approach. The Million Song Dataset and MagnaTagATune are standard benchmarks.</p> </li> <li> <p>Audio fingerprinting identifies a specific recording from a short excerpt, even with noise, reverberation, or compression artifacts. The classic system is Shazam, which hashes constellation points (prominent peaks in the spectrogram). Neural approaches learn robust embeddings that are invariant to acoustic degradation while remaining discriminative between different recordings, echoing the invariant feature learning from chapter 06 and chapter 08.</p> </li> </ul>"},{"location":"chapter%2009%3A%20audio%20and%20speech/04.%20speaker%20and%20audio%20analysis/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ul> <li>Task 1: Speaker embedding extraction with statistics pooling. Build a simple x-vector-style model that processes frame-level features through TDNN layers and statistics pooling to produce speaker embeddings.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\n# Simulate frame-level MFCC features for multiple speakers\ndef generate_speaker_data(key, n_speakers=5, utterances_per_speaker=20,\n                          n_frames=100, n_features=40):\n    \"\"\"Generate synthetic speaker data with speaker-dependent patterns.\"\"\"\n    keys = jr.split(key, 3)\n    all_features = []\n    all_labels = []\n\n    # Each speaker has a characteristic spectral pattern\n    speaker_patterns = jr.normal(keys[0], (n_speakers, n_features)) * 0.5\n\n    for spk in range(n_speakers):\n        for utt in range(utterances_per_speaker):\n            k = jr.fold_in(keys[1], spk * utterances_per_speaker + utt)\n            noise = jr.normal(k, (n_frames, n_features)) * 0.3\n            features = speaker_patterns[spk][None, :] + noise\n            all_features.append(features)\n            all_labels.append(spk)\n\n    perm = jr.permutation(keys[2], len(all_features))\n    features = jnp.stack(all_features)[perm]\n    labels = jnp.array(all_labels)[perm]\n    return features, labels\n\nkey = jr.PRNGKey(42)\nfeatures, labels = generate_speaker_data(key)\nn_speakers = 5\nn_features = 40\n\n# x-vector-style model\ndef init_xvector(key, n_features=40, hidden=128, embed_dim=64, n_speakers=5):\n    keys = jr.split(key, 8)\n    params = {\n        # TDNN layer 1: context [-2, 2]\n        'tdnn1_w': jr.normal(keys[0], (5, n_features, hidden)) * jnp.sqrt(2.0 / (5 * n_features)),\n        'tdnn1_b': jnp.zeros(hidden),\n        # TDNN layer 2: context [-2, 2]\n        'tdnn2_w': jr.normal(keys[1], (5, hidden, hidden)) * jnp.sqrt(2.0 / (5 * hidden)),\n        'tdnn2_b': jnp.zeros(hidden),\n        # TDNN layer 3: context [-3, 3]\n        'tdnn3_w': jr.normal(keys[2], (7, hidden, hidden)) * jnp.sqrt(2.0 / (7 * hidden)),\n        'tdnn3_b': jnp.zeros(hidden),\n        # Segment-level layers (after pooling: 2*hidden -&gt; embed_dim)\n        'seg1_w': jr.normal(keys[3], (2 * hidden, embed_dim)) * jnp.sqrt(2.0 / (2 * hidden)),\n        'seg1_b': jnp.zeros(embed_dim),\n        # Classification head\n        'cls_w': jr.normal(keys[4], (embed_dim, n_speakers)) * jnp.sqrt(2.0 / embed_dim),\n        'cls_b': jnp.zeros(n_speakers),\n    }\n    return params\n\ndef xvector_forward(params, x, return_embedding=False):\n    \"\"\"x: (batch, frames, features) -&gt; logits or embeddings.\"\"\"\n    # TDNN layers (1D convolutions)\n    h = jax.lax.conv_general_dilated(\n        x.transpose(0, 2, 1), params['tdnn1_w'].transpose(2, 1, 0),\n        window_strides=(1,), padding='SAME'\n    ).transpose(0, 2, 1) + params['tdnn1_b']\n    h = jax.nn.relu(h)\n\n    h = jax.lax.conv_general_dilated(\n        h.transpose(0, 2, 1), params['tdnn2_w'].transpose(2, 1, 0),\n        window_strides=(1,), padding='SAME'\n    ).transpose(0, 2, 1) + params['tdnn2_b']\n    h = jax.nn.relu(h)\n\n    h = jax.lax.conv_general_dilated(\n        h.transpose(0, 2, 1), params['tdnn3_w'].transpose(2, 1, 0),\n        window_strides=(1,), padding='SAME'\n    ).transpose(0, 2, 1) + params['tdnn3_b']\n    h = jax.nn.relu(h)\n\n    # Statistics pooling: mean and std over time\n    mu = jnp.mean(h, axis=1)\n    sigma = jnp.std(h, axis=1)\n    pooled = jnp.concatenate([mu, sigma], axis=-1)\n\n    # Segment-level layer -&gt; embedding\n    embedding = jax.nn.relu(pooled @ params['seg1_w'] + params['seg1_b'])\n\n    if return_embedding:\n        return embedding\n\n    # Classification\n    logits = embedding @ params['cls_w'] + params['cls_b']\n    return logits\n\ndef cross_entropy_loss(params, features, labels):\n    logits = xvector_forward(params, features)\n    one_hot = jax.nn.one_hot(labels, n_speakers)\n    log_probs = jax.nn.log_softmax(logits)\n    return -jnp.mean(jnp.sum(one_hot * log_probs, axis=-1))\n\ngrad_fn = jax.jit(jax.value_and_grad(cross_entropy_loss))\n\n# Train\nparams = init_xvector(jr.PRNGKey(0))\nlr = 1e-3\nlosses = []\n\nfor epoch in range(300):\n    loss_val, grads = grad_fn(params, features, labels)\n    params = jax.tree.map(lambda p, g: p - lr * g, params, grads)\n    losses.append(float(loss_val))\n\n# Extract embeddings and visualise with t-SNE-style 2D projection (using PCA)\nembeddings = xvector_forward(params, features, return_embedding=True)\n\n# Simple PCA to 2D\nemb_centered = embeddings - jnp.mean(embeddings, axis=0)\n_, _, Vt = jnp.linalg.svd(emb_centered, full_matrices=False)\nproj_2d = emb_centered @ Vt[:2].T\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(losses, color='#3498db', linewidth=1.5)\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Cross-Entropy Loss')\naxes[0].set_title('Speaker Classification Training')\naxes[0].set_yscale('log')\n\ncolors = ['#3498db', '#e74c3c', '#27ae60', '#f39c12', '#9b59b6']\nfor spk in range(n_speakers):\n    mask = labels == spk\n    axes[1].scatter(proj_2d[mask, 0], proj_2d[mask, 1], c=colors[spk],\n                    label=f'Speaker {spk}', alpha=0.7, s=30)\naxes[1].set_xlabel('PC 1')\naxes[1].set_ylabel('PC 2')\naxes[1].set_title('Speaker Embeddings (PCA projection)')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n# Verification demo: cosine similarity\nemb_norm = embeddings / jnp.linalg.norm(embeddings, axis=-1, keepdims=True)\nsim_matrix = emb_norm @ emb_norm.T\nprint(f\"Embedding shape: {embeddings.shape}\")\nprint(f\"Avg same-speaker similarity: {jnp.mean(sim_matrix[labels[:, None] == labels[None, :]]):.4f}\")\nprint(f\"Avg diff-speaker similarity: {jnp.mean(sim_matrix[labels[:, None] != labels[None, :]]):.4f}\")\n</code></pre> <ul> <li>Task 2: Speaker verification with cosine similarity scoring. Given pre-computed speaker embeddings, implement a verification system that computes EER (Equal Error Rate) and plots the DET curve.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\ndef generate_verification_pairs(key, n_speakers=20, dim=64, n_pairs=2000):\n    \"\"\"Generate speaker embeddings and verification trial pairs.\"\"\"\n    keys = jr.split(key, 5)\n\n    # Speaker centroids with some variance\n    centroids = jr.normal(keys[0], (n_speakers, dim))\n    centroids = centroids / jnp.linalg.norm(centroids, axis=-1, keepdims=True)\n\n    # Generate enrollment and test embeddings with intra-speaker variance\n    enroll_embs = []\n    test_embs = []\n    trial_labels = []  # 1 = same speaker (target), 0 = different (impostor)\n\n    for i in range(n_pairs):\n        k1, k2, k3 = jr.split(jr.fold_in(keys[1], i), 3)\n        is_target = jr.bernoulli(k1).astype(int)\n\n        spk1 = jr.randint(k2, (), 0, n_speakers)\n        emb1 = centroids[spk1] + jr.normal(jr.fold_in(k3, 0), (dim,)) * 0.15\n\n        if is_target:\n            spk2 = spk1\n        else:\n            spk2 = (spk1 + jr.randint(jr.fold_in(k3, 1), (), 1, n_speakers)) % n_speakers\n\n        emb2 = centroids[spk2] + jr.normal(jr.fold_in(k3, 2), (dim,)) * 0.15\n\n        enroll_embs.append(emb1)\n        test_embs.append(emb2)\n        trial_labels.append(int(is_target))\n\n    return (jnp.stack(enroll_embs), jnp.stack(test_embs),\n            jnp.array(trial_labels))\n\nkey = jr.PRNGKey(42)\nenroll, test, labels = generate_verification_pairs(key)\n\n# Compute cosine similarity scores\nenroll_norm = enroll / jnp.linalg.norm(enroll, axis=-1, keepdims=True)\ntest_norm = test / jnp.linalg.norm(test, axis=-1, keepdims=True)\nscores = jnp.sum(enroll_norm * test_norm, axis=-1)\n\n# Compute FAR and FRR at various thresholds\nthresholds = jnp.linspace(-1.0, 1.0, 500)\n\ntarget_scores = scores[labels == 1]\nimpostor_scores = scores[labels == 0]\n\nfars = []\nfrrs = []\nfor thresh in thresholds:\n    far = jnp.mean(impostor_scores &gt;= thresh)  # false accepts\n    frr = jnp.mean(target_scores &lt; thresh)     # false rejects\n    fars.append(float(far))\n    frrs.append(float(frr))\n\nfars = jnp.array(fars)\nfrrs = jnp.array(frrs)\n\n# Find EER: where FAR \u2248 FRR\neer_idx = jnp.argmin(jnp.abs(fars - frrs))\neer = float((fars[eer_idx] + frrs[eer_idx]) / 2)\neer_threshold = float(thresholds[eer_idx])\n\nprint(f\"Equal Error Rate (EER): {eer:.4f} ({eer*100:.2f}%)\")\nprint(f\"EER threshold: {eer_threshold:.4f}\")\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Score distributions\nbins = jnp.linspace(-0.5, 1.0, 60)\naxes[0].hist(target_scores, bins=bins, alpha=0.6, color='#27ae60',\n             label='Target (same speaker)', density=True)\naxes[0].hist(impostor_scores, bins=bins, alpha=0.6, color='#e74c3c',\n             label='Impostor (different speaker)', density=True)\naxes[0].axvline(eer_threshold, color='#f39c12', linestyle='--', linewidth=2,\n                label=f'EER threshold = {eer_threshold:.3f}')\naxes[0].set_xlabel('Cosine Similarity Score')\naxes[0].set_ylabel('Density')\naxes[0].set_title('Score Distributions')\naxes[0].legend()\n\n# FAR vs FRR\naxes[1].plot(thresholds, fars, color='#e74c3c', linewidth=2, label='FAR')\naxes[1].plot(thresholds, frrs, color='#3498db', linewidth=2, label='FRR')\naxes[1].axvline(eer_threshold, color='#f39c12', linestyle='--', linewidth=1.5)\naxes[1].scatter([eer_threshold], [eer], color='#f39c12', s=100, zorder=5,\n                label=f'EER = {eer:.4f}')\naxes[1].set_xlabel('Threshold')\naxes[1].set_ylabel('Error Rate')\naxes[1].set_title('FAR and FRR vs Threshold')\naxes[1].legend()\n\n# DET curve (FAR vs FRR)\naxes[2].plot(fars, frrs, color='#9b59b6', linewidth=2)\naxes[2].plot([0, 1], [0, 1], 'k--', alpha=0.3)\naxes[2].scatter([eer], [eer], color='#f39c12', s=100, zorder=5,\n                label=f'EER = {eer:.4f}')\naxes[2].set_xlabel('False Acceptance Rate')\naxes[2].set_ylabel('False Rejection Rate')\naxes[2].set_title('DET Curve')\naxes[2].set_xlim([0, 0.5])\naxes[2].set_ylim([0, 0.5])\naxes[2].legend()\naxes[2].set_aspect('equal')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <ul> <li>Task 3: Audio spectrogram patch embedding (AST-style). Implement the patch extraction and embedding layer of the Audio Spectrogram Transformer, visualising how a spectrogram is tokenised.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\n# Generate a synthetic spectrogram (harmonic structure + noise)\ndef generate_spectrogram(key, n_time=128, n_freq=128):\n    \"\"\"Create a synthetic spectrogram with harmonic patterns.\"\"\"\n    k1, k2 = jr.split(key)\n    spec = jr.normal(k1, (n_time, n_freq)) * 0.1\n\n    # Add harmonic bands (simulating speech formants)\n    for f0 in [15, 30, 45, 70]:\n        width = 3\n        envelope = jnp.exp(-0.5 * ((jnp.arange(n_freq) - f0) / width) ** 2)\n        time_mod = 0.5 + 0.5 * jnp.sin(2 * jnp.pi * jnp.arange(n_time) / 40)\n        spec += jnp.outer(time_mod, envelope)\n\n    return jnp.clip(spec, 0, None)\n\nkey = jr.PRNGKey(42)\nspectrogram = generate_spectrogram(key)\nn_time, n_freq = spectrogram.shape\n\n# Patch extraction parameters\npatch_h = 16  # time\npatch_w = 16  # frequency\nstride_h = 16\nstride_w = 16\nembed_dim = 192  # ViT-Small dimension\n\nn_patches_h = n_time // stride_h\nn_patches_w = n_freq // stride_w\nn_patches = n_patches_h * n_patches_w\n\nprint(f\"Spectrogram: {n_time} x {n_freq}\")\nprint(f\"Patch size: {patch_h} x {patch_w}\")\nprint(f\"Number of patches: {n_patches_h} x {n_patches_w} = {n_patches}\")\n\n# Extract patches\ndef extract_patches(spec, patch_h, patch_w, stride_h, stride_w):\n    \"\"\"Extract non-overlapping patches from spectrogram.\"\"\"\n    patches = []\n    positions = []\n    for i in range(0, spec.shape[0] - patch_h + 1, stride_h):\n        for j in range(0, spec.shape[1] - patch_w + 1, stride_w):\n            patch = spec[i:i+patch_h, j:j+patch_w]\n            patches.append(patch.flatten())\n            positions.append((i, j))\n    return jnp.stack(patches), positions\n\npatches, positions = extract_patches(spectrogram, patch_h, patch_w, stride_h, stride_w)\nprint(f\"Patches shape: {patches.shape}\")  # (n_patches, patch_h * patch_w)\n\n# Linear projection (patch embedding)\npatch_dim = patch_h * patch_w\nk1, k2 = jr.split(jr.PRNGKey(0))\nW_embed = jr.normal(k1, (patch_dim, embed_dim)) * jnp.sqrt(2.0 / patch_dim)\nb_embed = jnp.zeros(embed_dim)\n\n# Learnable positional embeddings\npos_embed = jr.normal(k2, (n_patches + 1, embed_dim)) * 0.02  # +1 for CLS\n\n# CLS token\ncls_token = jnp.zeros((1, embed_dim))\n\n# Forward pass\npatch_tokens = patches @ W_embed + b_embed  # (n_patches, embed_dim)\ntokens = jnp.concatenate([cls_token, patch_tokens], axis=0)  # (n_patches+1, embed_dim)\ntokens = tokens + pos_embed  # Add positional embeddings\n\nprint(f\"Token sequence shape: {tokens.shape}\")\nprint(f\"Each token has dimension: {embed_dim}\")\n\n# Visualisation\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Original spectrogram with patch grid\naxes[0, 0].imshow(spectrogram.T, aspect='auto', origin='lower', cmap='magma')\nfor i in range(0, n_time + 1, stride_h):\n    axes[0, 0].axvline(i - 0.5, color='white', linewidth=0.5, alpha=0.5)\nfor j in range(0, n_freq + 1, stride_w):\n    axes[0, 0].axhline(j - 0.5, color='white', linewidth=0.5, alpha=0.5)\naxes[0, 0].set_title(f'Spectrogram with {patch_h}x{patch_w} Patch Grid')\naxes[0, 0].set_xlabel('Time frame')\naxes[0, 0].set_ylabel('Frequency bin')\n\n# Individual patches visualised\nn_show = min(16, n_patches)\npatch_grid = patches[:n_show].reshape(n_show, patch_h, patch_w)\ncombined = jnp.concatenate([patch_grid[i] for i in range(min(8, n_show))], axis=1)\naxes[0, 1].imshow(combined.T, aspect='auto', origin='lower', cmap='magma')\naxes[0, 1].set_title(f'First {min(8, n_show)} Patches (concatenated)')\naxes[0, 1].set_xlabel('Patch index (horizontal)')\naxes[0, 1].set_ylabel('Frequency within patch')\n\n# Token embeddings similarity matrix\ntoken_norms = tokens / jnp.linalg.norm(tokens, axis=-1, keepdims=True)\nsim = token_norms @ token_norms.T\nim = axes[1, 0].imshow(sim, cmap='RdBu_r', vmin=-1, vmax=1)\naxes[1, 0].set_title('Token Similarity Matrix (cosine)')\naxes[1, 0].set_xlabel('Token index')\naxes[1, 0].set_ylabel('Token index')\nplt.colorbar(im, ax=axes[1, 0], fraction=0.046)\n\n# Positional embedding similarity\npos_norms = pos_embed / jnp.linalg.norm(pos_embed, axis=-1, keepdims=True)\npos_sim = pos_norms @ pos_norms.T\nim2 = axes[1, 1].imshow(pos_sim, cmap='RdBu_r', vmin=-1, vmax=1)\naxes[1, 1].set_title('Positional Embedding Similarity')\naxes[1, 1].set_xlabel('Position index')\naxes[1, 1].set_ylabel('Position index')\nplt.colorbar(im2, ax=axes[1, 1], fraction=0.046)\n\nplt.tight_layout()\nplt.show()\n</code></pre> <ul> <li>Task 4: Simple chromagram computation for chord analysis. Compute and visualise a chromagram from a synthetic harmonic signal, demonstrating the pitch class folding used in music information retrieval.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Generate a synthetic musical signal: C major chord -&gt; G major chord\nsr = 16000\nduration = 2.0\nt = jnp.linspace(0, duration, int(sr * duration))\n\n# C major (C4=261.6, E4=329.6, G4=392.0) for first half\n# G major (G3=196.0, B3=246.9, D4=293.7) for second half\nhalf = len(t) // 2\n\nc_major = (0.5 * jnp.sin(2 * jnp.pi * 261.63 * t[:half]) +\n           0.4 * jnp.sin(2 * jnp.pi * 329.63 * t[:half]) +\n           0.3 * jnp.sin(2 * jnp.pi * 392.00 * t[:half]))\n\ng_major = (0.5 * jnp.sin(2 * jnp.pi * 196.00 * t[:half]) +\n           0.4 * jnp.sin(2 * jnp.pi * 246.94 * t[:half]) +\n           0.3 * jnp.sin(2 * jnp.pi * 293.66 * t[:half]))\n\nsignal = jnp.concatenate([c_major, g_major])\n\n# Compute STFT\nn_fft = 4096  # high resolution for pitch accuracy\nhop_length = 512\nwindow = jnp.hanning(n_fft)\n\ndef stft(signal, n_fft, hop_length, window):\n    n_frames = 1 + (len(signal) - n_fft) // hop_length\n    frames = jnp.stack([\n        signal[i * hop_length : i * hop_length + n_fft] * window\n        for i in range(n_frames)\n    ])\n    return jnp.fft.rfft(frames, n=n_fft)\n\nS = stft(signal, n_fft, hop_length, window)\npower_spec = jnp.abs(S) ** 2\nfreqs = jnp.fft.rfftfreq(n_fft, 1.0 / sr)\n\n# Compute chromagram by mapping frequency bins to pitch classes\n# MIDI note number from frequency: 69 + 12 * log2(f / 440)\nnote_names = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']\n\ndef freq_to_chroma(freq):\n    \"\"\"Map frequency to pitch class (0-11). Returns -1 for freq &lt;= 0.\"\"\"\n    midi = 69 + 12 * jnp.log2(jnp.clip(freq, 1e-10, None) / 440.0)\n    return jnp.round(midi).astype(int) % 12\n\n# Build chromagram: sum power spectrum energy for each pitch class\nchromagram = jnp.zeros((power_spec.shape[0], 12))\nvalid_freqs = freqs[1:]  # skip DC\nvalid_power = power_spec[:, 1:]\n\nfor p in range(12):\n    # Find frequency bins belonging to this pitch class\n    chroma_bins = freq_to_chroma(valid_freqs)\n    mask = (chroma_bins == p).astype(jnp.float32)\n    chromagram = chromagram.at[:, p].set(\n        jnp.sum(valid_power * mask[None, :], axis=1)\n    )\n\n# Normalise each frame\nchromagram = chromagram / (jnp.max(chromagram, axis=1, keepdims=True) + 1e-8)\n\n# Visualisation\nfig, axes = plt.subplots(3, 1, figsize=(14, 10))\n\n# Waveform\naxes[0].plot(t[:3000], signal[:3000], color='#3498db', linewidth=0.5,\n             label='C major')\naxes[0].plot(t[half:half+3000], signal[half:half+3000], color='#e74c3c',\n             linewidth=0.5, label='G major')\naxes[0].set_title('Waveform: C major \u2192 G major')\naxes[0].set_ylabel('Amplitude')\naxes[0].set_xlabel('Time (s)')\naxes[0].legend()\n\n# Spectrogram (log scale)\ntime_axis = jnp.arange(power_spec.shape[0]) * hop_length / sr\naxes[1].imshow(jnp.log1p(power_spec[:, :500].T), aspect='auto', origin='lower',\n               cmap='magma', extent=[0, time_axis[-1], 0, freqs[500]])\naxes[1].set_title('Power Spectrogram')\naxes[1].set_ylabel('Frequency (Hz)')\naxes[1].set_xlabel('Time (s)')\n\n# Chromagram\nim = axes[2].imshow(chromagram.T, aspect='auto', origin='lower', cmap='YlOrRd',\n                     extent=[0, time_axis[-1], -0.5, 11.5])\naxes[2].set_yticks(range(12))\naxes[2].set_yticklabels(note_names)\naxes[2].set_title('Chromagram (pitch class energy over time)')\naxes[2].set_ylabel('Pitch class')\naxes[2].set_xlabel('Time (s)')\nplt.colorbar(im, ax=axes[2], fraction=0.046, label='Normalised energy')\n\n# Mark expected active pitch classes\nmid_frame = chromagram.shape[0] // 2\nprint(f\"C major region - expected: C, E, G\")\nprint(f\"  Chroma values: {dict(zip(note_names, [f'{v:.2f}' for v in chromagram[mid_frame//2]]))}\")\nprint(f\"G major region - expected: G, B, D\")\nprint(f\"  Chroma values: {dict(zip(note_names, [f'{v:.2f}' for v in chromagram[mid_frame + mid_frame//2]]))}\")\n\nplt.tight_layout()\nplt.show()\n</code></pre>"},{"location":"chapter%2009%3A%20audio%20and%20speech/05.%20source%20separation%20and%20noise/","title":"Source Separation and Noise Cancellation","text":"<p>Source separation and noise cancellation recover individual signals from mixed audio -- the computational cocktail party problem. This file covers ICA, NMF, time-frequency masking, beamforming, deep learning separation networks (Conv-TasNet, SepFormer), speech enhancement, and adaptive noise cancellation.</p> <ul> <li> <p>Imagine standing at a crowded cocktail party. Dozens of people are talking simultaneously, music is playing, glasses are clinking, yet you can focus on one conversation and follow it clearly. This remarkable ability, the cocktail party problem (Cherry, 1953), is something the human auditory system solves effortlessly but machines find extraordinarily difficult. This file covers the algorithms that attempt it: separating mixed audio sources, cancelling unwanted noise, and enhancing speech in adverse conditions.</p> </li> <li> <p>The signal-processing foundations from file 01 (STFT, spectrograms, filterbanks) underpin every method here. The matrix decomposition techniques from chapter 02 (NMF, ICA, SVD) provide the classical toolkit. The deep learning architectures from chapter 06 (CNNs, RNNs, attention) and the probability theory from chapter 04/05 inform the modern approaches.</p> </li> </ul> <p></p> <ul> <li>Problem formulation: a mixture signal \\(x(t)\\) is observed at one or more microphones. The mixture is a sum (in the simplest case) of \\(C\\) source signals:</li> </ul> \\[x(t) = \\sum_{c=1}^{C} s_c(t) + n(t)\\] <ul> <li> <p>where \\(s_c(t)\\) is the \\(c\\)-th source signal and \\(n(t)\\) is background noise. The goal is to recover the individual \\(s_c(t)\\) from \\(x(t)\\). In the single-microphone case this is severely underdetermined: one equation, \\(C\\) unknowns. Additional assumptions (statistical independence, spectral structure, learned priors) are needed to make the problem tractable.</p> </li> <li> <p>In the frequency domain (via STFT from file 01), the mixture becomes:</p> </li> </ul> \\[X(t, f) = \\sum_{c=1}^{C} S_c(t, f) + N(t, f)\\] <ul> <li>Many separation methods work in the time-frequency domain by estimating a mask \\(M_c(t, f) \\in [0, 1]\\) for each source, then recovering the source as \\(\\hat{S}_c(t, f) = M_c(t, f) \\cdot X(t, f)\\). The ideal binary mask (IBM) sets \\(M_c(t, f) = 1\\) if source \\(c\\) dominates that time-frequency bin and 0 otherwise. The ideal ratio mask (IRM) is a soft version:</li> </ul> \\[\\text{IRM}_c(t, f) = \\frac{|S_c(t, f)|^2}{\\sum_{j=1}^{C} |S_j(t, f)|^2}\\] <ul> <li> <p>Independent Component Analysis (ICA) is the classical approach when the number of microphones equals or exceeds the number of sources. ICA (chapter 02) finds a linear unmixing matrix \\(W\\) such that \\(\\hat{s} = Wx\\), where the recovered sources \\(\\hat{s}\\) are maximally statistically independent. The key assumption is that source signals are non-Gaussian and independent, which is typically valid for speech and music.</p> </li> <li> <p>For the multi-microphone instantaneous mixing model \\(x = As\\) (where \\(A\\) is the mixing matrix), ICA recovers \\(W \\approx A^{-1}\\) by maximising the non-Gaussianity of the outputs (FastICA uses negentropy) or by minimising mutual information. ICA works well in controlled settings but fails when mixing involves convolution (room reverberation), when sources outnumber microphones, or when the independence assumption is violated.</p> </li> <li> <p>Non-negative Matrix Factorisation (NMF) decomposes the magnitude spectrogram \\(V \\in \\mathbb{R}_+^{F \\times T}\\) into a product of two non-negative matrices (chapter 02):</p> </li> </ul> \\[V \\approx WH\\] <ul> <li> <p>where \\(W \\in \\mathbb{R}_+^{F \\times K}\\) is a dictionary of \\(K\\) spectral basis vectors and \\(H \\in \\mathbb{R}_+^{K \\times T}\\) contains the activation coefficients over time. The non-negativity constraint is physically motivated: magnitudes are non-negative, and sounds combine additively.</p> </li> <li> <p>For source separation, NMF learns separate dictionaries for each source: \\(W_{\\text{speech}}\\) captures the spectral patterns of speech (formant structures), while \\(W_{\\text{noise}}\\) captures noise patterns. The mixture is decomposed as \\(V \\approx W_{\\text{speech}} H_{\\text{speech}} + W_{\\text{noise}} H_{\\text{noise}}\\), and each source is recovered by masking. NMF is minimised using multiplicative updates with either the Frobenius norm or KL divergence as the cost function:</p> </li> </ul> \\[ \\begin{aligned} \\text{Frobenius:} \\quad D_F(V \\| WH) &amp;= \\|V - WH\\|_F^2 \\\\ \\text{KL:} \\quad D_{KL}(V \\| WH) &amp;= \\sum_{f,t} \\left[ V_{ft} \\log \\frac{V_{ft}}{(WH)_{ft}} - V_{ft} + (WH)_{ft} \\right] \\end{aligned} \\] <ul> <li>Beamforming exploits spatial information from microphone arrays. When a source signal arrives at different microphones with different delays (due to the spatial arrangement), these delays can be used to enhance the signal from one direction while suppressing others.</li> </ul> <p></p> <ul> <li>Delay-and-sum beamforming is the simplest approach. If the desired source is at angle \\(\\theta\\) relative to the array, the time delay at microphone \\(m\\) is \\(\\tau_m(\\theta) = d_m \\sin \\theta / c\\), where \\(d_m\\) is the microphone position and \\(c\\) is the speed of sound. The beamformer output aligns and sums the microphone signals:</li> </ul> \\[y(t) = \\frac{1}{M} \\sum_{m=1}^{M} x_m(t - \\tau_m(\\theta))\\] <ul> <li> <p>Signals from the target direction add coherently, while signals from other directions add incoherently, providing spatial filtering. The array geometry determines the spatial resolution: larger arrays give narrower beams.</p> </li> <li> <p>Minimum Variance Distortionless Response (MVDR) beamforming optimises the weights to minimise total output power while passing the target direction without distortion:</p> </li> </ul> \\[ \\begin{aligned} \\min_{\\mathbf{w}} \\quad &amp; \\mathbf{w}^H \\Phi_{nn} \\mathbf{w} \\\\ \\text{subject to} \\quad &amp; \\mathbf{w}^H \\mathbf{d}(\\theta) = 1 \\end{aligned} \\] <ul> <li>where \\(\\Phi_{nn}\\) is the noise spatial covariance matrix and \\(\\mathbf{d}(\\theta)\\) is the steering vector for direction \\(\\theta\\). The closed-form solution is:</li> </ul> \\[\\mathbf{w}_{\\text{MVDR}} = \\frac{\\Phi_{nn}^{-1} \\mathbf{d}(\\theta)}{\\mathbf{d}(\\theta)^H \\Phi_{nn}^{-1} \\mathbf{d}(\\theta)}\\] <ul> <li> <p>MVDR adapts to the noise environment by using the estimated noise covariance, providing better interference rejection than delay-and-sum. It is widely used in hearing aids, smart speakers, and teleconferencing systems.</p> </li> <li> <p>Deep learning for source separation has dramatically improved performance, especially in the single-microphone case where classical methods struggle. The general paradigm is: encode the mixture, estimate masks or source representations with a neural network, and decode to recover individual sources.</p> </li> <li> <p>Deep clustering (Hershey et al., 2016) embeds each time-frequency bin into a high-dimensional space where bins belonging to the same source are close together and bins from different sources are far apart. A bidirectional LSTM (chapter 06) maps each T-F bin \\((t, f)\\) to an embedding \\(v_{t,f} \\in \\mathbb{R}^D\\). The training objective is:</p> </li> </ul> \\[\\mathcal{L} = \\|VV^T - YY^T\\|_F^2\\] <ul> <li> <p>where \\(V\\) is the matrix of embeddings and \\(Y\\) is the one-hot matrix of source assignments. The product \\(VV^T\\) is an affinity matrix (how similar two bins' embeddings are), and \\(YY^T\\) is the ideal affinity (1 if same source, 0 otherwise). At inference, K-means clustering on the embeddings produces binary masks.</p> </li> <li> <p>Conv-TasNet (Luo and Mesgarani, 2019) operates entirely in the time domain, bypassing the STFT. It has three components:</p> </li> </ul> <p></p> <ul> <li> <p>Encoder: a 1D convolution maps short segments of the mixture waveform to a latent representation. For a mixture \\(x \\in \\mathbb{R}^T\\), the encoder output is \\(w = \\text{ReLU}(U \\ast x) \\in \\mathbb{R}^{N \\times L}\\), where \\(U\\) is a learnable basis (analogous to the STFT basis but learned from data), \\(N\\) is the number of basis functions, and \\(L\\) is the number of segments. The encoder kernel size and stride (typically 2ms and 1ms) determine the temporal resolution.</p> </li> <li> <p>Separator: a Temporal Convolutional Network (TCN) processes the encoded mixture and outputs \\(C\\) masks. The TCN stacks dilated 1D depthwise separable convolutions (from chapter 08's efficient convolutions) in blocks with exponentially increasing dilation factors \\(1, 2, 4, \\ldots, 2^{B-1}\\), repeated \\(R\\) times. This gives a very large receptive field while keeping computation efficient.</p> </li> <li> <p>Decoder: a transposed 1D convolution (with a learned basis \\(V\\)) converts each masked representation back to the time domain: \\(\\hat{s}_c = V^T (M_c \\odot w)\\).</p> </li> <li> <p>Conv-TasNet significantly outperforms spectrogram-based methods because the learned encoder-decoder basis can capture information (particularly phase) that the STFT magnitude discards.</p> </li> <li> <p>Dual-Path RNN (DPRNN) (Luo et al., 2020) addresses the long sequence modelling problem in separation. Rather than processing the entire encoded sequence with a single RNN or TCN, DPRNN splits the sequence into overlapping chunks and applies RNNs along two paths: an intra-chunk path (modelling local patterns within each chunk) and an inter-chunk path (modelling global patterns across chunks). This reduces the RNN sequence length from \\(L\\) to \\(\\sqrt{L}\\) in each dimension:</p> </li> </ul> \\[ \\begin{aligned} \\text{Intra-chunk:} \\quad &amp; h_{k,n}^{\\text{intra}} = \\text{BiLSTM}_{\\text{intra}}(z_{k,n}) \\\\ \\text{Inter-chunk:} \\quad &amp; h_{k,n}^{\\text{inter}} = \\text{BiLSTM}_{\\text{inter}}(h_{k,n}^{\\text{intra}}) \\end{aligned} \\] <ul> <li> <p>where \\(k\\) indexes the chunk and \\(n\\) indexes the position within the chunk. The intra-chunk LSTM processes across \\(n\\) for a fixed \\(k\\); the inter-chunk LSTM processes across \\(k\\) for a fixed \\(n\\).</p> </li> <li> <p>SepFormer (Subakan et al., 2021) replaces the RNNs in the dual-path framework with transformers (chapter 07). The intra-chunk transformer captures local dependencies with self-attention, and the inter-chunk transformer captures global dependencies. Multi-head attention's ability to model long-range dependencies without the vanishing gradient problem (chapter 06) makes SepFormer particularly effective for long recordings. SepFormer achieves state-of-the-art results on the WSJ0-2mix benchmark.</p> </li> <li> <p>Permutation Invariant Training (PIT) solves a fundamental problem in supervised source separation: the label assignment ambiguity. If the network has two outputs (for two speakers), which output should correspond to which speaker? There is no natural ordering. PIT computes the loss for all possible assignments and takes the minimum:</p> </li> </ul> \\[\\mathcal{L}_{\\text{PIT}} = \\min_{\\pi \\in \\mathcal{P}} \\sum_{c=1}^{C} \\ell(\\hat{s}_{\\pi(c)}, s_c)\\] <ul> <li> <p>where \\(\\mathcal{P}\\) is the set of all permutations of \\(\\{1, \\ldots, C\\}\\) and \\(\\ell\\) is the per-source loss (typically scale-invariant signal-to-distortion ratio, SI-SDR). For \\(C = 2\\) sources there are only 2 permutations; for \\(C = 3\\) there are 6. This is computed efficiently using the Hungarian algorithm for larger \\(C\\).</p> </li> <li> <p>Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) is the standard evaluation metric for source separation:</p> </li> </ul> \\[ \\begin{aligned} s_{\\text{target}} &amp;= \\frac{\\langle \\hat{s}, s \\rangle}{\\|s\\|^2} s \\\\ e_{\\text{noise}} &amp;= \\hat{s} - s_{\\text{target}} \\\\ \\text{SI-SDR} &amp;= 10 \\log_{10} \\frac{\\|s_{\\text{target}}\\|^2}{\\|e_{\\text{noise}}\\|^2} \\end{aligned} \\] <ul> <li> <p>where \\(\\hat{s}\\) is the estimated source and \\(s\\) is the ground truth. SI-SDR is invariant to the overall scale of the estimate, which is desirable because absolute volume is less important than the quality of the separation. Higher SI-SDR (in dB) is better. State-of-the-art systems achieve around 20-22 dB SI-SDR improvement on WSJ0-2mix.</p> </li> <li> <p>Music source separation separates a music recording into stems: vocals, drums, bass, and other instruments. This enables applications like karaoke (remove vocals), remixing (adjust instrument levels), and transcription (analyse one instrument at a time).</p> </li> <li> <p>Open-Unmix (Stoter et al., 2019) is a reference baseline that uses a 3-layer bidirectional LSTM to predict a soft mask for each source in the magnitude STFT domain. It processes each source independently with a dedicated model. Simple but effective, Open-Unmix established reproducible benchmarks on MUSDB18.</p> </li> <li> <p>Demucs (Defossez et al., 2019; updated as Hybrid Demucs, 2021) uses a U-Net architecture (chapter 08) that operates directly on the waveform. The encoder compresses the mixture through strided convolutions, the decoder expands it back through transposed convolutions with skip connections, and each source gets its own decoder head. Hybrid Demucs combines time-domain and frequency-domain processing: the encoder has parallel time-domain and STFT branches whose features are fused before the decoder. This captures both fine temporal details and spectral structure.</p> </li> <li> <p>Demucs achieves state-of-the-art separation quality on MUSDB18, with particularly strong vocal separation. Its U-Net architecture is reminiscent of the image segmentation architectures from chapter 08, treating the separation problem as a form of \"audio segmentation\".</p> </li> <li> <p>Active noise cancellation (ANC) reduces unwanted sound by generating an anti-noise signal that destructively interferes with the noise. Think of noise-cancelling headphones: a microphone picks up ambient noise, the ANC system generates an inverted version, and the combined signal (noise + anti-noise) ideally cancels to silence.</p> </li> <li> <p>The physics is simple: if the noise is \\(n(t)\\), generating \\(-n(t)\\) at the same point in space produces silence: \\(n(t) + (-n(t)) = 0\\). The challenge is that the anti-noise must be precisely aligned in time, amplitude, and phase. Even small errors produce residual noise or artifacts.</p> </li> <li> <p>Feedforward ANC uses a reference microphone that picks up the noise before it reaches the listener. The system has time to process the noise and generate the anti-noise. The reference signal passes through an adaptive filter whose output is subtracted from the noise at the error microphone (near the listener). This works well for predictable, broadband noise (engine hum, fan noise).</p> </li> <li> <p>Feedback ANC uses only an error microphone at the listener's ear. The system estimates the noise from the residual signal (what the listener actually hears) and adjusts the anti-noise. Feedback ANC is simpler (no reference microphone needed) but has limited bandwidth and can become unstable.</p> </li> <li> <p>Adaptive filtering is the mathematical engine behind ANC. The filter coefficients must continuously adapt to the changing noise environment. The most common algorithm is the Least Mean Squares (LMS) filter.</p> </li> </ul> <p></p> <ul> <li>LMS algorithm: an FIR filter with coefficients \\(\\mathbf{w} = [w_0, w_1, \\ldots, w_{L-1}]^T\\) processes the reference signal \\(\\mathbf{x}(n) = [x(n), x(n-1), \\ldots, x(n-L+1)]^T\\). The output is \\(y(n) = \\mathbf{w}^T \\mathbf{x}(n)\\), the error is \\(e(n) = d(n) - y(n)\\) (where \\(d(n)\\) is the desired/primary signal), and the weight update is:</li> </ul> \\[\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\mu \\, e(n) \\, \\mathbf{x}(n)\\] <ul> <li> <p>where \\(\\mu\\) is the step size (learning rate). This is a stochastic gradient descent step on the mean squared error \\(E[e^2(n)]\\), using the instantaneous gradient estimate \\(-2 e(n) \\mathbf{x}(n)\\) instead of the true gradient (chapter 03's gradient descent and chapter 06's SGD).</p> </li> <li> <p>The step size \\(\\mu\\) controls the trade-off between convergence speed and steady-state error. Too large and the filter oscillates or diverges; too small and adaptation is sluggish. The stability condition is \\(0 &lt; \\mu &lt; 2 / (\\lambda_{\\max})\\), where \\(\\lambda_{\\max}\\) is the largest eigenvalue of the input autocorrelation matrix \\(R = E[\\mathbf{x}\\mathbf{x}^T]\\).</p> </li> <li> <p>Normalised LMS (NLMS) normalises the step size by the input power, making convergence independent of the signal level:</p> </li> </ul> \\[\\mathbf{w}(n+1) = \\mathbf{w}(n) + \\frac{\\mu}{\\|\\mathbf{x}(n)\\|^2 + \\epsilon} \\, e(n) \\, \\mathbf{x}(n)\\] <ul> <li> <p>where \\(\\epsilon\\) is a small regularisation constant to prevent division by zero. NLMS converges more reliably than LMS because the effective step size adapts to the input power.</p> </li> <li> <p>Recursive Least Squares (RLS) is a faster-converging alternative that minimises the weighted least squares cost \\(\\sum_{k=1}^{n} \\lambda^{n-k} e^2(k)\\), where \\(\\lambda \\in (0, 1]\\) is a forgetting factor. RLS maintains an estimate of the inverse autocorrelation matrix and updates it recursively, achieving optimal convergence at the cost of \\(O(L^2)\\) computation per sample (versus \\(O(L)\\) for LMS).</p> </li> <li> <p>Noise reduction and speech enhancement aim to improve speech quality and intelligibility in noisy recordings. Unlike source separation (which separates distinct sources), speech enhancement specifically targets the speech-plus-noise case, recovering clean speech from a noisy observation.</p> </li> <li> <p>Spectral subtraction is the simplest approach. During noise-only frames (detected by VAD from file 03), estimate the noise spectrum \\(|\\hat{N}(f)|^2\\). Then subtract it from each frame:</p> </li> </ul> \\[|\\hat{S}(f)|^2 = \\max(|X(f)|^2 - \\alpha |\\hat{N}(f)|^2, \\beta |X(f)|^2)\\] <ul> <li> <p>where \\(\\alpha\\) is an over-subtraction factor (typically 1-4, aggressive subtraction removes more noise but introduces more artifacts) and \\(\\beta\\) is a spectral floor that prevents negative values and reduces \"musical noise\" artifacts (isolated tonal remnants that sound like random musical notes).</p> </li> <li> <p>Wiener filtering provides the minimum mean squared error estimate of the clean speech spectrum:</p> </li> </ul> \\[\\hat{S}(t, f) = \\frac{|S(t,f)|^2}{|S(t,f)|^2 + |N(t,f)|^2} \\cdot X(t, f) = G(t, f) \\cdot X(t, f)\\] <ul> <li> <p>The Wiener gain \\(G(t, f) = \\text{SNR}(t, f) / (1 + \\text{SNR}(t, f))\\) ranges from 0 (pure noise) to 1 (pure speech), acting as a soft mask. The challenge is estimating the speech and noise power spectra. The a priori SNR \\(\\xi(t, f) = |S(t,f)|^2 / |N(t,f)|^2\\) is estimated using the \"decision-directed\" approach: a smoothed combination of the current frame's estimate and the previous frame's Wiener-filtered output.</p> </li> <li> <p>Neural speech enhancement uses deep learning to estimate either a mask (like the Wiener gain) or the clean spectrogram directly. Architectures range from simple feedforward networks to U-Nets (chapter 08), CRNs (Convolutional Recurrent Networks), and transformers.</p> </li> <li> <p>DCCRN (Deep Complex Convolutional Recurrent Network) operates on the complex STFT (both magnitude and phase), using complex-valued convolutions that naturally handle the real and imaginary parts. This avoids the phase estimation problem that plagues magnitude-only approaches.</p> </li> <li> <p>FullSubNet uses a dual-path architecture with a full-band model (capturing global spectral patterns) and a sub-band model (capturing local harmonic details). The full-band model processes the entire spectrum, while the sub-band model processes narrow frequency bands centred on each frequency bin. Their outputs are combined for the final mask estimate.</p> </li> <li> <p>DNS (Deep Noise Suppression) Challenge by Microsoft benchmarks speech enhancement systems annually. Winners typically use large-scale training with diverse noise types, data augmentation (adding noise at various SNRs, reverberation, codec artifacts), and real-time-capable architectures.</p> </li> <li> <p>Echo cancellation removes acoustic echo in two-way communication. When you are on a phone call, the far-end speaker's voice plays through your loudspeaker, bounces around the room, and is picked up by your microphone, creating an echo that the far-end speaker hears. Acoustic Echo Cancellation (AEC) models the acoustic path from loudspeaker to microphone and subtracts the predicted echo.</p> </li> <li> <p>The acoustic path is modelled as an adaptive FIR filter (using LMS or NLMS) with the far-end signal as input. The filter models the room impulse response, which includes direct path, early reflections, and late reverberation. Room impulse responses can be hundreds of milliseconds long, requiring filters with thousands of taps.</p> </li> <li> <p>Double-talk detection is critical for AEC: when both the near-end and far-end speakers talk simultaneously, the adaptive filter must freeze (stop updating) to prevent it from cancelling the near-end speaker's voice. Double-talk detectors compare the energy of the error signal with the far-end signal energy; a sudden increase in error energy that is not explained by the far-end signal suggests near-end speech.</p> </li> <li> <p>The normalised cross-correlation between the far-end signal \\(x(n)\\) and the microphone signal \\(d(n)\\) provides a double-talk indicator:</p> </li> </ul> \\[\\xi(n) = \\frac{|\\sum_{k=0}^{L-1} x(n-k) d(n-k)|}{\\sqrt{\\sum_{k} x^2(n-k)} \\sqrt{\\sum_{k} d^2(n-k)}}\\] <ul> <li> <p>During single-talk (far-end only), \\(\\xi\\) is high because \\(d\\) is mostly echo of \\(x\\). During double-talk, \\(\\xi\\) drops because the near-end speech is uncorrelated with \\(x\\).</p> </li> <li> <p>Modern AEC systems combine adaptive filtering with neural networks: the adaptive filter provides an initial echo estimate, and a neural network (similar to the speech enhancement models above) cleans up residual echo and handles non-linearities (loudspeaker distortion) that linear filters cannot capture.</p> </li> <li> <p>Evaluation metrics for separation and enhancement:</p> <ul> <li>SI-SDR (defined above): standard for source separation.</li> <li>SDR (Signal-to-Distortion Ratio): from BSS Eval, measures overall separation quality including artifacts and interference.</li> <li>PESQ (Perceptual Evaluation of Speech Quality): ITU standard that predicts subjective quality scores. Range: -0.5 to 4.5.</li> <li>STOI (Short-Time Objective Intelligibility): predicts speech intelligibility. Range: 0 to 1.</li> <li>DNSMOS: Microsoft's deep noise suppression MOS predictor, a neural network trained to predict human MOS scores without requiring clean reference audio.</li> </ul> </li> </ul>"},{"location":"chapter%2009%3A%20audio%20and%20speech/05.%20source%20separation%20and%20noise/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ul> <li>Task 1: Independent Component Analysis for source separation. Implement FastICA to separate two mixed audio sources, demonstrating the classical cocktail party solution for the determined case (equal sources and microphones).</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\n# Generate two source signals\nsr = 8000\nduration = 1.0\nt = jnp.linspace(0, duration, int(sr * duration))\n\n# Source 1: sinusoidal (like a tone)\ns1 = jnp.sin(2 * jnp.pi * 440 * t) + 0.3 * jnp.sin(2 * jnp.pi * 880 * t)\n\n# Source 2: sawtooth-like (rich harmonics)\ns2 = 2 * (t * 200 % 1) - 1  # sawtooth at 200 Hz\n\n# Normalise sources\ns1 = s1 / jnp.max(jnp.abs(s1))\ns2 = s2 / jnp.max(jnp.abs(s2))\nsources = jnp.stack([s1, s2])  # (2, T)\n\n# Mixing matrix (unknown to the algorithm)\nA = jnp.array([[0.8, 0.4],\n               [0.3, 0.9]])\nmixtures = A @ sources  # (2, T)\n\n# FastICA implementation\ndef whiten(X):\n    \"\"\"Centre and whiten the data.\"\"\"\n    X_centered = X - jnp.mean(X, axis=1, keepdims=True)\n    cov = (X_centered @ X_centered.T) / X_centered.shape[1]\n    eigvals, eigvecs = jnp.linalg.eigh(cov)\n    D_inv_sqrt = jnp.diag(1.0 / jnp.sqrt(eigvals + 1e-8))\n    whitening = D_inv_sqrt @ eigvecs.T\n    return whitening @ X_centered, whitening\n\ndef fastica(X, n_components=2, max_iter=200, tol=1e-6):\n    \"\"\"FastICA using tanh non-linearity (approximation to negentropy).\"\"\"\n    X_white, whitening = whiten(X)\n    n, T = X_white.shape\n\n    key = jr.PRNGKey(42)\n    W = jr.normal(key, (n_components, n))\n    # Orthogonalise W\n    U, _, Vt = jnp.linalg.svd(W, full_matrices=False)\n    W = U @ Vt\n\n    for iteration in range(max_iter):\n        W_old = W.copy()\n\n        # For each component\n        for i in range(n_components):\n            w = W[i]\n            # w^T X_white: (T,)\n            wx = w @ X_white  # (T,)\n\n            # g(u) = tanh(u), g'(u) = 1 - tanh^2(u)\n            g_wx = jnp.tanh(wx)\n            g_prime_wx = 1 - g_wx ** 2\n\n            # Newton update: w_new = E[X * g(w^T X)] - E[g'(w^T X)] * w\n            w_new = jnp.mean(X_white * g_wx[None, :], axis=1) - \\\n                    jnp.mean(g_prime_wx) * w\n\n            # Decorrelate from previous components (deflation)\n            for j in range(i):\n                w_new = w_new - jnp.dot(w_new, W[j]) * W[j]\n\n            w_new = w_new / jnp.linalg.norm(w_new)\n            W = W.at[i].set(w_new)\n\n        # Check convergence\n        convergence = jnp.min(jnp.abs(jnp.diag(W @ W_old.T)))\n        if convergence &gt; 1 - tol:\n            print(f\"FastICA converged in {iteration + 1} iterations\")\n            break\n\n    # Unmixing matrix\n    unmixing = W @ whitening\n    recovered = unmixing @ X\n    return recovered, unmixing\n\nrecovered, W_unmix = fastica(mixtures)\n\n# Fix sign ambiguity (ICA can flip signs)\nfor i in range(2):\n    if jnp.corrcoef(recovered[i], sources[i])[0, 1] &lt; -0.5:\n        recovered = recovered.at[i].set(-recovered[i])\n\n# If sources are swapped, fix permutation\ncorr_00 = jnp.abs(jnp.corrcoef(recovered[0], sources[0])[0, 1])\ncorr_01 = jnp.abs(jnp.corrcoef(recovered[0], sources[1])[0, 1])\nif corr_01 &gt; corr_00:\n    recovered = recovered[::-1]\n\n# Normalise for display\nrecovered = recovered / jnp.max(jnp.abs(recovered), axis=1, keepdims=True)\n\nfig, axes = plt.subplots(3, 2, figsize=(14, 9))\n\naxes[0, 0].plot(t[:1000], s1[:1000], color='#3498db', linewidth=0.8)\naxes[0, 0].set_title('Source 1 (Original)')\naxes[0, 0].set_ylabel('Amplitude')\n\naxes[0, 1].plot(t[:1000], s2[:1000], color='#e74c3c', linewidth=0.8)\naxes[0, 1].set_title('Source 2 (Original)')\n\naxes[1, 0].plot(t[:1000], mixtures[0, :1000], color='#9b59b6', linewidth=0.8)\naxes[1, 0].set_title('Mixture 1 (Microphone 1)')\naxes[1, 0].set_ylabel('Amplitude')\n\naxes[1, 1].plot(t[:1000], mixtures[1, :1000], color='#9b59b6', linewidth=0.8)\naxes[1, 1].set_title('Mixture 2 (Microphone 2)')\n\naxes[2, 0].plot(t[:1000], recovered[0, :1000], color='#27ae60', linewidth=0.8)\naxes[2, 0].set_title('Recovered Source 1 (FastICA)')\naxes[2, 0].set_ylabel('Amplitude')\naxes[2, 0].set_xlabel('Time (s)')\n\naxes[2, 1].plot(t[:1000], recovered[1, :1000], color='#f39c12', linewidth=0.8)\naxes[2, 1].set_title('Recovered Source 2 (FastICA)')\naxes[2, 1].set_xlabel('Time (s)')\n\nplt.tight_layout()\nplt.show()\n\n# Report correlation with originals\nfor i in range(2):\n    corr = jnp.corrcoef(recovered[i], sources[i])[0, 1]\n    print(f\"Source {i+1} recovery correlation: {corr:.4f}\")\n</code></pre> <ul> <li>Task 2: NMF-based source separation on spectrograms. Use non-negative matrix factorisation (chapter 02) to separate a spectrogram into two components, demonstrating how NMF learns spectral dictionaries for each source.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\n# Generate two signals with distinct spectral characteristics\nsr = 8000\nduration = 1.0\nt = jnp.linspace(0, duration, int(sr * duration))\n\n# Source 1: low-frequency harmonic (simulating bass)\nsrc1 = (jnp.sin(2 * jnp.pi * 100 * t) +\n        0.5 * jnp.sin(2 * jnp.pi * 200 * t) +\n        0.3 * jnp.sin(2 * jnp.pi * 300 * t))\n\n# Source 2: high-frequency harmonic (simulating a flute)\nsrc2 = (jnp.sin(2 * jnp.pi * 800 * t) +\n        0.4 * jnp.sin(2 * jnp.pi * 1600 * t))\n\n# Time-varying amplitudes (sources active at different times)\nenv1 = jnp.where(t &lt; 0.5, 1.0, 0.3)\nenv2 = jnp.where(t &gt; 0.3, 1.0, 0.2)\nsrc1 = src1 * env1\nsrc2 = src2 * env2\n\nmixture = src1 + src2\n\n# Compute magnitude spectrogram (STFT)\nn_fft = 512\nhop = 128\nwindow = jnp.hanning(n_fft)\n\ndef compute_stft(signal, n_fft, hop, window):\n    n_frames = 1 + (len(signal) - n_fft) // hop\n    frames = jnp.stack([\n        signal[i * hop : i * hop + n_fft] * window\n        for i in range(n_frames)\n    ])\n    return jnp.fft.rfft(frames, n=n_fft)\n\nS_mix = compute_stft(mixture, n_fft, hop, window)\nV = jnp.abs(S_mix).T  # (F, T) - frequency x time\nphase = jnp.angle(S_mix).T\n\nF, T = V.shape\nprint(f\"Spectrogram shape: {F} freq bins x {T} time frames\")\n\n# NMF: V \u2248 WH using multiplicative update rules\ndef nmf(V, K, n_iter=200, key=jr.PRNGKey(0)):\n    \"\"\"Non-negative Matrix Factorisation with Frobenius norm.\"\"\"\n    k1, k2 = jr.split(key)\n    W = jnp.abs(jr.normal(k1, (F, K))) * 0.1 + 0.01  # (F, K)\n    H = jnp.abs(jr.normal(k2, (K, T))) * 0.1 + 0.01  # (K, T)\n\n    costs = []\n    for i in range(n_iter):\n        # Multiplicative update for H\n        WtV = W.T @ V\n        WtWH = W.T @ W @ H + 1e-8\n        H = H * (WtV / WtWH)\n\n        # Multiplicative update for W\n        VHt = V @ H.T\n        WHHt = W @ H @ H.T + 1e-8\n        W = W * (VHt / WHHt)\n\n        cost = jnp.sum((V - W @ H) ** 2)\n        costs.append(float(cost))\n\n    return W, H, costs\n\n# Run NMF with K=2 components\nK = 2\nW, H, costs = nmf(V, K, n_iter=300)\n\n# Reconstruct each source using soft masks\nV_hat = W @ H\nmask1 = (W[:, 0:1] @ H[0:1, :]) / (V_hat + 1e-8)\nmask2 = (W[:, 1:2] @ H[1:2, :]) / (V_hat + 1e-8)\n\nV_src1 = mask1 * V\nV_src2 = mask2 * V\n\n# Visualisation\nfig, axes = plt.subplots(3, 2, figsize=(14, 10))\n\n# Mixture spectrogram\naxes[0, 0].imshow(jnp.log1p(V), aspect='auto', origin='lower', cmap='magma')\naxes[0, 0].set_title('Mixture Spectrogram |X|')\naxes[0, 0].set_ylabel('Frequency bin')\n\n# NMF convergence\naxes[0, 1].plot(costs, color='#3498db', linewidth=1.5)\naxes[0, 1].set_title('NMF Convergence')\naxes[0, 1].set_xlabel('Iteration')\naxes[0, 1].set_ylabel('Frobenius cost')\naxes[0, 1].set_yscale('log')\n\n# Spectral basis vectors W\nfreq_hz = jnp.arange(F) * sr / n_fft\naxes[1, 0].plot(freq_hz, W[:, 0], color='#27ae60', linewidth=1.5,\n                label='Basis 1 (low freq)')\naxes[1, 0].plot(freq_hz, W[:, 1], color='#e74c3c', linewidth=1.5,\n                label='Basis 2 (high freq)')\naxes[1, 0].set_title('Learned Spectral Bases W')\naxes[1, 0].set_xlabel('Frequency (Hz)')\naxes[1, 0].set_ylabel('Magnitude')\naxes[1, 0].legend()\n\n# Temporal activations H\ntime_s = jnp.arange(T) * hop / sr\naxes[1, 1].plot(time_s, H[0], color='#27ae60', linewidth=1.5,\n                label='Activation 1')\naxes[1, 1].plot(time_s, H[1], color='#e74c3c', linewidth=1.5,\n                label='Activation 2')\naxes[1, 1].set_title('Temporal Activations H')\naxes[1, 1].set_xlabel('Time (s)')\naxes[1, 1].set_ylabel('Activation')\naxes[1, 1].legend()\n\n# Separated spectrograms\naxes[2, 0].imshow(jnp.log1p(V_src1), aspect='auto', origin='lower', cmap='magma')\naxes[2, 0].set_title('Separated Source 1 (low-frequency)')\naxes[2, 0].set_ylabel('Frequency bin')\naxes[2, 0].set_xlabel('Time frame')\n\naxes[2, 1].imshow(jnp.log1p(V_src2), aspect='auto', origin='lower', cmap='magma')\naxes[2, 1].set_title('Separated Source 2 (high-frequency)')\naxes[2, 1].set_xlabel('Time frame')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Reconstruction error: {jnp.sum((V - W @ H)**2):.2f}\")\nprint(f\"NMF learns spectral bases that capture each source's frequency profile.\")\n</code></pre> <ul> <li>Task 3: LMS adaptive filter for noise cancellation. Implement the LMS and NLMS algorithms for echo/noise cancellation, showing convergence behaviour and the effect of step size.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\n# Simulate an echo cancellation scenario\n# Far-end signal -&gt; room impulse response -&gt; echo at microphone\n# Near-end speech is the desired signal we want to preserve\n\nsr = 8000\nduration = 2.0\nn_samples = int(sr * duration)\nkey = jr.PRNGKey(42)\nkeys = jr.split(key, 5)\n\n# Far-end signal (reference): random speech-like signal\nfar_end = jr.normal(keys[0], (n_samples,)) * 0.5\n\n# Room impulse response (unknown to the algorithm)\nrir_length = 64\nrir = jnp.zeros(rir_length)\nrir = rir.at[0].set(0.8)   # direct path\nrir = rir.at[5].set(0.3)   # early reflection\nrir = rir.at[12].set(-0.2) # reflection\nrir = rir.at[25].set(0.1)  # late reflection\nrir = rir.at[40].set(-0.05)\n\n# Echo: convolution of far-end with RIR\necho = jnp.convolve(far_end, rir)[:n_samples]\n\n# Near-end speech (active in a portion of the signal)\nnear_end = jnp.zeros(n_samples)\nstart, end = n_samples // 3, 2 * n_samples // 3\nnear_speech = 0.3 * jnp.sin(\n    2 * jnp.pi * 300 * jnp.linspace(0, (end - start) / sr, end - start)\n)\nnear_end = near_end.at[start:end].set(near_speech)\n\n# Microphone signal: echo + near-end + noise\nnoise = jr.normal(keys[1], (n_samples,)) * 0.01\nmic_signal = echo + near_end + noise\n\n# LMS adaptive filter\ndef lms_filter(reference, desired, filter_length, mu):\n    \"\"\"Standard LMS adaptive filter.\"\"\"\n    n = len(reference)\n    w = jnp.zeros(filter_length)\n    output = jnp.zeros(n)\n    error = jnp.zeros(n)\n    w_history = []\n\n    for i in range(filter_length, n):\n        x = reference[i:i-filter_length:-1]  # reversed segment\n        if len(x) &lt; filter_length:\n            x = jnp.pad(x, (0, filter_length - len(x)))\n        x = reference[max(0, i-filter_length+1):i+1][::-1]\n\n        y = jnp.dot(w, x)\n        e = desired[i] - y\n        w = w + mu * e * x\n\n        output = output.at[i].set(y)\n        error = error.at[i].set(e)\n\n        if i % 500 == 0:\n            w_history.append(w.copy())\n\n    return output, error, w_history\n\n# NLMS adaptive filter\ndef nlms_filter(reference, desired, filter_length, mu, eps=1e-6):\n    \"\"\"Normalised LMS adaptive filter.\"\"\"\n    n = len(reference)\n    w = jnp.zeros(filter_length)\n    output = jnp.zeros(n)\n    error = jnp.zeros(n)\n\n    for i in range(filter_length, n):\n        x = reference[max(0, i-filter_length+1):i+1][::-1]\n\n        y = jnp.dot(w, x)\n        e = desired[i] - y\n        norm_factor = jnp.dot(x, x) + eps\n        w = w + (mu / norm_factor) * e * x\n\n        output = output.at[i].set(y)\n        error = error.at[i].set(e)\n\n    return output, error\n\n# Run LMS with different step sizes\nfilter_len = 64\nmu_values = [0.001, 0.01, 0.05]\ncolors_mu = ['#3498db', '#e74c3c', '#27ae60']\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Original signals\nt = jnp.arange(n_samples) / sr\naxes[0, 0].plot(t, mic_signal, color='#9b59b6', linewidth=0.5, alpha=0.7,\n                label='Mic (echo + near-end)')\naxes[0, 0].plot(t, echo, color='#e74c3c', linewidth=0.5, alpha=0.7,\n                label='Echo (to cancel)')\naxes[0, 0].plot(t, near_end, color='#27ae60', linewidth=0.8,\n                label='Near-end speech (to preserve)')\naxes[0, 0].set_title('Signal Components')\naxes[0, 0].set_xlabel('Time (s)')\naxes[0, 0].set_ylabel('Amplitude')\naxes[0, 0].legend(fontsize=8)\n\n# LMS convergence for different step sizes\nfor mu, color in zip(mu_values, colors_mu):\n    _, err, _ = lms_filter(far_end, mic_signal, filter_len, mu)\n    # Smoothed squared error\n    sq_err = err ** 2\n    window_size = 200\n    smoothed = jnp.convolve(sq_err, jnp.ones(window_size)/window_size,\n                             mode='valid')\n    axes[0, 1].plot(smoothed, color=color, linewidth=1.2,\n                    label=f'mu={mu}')\n\naxes[0, 1].set_title('LMS Convergence (smoothed MSE)')\naxes[0, 1].set_xlabel('Sample')\naxes[0, 1].set_ylabel('Squared Error')\naxes[0, 1].set_yscale('log')\naxes[0, 1].legend()\n\n# Best LMS result\n_, err_lms, w_hist = lms_filter(far_end, mic_signal, filter_len, 0.01)\naxes[1, 0].plot(t, mic_signal, color='#9b59b6', linewidth=0.5, alpha=0.4,\n                label='Before cancellation')\naxes[1, 0].plot(t, err_lms, color='#3498db', linewidth=0.5, alpha=0.8,\n                label='After LMS cancellation')\naxes[1, 0].plot(t, near_end, color='#27ae60', linewidth=0.8, alpha=0.5,\n                label='True near-end')\naxes[1, 0].set_title('LMS Echo Cancellation Result (mu=0.01)')\naxes[1, 0].set_xlabel('Time (s)')\naxes[1, 0].set_ylabel('Amplitude')\naxes[1, 0].legend(fontsize=8)\n\n# NLMS result\n_, err_nlms = nlms_filter(far_end, mic_signal, filter_len, 0.5)\naxes[1, 1].plot(t, mic_signal, color='#9b59b6', linewidth=0.5, alpha=0.4,\n                label='Before cancellation')\naxes[1, 1].plot(t, err_nlms, color='#f39c12', linewidth=0.5, alpha=0.8,\n                label='After NLMS cancellation')\naxes[1, 1].plot(t, near_end, color='#27ae60', linewidth=0.8, alpha=0.5,\n                label='True near-end')\naxes[1, 1].set_title('NLMS Echo Cancellation Result (mu=0.5)')\naxes[1, 1].set_xlabel('Time (s)')\naxes[1, 1].set_ylabel('Amplitude')\naxes[1, 1].legend(fontsize=8)\n\nplt.tight_layout()\nplt.show()\n\n# Measure echo reduction\necho_power = jnp.mean(echo ** 2)\nlms_residual = jnp.mean(err_lms[n_samples//2:] ** 2)  # after convergence\nnlms_residual = jnp.mean(err_nlms[n_samples//2:] ** 2)\nprint(f\"Echo power: {10*jnp.log10(echo_power):.1f} dB\")\nprint(f\"LMS residual: {10*jnp.log10(lms_residual):.1f} dB \"\n      f\"(ERLE: {10*jnp.log10(echo_power/lms_residual):.1f} dB)\")\nprint(f\"NLMS residual: {10*jnp.log10(nlms_residual):.1f} dB \"\n      f\"(ERLE: {10*jnp.log10(echo_power/nlms_residual):.1f} dB)\")\n</code></pre> <ul> <li>Task 4: Time-frequency masking for speech enhancement. Implement a simple spectral masking approach (ideal ratio mask) and compare it with spectral subtraction, visualising the separation quality on a synthetic noisy speech signal.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport matplotlib.pyplot as plt\n\n# Create synthetic \"speech\" and \"noise\" signals\nsr = 8000\nduration = 2.0\nt = jnp.linspace(0, duration, int(sr * duration))\n\n# Speech: harmonic series with time-varying amplitude (simulating speech)\nspeech = jnp.zeros_like(t)\nfor f0 in [150, 300, 450, 600, 900]:\n    amp_env = 0.5 + 0.5 * jnp.sin(2 * jnp.pi * 2.0 * t)  # 2 Hz modulation\n    speech = speech + (0.5 / (f0/150)) * amp_env * jnp.sin(2 * jnp.pi * f0 * t)\nspeech = speech / jnp.max(jnp.abs(speech))\n\n# Noise: band-limited noise\nkey = jr.PRNGKey(42)\nnoise_raw = jr.normal(key, t.shape) * 0.4\n\n# Mix at a given SNR\nsnr_db = 5.0\nspeech_power = jnp.mean(speech ** 2)\nnoise_power = jnp.mean(noise_raw ** 2)\nnoise_scale = jnp.sqrt(speech_power / (noise_power * 10 ** (snr_db / 10)))\nnoise = noise_raw * noise_scale\nmixture = speech + noise\n\n# STFT\nn_fft = 512\nhop = 128\nwindow = jnp.hanning(n_fft)\n\ndef stft(signal, n_fft, hop, window):\n    n_frames = 1 + (len(signal) - n_fft) // hop\n    frames = jnp.stack([\n        signal[i * hop : i * hop + n_fft] * window\n        for i in range(n_frames)\n    ])\n    return jnp.fft.rfft(frames, n=n_fft)\n\ndef istft(S, hop, window, length):\n    n_fft = (S.shape[1] - 1) * 2\n    n_frames = S.shape[0]\n    frames = jnp.fft.irfft(S, n=n_fft) * window[None, :]\n    output = jnp.zeros(length)\n    window_sum = jnp.zeros(length)\n    for i in range(n_frames):\n        start = i * hop\n        end = start + n_fft\n        if end &lt;= length:\n            output = output.at[start:end].add(frames[i])\n            window_sum = window_sum.at[start:end].add(window ** 2)\n    window_sum = jnp.maximum(window_sum, 1e-8)\n    return output / window_sum\n\nS_speech = stft(speech, n_fft, hop, window)\nS_noise = stft(noise, n_fft, hop, window)\nS_mix = stft(mixture, n_fft, hop, window)\n\nmag_speech = jnp.abs(S_speech)\nmag_noise = jnp.abs(S_noise)\nmag_mix = jnp.abs(S_mix)\nphase_mix = jnp.angle(S_mix)\n\n# Method 1: Ideal Ratio Mask (oracle - upper bound)\nirm = mag_speech ** 2 / (mag_speech ** 2 + mag_noise ** 2 + 1e-8)\nS_irm = (irm * mag_mix) * jnp.exp(1j * phase_mix)\nenhanced_irm = istft(S_irm, hop, window, len(mixture))\n\n# Method 2: Spectral subtraction\n# Estimate noise from first 0.2s (assumed silence)\nnoise_frames = int(0.2 * sr / hop)\nnoise_est = jnp.mean(mag_mix[:noise_frames] ** 2, axis=0, keepdims=True)\nalpha = 2.0  # over-subtraction factor\nbeta = 0.02  # spectral floor\nmag_sub = jnp.maximum(mag_mix ** 2 - alpha * noise_est, beta * mag_mix ** 2)\nmag_sub = jnp.sqrt(mag_sub)\nS_sub = mag_sub * jnp.exp(1j * phase_mix)\nenhanced_sub = istft(S_sub, hop, window, len(mixture))\n\n# Method 3: Wiener filter\nsnr_est = mag_mix ** 2 / (noise_est + 1e-8)\nwiener_gain = snr_est / (1 + snr_est)\nS_wiener = (wiener_gain * mag_mix) * jnp.exp(1j * phase_mix)\nenhanced_wiener = istft(S_wiener, hop, window, len(mixture))\n\n# Compute SI-SDR for each method\ndef si_sdr(estimate, reference):\n    \"\"\"Scale-invariant signal-to-distortion ratio.\"\"\"\n    ref = reference[:len(estimate)]\n    est = estimate[:len(reference)]\n    s_target = (jnp.dot(est, ref) / (jnp.dot(ref, ref) + 1e-8)) * ref\n    e_noise = est - s_target\n    return 10 * jnp.log10(jnp.dot(s_target, s_target) /\n                           (jnp.dot(e_noise, e_noise) + 1e-8))\n\nsi_sdr_mix = si_sdr(mixture, speech)\nsi_sdr_irm_val = si_sdr(enhanced_irm, speech)\nsi_sdr_sub_val = si_sdr(enhanced_sub, speech)\nsi_sdr_wiener_val = si_sdr(enhanced_wiener, speech)\n\n# Visualisation\nfig, axes = plt.subplots(3, 2, figsize=(14, 12))\n\n# Spectrograms\naxes[0, 0].imshow(jnp.log1p(mag_speech.T), aspect='auto', origin='lower',\n                   cmap='magma')\naxes[0, 0].set_title('Clean Speech Spectrogram')\naxes[0, 0].set_ylabel('Frequency bin')\n\naxes[0, 1].imshow(jnp.log1p(mag_mix.T), aspect='auto', origin='lower',\n                   cmap='magma')\naxes[0, 1].set_title(f'Noisy Mixture ({snr_db:.0f} dB SNR)')\n\n# Masks\naxes[1, 0].imshow(irm.T, aspect='auto', origin='lower', cmap='RdYlGn')\naxes[1, 0].set_title('Ideal Ratio Mask (Oracle)')\naxes[1, 0].set_ylabel('Frequency bin')\n\naxes[1, 1].imshow(wiener_gain.T, aspect='auto', origin='lower', cmap='RdYlGn',\n                   vmin=0, vmax=1)\naxes[1, 1].set_title('Estimated Wiener Gain')\n\n# Enhanced waveforms comparison\nn_show = 3000\naxes[2, 0].plot(t[:n_show], speech[:n_show], color='#27ae60', linewidth=0.8,\n                alpha=0.5, label='Clean')\naxes[2, 0].plot(t[:n_show], mixture[:n_show], color='#e74c3c', linewidth=0.5,\n                alpha=0.4, label='Noisy')\naxes[2, 0].plot(t[:n_show], enhanced_irm[:n_show], color='#3498db',\n                linewidth=0.8, label='IRM enhanced')\naxes[2, 0].set_title('Waveform Comparison (IRM)')\naxes[2, 0].set_xlabel('Time (s)')\naxes[2, 0].set_ylabel('Amplitude')\naxes[2, 0].legend(fontsize=8)\n\n# SI-SDR bar chart\nmethods = ['Mixture', 'Spectral\\nSubtraction', 'Wiener\\nFilter', 'Ideal Ratio\\nMask']\nsdr_values = [float(si_sdr_mix), float(si_sdr_sub_val),\n              float(si_sdr_wiener_val), float(si_sdr_irm_val)]\nbar_colors = ['#e74c3c', '#f39c12', '#9b59b6', '#27ae60']\nbars = axes[2, 1].bar(methods, sdr_values, color=bar_colors, alpha=0.8)\naxes[2, 1].set_ylabel('SI-SDR (dB)')\naxes[2, 1].set_title('Enhancement Quality Comparison')\nfor bar, val in zip(bars, sdr_values):\n    axes[2, 1].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.3,\n                    f'{val:.1f}', ha='center', fontsize=10)\naxes[2, 1].axhline(0, color='gray', linestyle='--', linewidth=0.8)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"SI-SDR (noisy mixture):        {si_sdr_mix:.2f} dB\")\nprint(f\"SI-SDR (spectral subtraction): {si_sdr_sub_val:.2f} dB\")\nprint(f\"SI-SDR (Wiener filter):        {si_sdr_wiener_val:.2f} dB\")\nprint(f\"SI-SDR (ideal ratio mask):     {si_sdr_irm_val:.2f} dB (oracle upper bound)\")\n</code></pre>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/","title":"Multimodal Representations","text":"<p>Multimodal representations bridge vision, language, and audio into shared embedding spaces. This file covers fusion strategies, CLIP, ALIGN, SigLIP, contrastive loss functions (InfoNCE, NT-Xent), zero-shot classification, and retrieval evaluation.</p> <ul> <li> <p>Imagine you are sitting in a cafe. You see a steaming cup on the table, hear the clinking of ceramic, smell roasted coffee beans, and feel warmth radiating from the mug. No single sense tells you everything: your brain fuses these signals into a unified percept of \"hot coffee.\" Multimodal learning does the same thing for machines: it combines information from multiple modalities (vision, language, audio, and others) to build richer, more robust representations than any single modality provides alone.</p> </li> <li> <p>A modality is a distinct channel of information. In machine learning, the most common modalities are images (pixel grids), text (token sequences), audio (waveforms or spectrograms, as in Chapter 9), video (sequences of frames), and structured data (tables, graphs). Each modality has its own statistical structure: images are spatially coherent, text is sequential and discrete, audio is temporal and continuous. The challenge of multimodal learning is bridging these fundamentally different data types.</p> </li> <li> <p>Why bother combining modalities? Because they provide complementary information. A photograph of a dog tells you about its breed and colour but not its name. A caption like \"my golden retriever Max\" tells you the name and breed but not the exact pose. Together, the image and text give a fuller picture than either alone. This complementarity is the core motivation: multimodal models can answer questions, generate content, and make decisions that no unimodal model can.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#fusion-strategies","title":"Fusion Strategies","text":"<ul> <li> <p>Think of a group project. You can combine ideas in two ways: everyone works together in the same room from the start (sharing raw notes and drafts), or each person writes their section independently and you merge the final documents. These correspond to early fusion and late fusion in multimodal learning.</p> </li> <li> <p>Early fusion (also called feature-level fusion) concatenates or mixes raw or low-level features from different modalities before any serious processing happens. For example, you might concatenate an image's pixel features with a text's token embeddings and feed the combined sequence into a single transformer. The model can learn fine-grained cross-modal interactions from the start, but the input space is large and the model must learn to handle very different data types simultaneously.</p> </li> <li> <p>Formally, given feature vectors \\(x_{\\text{img}} \\in \\mathbb{R}^{d_1}\\) and \\(x_{\\text{txt}} \\in \\mathbb{R}^{d_2}\\) from two modalities, early fusion simply concatenates them:</p> </li> </ul> \\[x_{\\text{fused}} = [x_{\\text{img}}; x_{\\text{txt}}] \\in \\mathbb{R}^{d_1 + d_2}\\] <ul> <li> <p>This concatenated vector is then processed by a shared network. The advantage is that the model can discover cross-modal correlations at every layer. The disadvantage is computational cost and the difficulty of aligning very different feature types (dense pixel values vs. sparse token indices).</p> </li> <li> <p>Late fusion (also called decision-level fusion) processes each modality independently through its own encoder, producing a high-level representation or even a final prediction for each. These outputs are then combined, typically by averaging scores, voting, or a learned combination layer. Late fusion is simpler and lets you reuse pre-trained unimodal models off the shelf, but it cannot capture low-level cross-modal interactions because the modalities never \"see\" each other's raw features.</p> </li> <li> <p>Given modality-specific predictions \\(\\hat{y}_1\\) and \\(\\hat{y}_2\\), a simple late fusion rule is:</p> </li> </ul> \\[\\hat{y} = \\alpha \\hat{y}_1 + (1 - \\alpha) \\hat{y}_2\\] <ul> <li> <p>where \\(\\alpha \\in [0, 1]\\) is a learned or hand-tuned mixing weight.</p> </li> <li> <p>Middle fusion (also called intermediate fusion) is the pragmatic middle ground used by most modern systems. Each modality is first processed by its own encoder (extracting modality-specific features), and then the encoded representations are combined partway through the network, often via cross-attention layers. This lets each encoder specialise in its modality while still enabling rich cross-modal interactions. Flamingo, LLaVA, and most vision-language models (file 02) use middle fusion.</p> </li> </ul> <p></p> <ul> <li>The choice between fusion strategies depends on data availability, computational budget, and the task. Early fusion is powerful but data-hungry. Late fusion is cheap but limited. Middle fusion with cross-attention has become the dominant approach in large-scale multimodal models because it balances expressiveness with modularity.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#joint-embedding-spaces","title":"Joint Embedding Spaces","text":"<ul> <li> <p>Imagine a universal translator that can take any sentence in any language and map it to the same point in a shared \"meaning space.\" The sentence \"a dog on a beach\" in English, French, or Japanese would all land at the same coordinate. Joint embedding spaces do exactly this, but across modalities: an image of a dog on a beach and the text \"a dog on a beach\" should map to nearby points in the same vector space.</p> </li> <li> <p>Formally, we learn two encoder functions: \\(f_\\theta : \\mathcal{X}_1 \\to \\mathbb{R}^d\\) for modality 1 (e.g., images) and \\(g_\\phi : \\mathcal{X}_2 \\to \\mathbb{R}^d\\) for modality 2 (e.g., text). Both map their inputs into the same \\(d\\)-dimensional space. The training objective ensures that semantically matched pairs \\((x_1, x_2)\\) have embeddings \\(f_\\theta(x_1)\\) and \\(g_\\phi(x_2)\\) that are close (high cosine similarity), while unmatched pairs are far apart.</p> </li> <li> <p>This is a direct generalisation of the word embedding spaces from Chapter 7. Recall that Word2Vec and GloVe placed semantically similar words near each other in a vector space. Joint embedding spaces extend this idea across modalities: instead of word-to-word similarity, we measure image-to-text similarity, audio-to-text similarity, or even image-to-audio similarity.</p> </li> <li> <p>The similarity metric is almost always cosine similarity (Chapter 1):</p> </li> </ul> \\[\\text{sim}(u, v) = \\frac{u \\cdot v}{\\|u\\| \\|v\\|}\\] <ul> <li>By \\(L_2\\)-normalising all embeddings onto the unit hypersphere, cosine similarity reduces to a simple dot product \\(u \\cdot v\\), which is extremely efficient to compute and can be accelerated with approximate nearest-neighbour libraries.</li> </ul> <p></p> <ul> <li>The power of a joint embedding space is that it enables zero-shot transfer. Once you have aligned image and text embeddings, you can classify images into categories you have never trained on: just embed the category names as text and find which text embedding is closest to the image embedding. No task-specific fine-tuning is needed. This is the key insight behind CLIP and its successors.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#contrastive-learning-for-multimodal-alignment","title":"Contrastive Learning for Multimodal Alignment","text":"<ul> <li> <p>Think of a classroom exercise where students are given shuffled pairs of photos and captions, and asked to match each photo with its correct caption. To do this well, you need to understand both the visual content and the language, and know how they relate. Contrastive learning trains models in exactly this way: given a batch of (image, text) pairs, the model must figure out which image goes with which text.</p> </li> <li> <p>As we saw in Chapter 8 (file 04), contrastive learning in the unimodal setting (SimCLR, MoCo) pulls together augmented views of the same image and pushes apart views of different images. Multimodal contrastive learning replaces \"augmented views\" with \"matched modalities\": an image and its caption are the positive pair; the image paired with any other caption in the batch is a negative pair.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#clip","title":"CLIP","text":"<ul> <li> <p>CLIP (Contrastive Language-Image Pre-training, Radford et al., 2021) is the foundational model for multimodal contrastive learning. It trains an image encoder (a ViT or ResNet, Chapter 8) and a text encoder (a transformer, Chapter 7) jointly on 400 million (image, text) pairs scraped from the internet.</p> </li> <li> <p>Given a batch of \\(N\\) image-text pairs, CLIP computes the \\(N \\times N\\) matrix of cosine similarities between all image embeddings and all text embeddings. The diagonal entries are the matched pairs (positives); all off-diagonal entries are unmatched (negatives). The training loss pushes diagonal entries high and off-diagonal entries low.</p> </li> <li> <p>The loss is a symmetric cross-entropy. For image \\(i\\) paired with text \\(j = i\\), the image-to-text loss is:</p> </li> </ul> \\[\\mathcal{L}_{i \\to t} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(z_i^{\\text{img}}, z_i^{\\text{txt}}) / \\tau)}{\\sum_{k=1}^{N} \\exp(\\text{sim}(z_i^{\\text{img}}, z_k^{\\text{txt}}) / \\tau)}\\] <ul> <li>and the text-to-image loss is the same with the roles swapped:</li> </ul> \\[\\mathcal{L}_{t \\to i} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log \\frac{\\exp(\\text{sim}(z_i^{\\text{txt}}, z_i^{\\text{img}}) / \\tau)}{\\sum_{k=1}^{N} \\exp(\\text{sim}(z_i^{\\text{txt}}, z_k^{\\text{img}}) / \\tau)}\\] <ul> <li>The total CLIP loss is the average:</li> </ul> \\[\\mathcal{L}_{\\text{CLIP}} = \\frac{1}{2}(\\mathcal{L}_{i \\to t} + \\mathcal{L}_{t \\to i})\\] <ul> <li>Here \\(\\tau\\) is a learned temperature parameter (initialised at \\(\\tau = 0.07\\)). Temperature controls the sharpness of the softmax distribution: low \\(\\tau\\) makes the model focus harder on the closest match, high \\(\\tau\\) spreads probability more evenly. CLIP learns \\(\\tau\\) jointly with the model weights rather than treating it as a fixed hyperparameter.</li> </ul> <p></p> <ul> <li> <p>CLIP's image encoder is typically a ViT-L/14 (a large Vision Transformer with 14x14 patches, Chapter 8 file 04). The text encoder is a 12-layer transformer with causal masking (like GPT, Chapter 7 file 04). Both encoders project their outputs to a shared 512- or 768-dimensional space via a learned linear projection, followed by \\(L_2\\) normalisation.</p> </li> <li> <p>CLIP's most remarkable property is zero-shot image classification. To classify an image into one of \\(K\\) categories, you create \\(K\\) text prompts like \"a photo of a {class name}\", embed each prompt with the text encoder, embed the image with the image encoder, and pick the class whose text embedding has the highest cosine similarity with the image embedding. On ImageNet, CLIP achieves competitive accuracy without ever seeing a single ImageNet training example.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#align","title":"ALIGN","text":"<ul> <li>ALIGN (Jia et al., 2021) scales CLIP's approach to a noisier, larger dataset: 1.8 billion image-text pairs with minimal filtering. Where CLIP carefully curated its data, ALIGN shows that scale can compensate for noise. ALIGN uses an EfficientNet image encoder and a BERT text encoder, and trains with the same contrastive loss. The key finding is that with enough data, you do not need expensive data cleaning: the contrastive objective naturally downweights noisy pairs because they produce inconsistent gradients.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#siglip","title":"SigLIP","text":"<ul> <li> <p>SigLIP (Sigmoid Loss for Language-Image Pre-training, Zhai et al., 2023) replaces CLIP's softmax-based contrastive loss with a simpler sigmoid loss. Instead of treating the \\(N \\times N\\) similarity matrix as a classification problem (each row is a softmax over columns), SigLIP treats each entry independently as a binary classification: is this (image, text) pair matched or not?</p> </li> <li> <p>The SigLIP loss for a single pair \\((i, j)\\) is:</p> </li> </ul> \\[\\mathcal{L}_{ij} = -y_{ij} \\log \\sigma(z_i^{\\text{img}} \\cdot z_j^{\\text{txt}} / \\tau) - (1 - y_{ij}) \\log(1 - \\sigma(z_i^{\\text{img}} \\cdot z_j^{\\text{txt}} / \\tau))\\] <ul> <li> <p>where \\(y_{ij} = 1\\) if \\(i = j\\) (matched) and \\(y_{ij} = 0\\) otherwise, and \\(\\sigma\\) is the sigmoid function.</p> </li> <li> <p>The crucial advantage of SigLIP is that it eliminates the need for a global softmax normalisation across the entire batch. In CLIP, the softmax denominator requires gathering all embeddings across all devices, which is a communication bottleneck in distributed training. SigLIP's per-pair sigmoid loss can be computed locally, enabling more efficient scaling to very large batches. SigLIP matches CLIP's quality with lower training cost.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#contrastive-loss-functions-in-detail","title":"Contrastive Loss Functions in Detail","text":"<ul> <li>The loss functions used in contrastive learning share a common structure: they all try to make the similarity score of positive pairs higher than that of negative pairs, with some notion of \"margin\" or \"temperature\" controlling how hard the model pushes. Let us formalise the key variants.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#infonce","title":"InfoNCE","text":"<ul> <li>InfoNCE (Noise-Contrastive Estimation, van den Oord et al., 2018) is the theoretical foundation behind CLIP's loss. Given a query \\(q\\), one positive key \\(k^+\\), and \\(K\\) negative keys \\(\\{k_1^-, \\ldots, k_K^-\\}\\), the loss is:</li> </ul> \\[\\mathcal{L}_{\\text{InfoNCE}} = -\\log \\frac{\\exp(q \\cdot k^+ / \\tau)}{\\exp(q \\cdot k^+ / \\tau) + \\sum_{j=1}^{K} \\exp(q \\cdot k_j^- / \\tau)}\\] <ul> <li>This is a \\((K+1)\\)-way classification problem: identify the positive among \\(K+1\\) candidates. InfoNCE is a lower bound on the mutual information between the query and the positive key, which is why maximising it aligns representations of semantically matched inputs. The bound tightens as the number of negatives \\(K\\) increases, which explains why contrastive methods benefit from large batch sizes.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#nt-xent","title":"NT-Xent","text":"<ul> <li>NT-Xent (Normalised Temperature-scaled Cross-Entropy, Chen et al., 2020) is the loss used in SimCLR (Chapter 8 file 04) and is essentially InfoNCE applied symmetrically within a batch. For a batch of \\(N\\) pairs, the \\(2N\\) augmented views produce \\(2N - 2\\) negatives for each anchor (all views except itself and its positive). The loss for a positive pair \\((i, j)\\) is:</li> </ul> \\[\\ell_{i,j} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j) / \\tau)}{\\sum_{k=1}^{2N} \\mathbf{1}_{[k \\neq i]} \\exp(\\text{sim}(z_i, z_k) / \\tau)}\\] <ul> <li>NT-Xent and InfoNCE are the same mathematical formula; the names differ because they were introduced in different contexts (self-supervised vision vs. representation learning theory).</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#the-role-of-temperature","title":"The Role of Temperature","text":"<ul> <li> <p>The temperature \\(\\tau\\) is one of the most important hyperparameters in contrastive learning. To build intuition, think of temperature in the physical sense: at high temperature, molecules move randomly (the softmax is flat, all negatives look equally bad); at low temperature, molecules settle into rigid structures (the softmax is peaked, only the hardest negatives matter).</p> </li> <li> <p>Formally, as \\(\\tau \\to 0\\), the softmax approaches a hard argmax that selects only the single hardest negative. As \\(\\tau \\to \\infty\\), all negatives contribute equally. In practice, \\(\\tau \\in [0.01, 0.1]\\) works well for normalised embeddings. Too-low temperature causes training instability (gradients become very large for hard negatives); too-high temperature makes the loss insensitive to violations.</p> </li> <li> <p>CLIP initialises \\(\\tau = 0.07\\) and learns it as a log-parametrised scalar \\(\\tau = \\exp(t)\\), where \\(t\\) is updated by gradient descent alongside the model weights. This allows the model to automatically adjust the difficulty of the contrastive task during training.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#triplet-loss-and-margin-based-alternatives","title":"Triplet Loss and Margin-Based Alternatives","text":"<ul> <li>Before InfoNCE dominated, triplet loss was the standard for metric learning. Given an anchor \\(a\\), a positive \\(p\\), and a negative \\(n\\):</li> </ul> \\[\\mathcal{L}_{\\text{triplet}} = \\max(0, \\|a - p\\|^2 - \\|a - n\\|^2 + m)\\] <ul> <li> <p>where \\(m\\) is a margin that ensures the positive is at least \\(m\\) closer than the negative. Triplet loss operates on individual triplets rather than batches, making it less sample-efficient than InfoNCE. It is also sensitive to the mining strategy: random negatives are often too easy (the loss is zero), so hard negative mining (selecting the closest incorrect match) or semi-hard mining (selecting negatives within the margin) is critical.</p> </li> <li> <p>InfoNCE implicitly performs hard negative mining across the entire batch, which is one reason it outperforms triplet loss at scale. The softmax normalisation in InfoNCE automatically upweights hard negatives (those with high similarity to the anchor), providing a natural curriculum without explicit mining.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#image-text-retrieval-and-zero-shot-classification","title":"Image-Text Retrieval and Zero-Shot Classification","text":"<ul> <li> <p>Once you have a trained joint embedding space, you can perform image-text retrieval: given an image query, find the most relevant texts from a database (image-to-text retrieval), or given a text query, find the most relevant images (text-to-image retrieval). This is simply a nearest-neighbour search in the shared embedding space.</p> </li> <li> <p>Imagine a librarian who can instantly compare any photograph with any caption in a million-item catalogue. They do not need to understand every possible category in advance; they just measure how \"close\" each photo is to each caption. This is how CLIP-style models perform retrieval and zero-shot classification.</p> </li> <li> <p>Zero-shot classification is a special case of text-to-image retrieval. Given \\(K\\) class names, you construct text prompts \\(\\{t_1, \\ldots, t_K\\}\\) (e.g., \"a photo of a cat\", \"a photo of a dog\") and embed them. For a new image \\(x\\), the predicted class is:</p> </li> </ul> \\[\\hat{y} = \\arg\\max_{k} \\; \\text{sim}(f_\\theta(x), g_\\phi(t_k))\\] <ul> <li> <p>The key insight is that the text encoder acts as a flexible classifier head. Instead of training a new linear layer for each downstream task, you simply describe the task in natural language. This is why CLIP generalises so well: the text encoder has seen millions of diverse descriptions during pre-training.</p> </li> <li> <p>Prompt engineering matters. CLIP's zero-shot accuracy on ImageNet improves from 63.2% to 68.4% just by changing the prompt template from \"{class name}\" to \"a photo of a {class name}.\" Even better, prompt ensembling averages the text embeddings of multiple templates (e.g., \"a photo of a {class name}\", \"a good photo of a {class name}\", \"a drawing of a {class name}\") to produce a more robust text representation.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#audio-visual-correspondence","title":"Audio-Visual Correspondence","text":"<ul> <li> <p>Close your eyes and listen to someone bouncing a basketball. You can tell when it hits the floor from the rhythmic thuds. Now open your eyes: the visual bounce aligns perfectly with each thud. This tight correspondence between audio and visual events is a free supervisory signal that machines can learn from. Audio-visual correspondence learning trains models to associate sounds with their visual sources without any human labels.</p> </li> <li> <p>The idea is strikingly similar to CLIP, but replaces text with audio. Given paired video frames and audio segments, the model learns an embedding space where temporally aligned audio-visual pairs are close and misaligned pairs are far apart.</p> </li> <li> <p>Audio-Visual Embedding (AVE) methods (Arandjelovic and Zisserman, 2017) train a visual encoder \\(f\\) and an audio encoder \\(g\\) with a contrastive loss on video data. The positive pair is (video frame, audio clip from the same time), and negatives are audio clips from different videos or different times. The model learns that a barking sound goes with the image of a dog, and a guitar sound goes with the image of a guitar, all without labels.</p> </li> <li> <p>The audio encoder typically processes log-mel spectrograms (Chapter 9 file 01) using a CNN or audio transformer, producing a fixed-size embedding. The visual encoder processes video frames using a standard image backbone (ResNet, ViT). Both project to a shared \\(d\\)-dimensional space, and training uses the same InfoNCE loss as CLIP:</p> </li> </ul> \\[\\mathcal{L}_{\\text{AV}} = -\\log \\frac{\\exp(\\text{sim}(z^{\\text{vis}}, z^{\\text{aud}}) / \\tau)}{\\sum_{k=1}^{N} \\exp(\\text{sim}(z^{\\text{vis}}, z_k^{\\text{aud}}) / \\tau)}\\] <p></p> <ul> <li> <p>Applications of audio-visual learning include: sound source localisation (where in the image is the sound coming from?), audio-visual speech recognition (combining lip movements with audio, as in Chapter 9 file 02), audio-visual source separation (isolating one speaker's voice by watching their face, the \"cocktail party\" problem from Chapter 9 file 05), and video generation conditioned on audio.</p> </li> <li> <p>ImageBind (Girdhar et al., 2023) extends this to six modalities: images, text, audio, depth, thermal, and IMU data. The key insight is that you do not need paired data for every combination. By aligning each modality to images (using image-text pairs for text, image-audio pairs for audio, etc.), all modalities become implicitly aligned through the shared image embedding space. This \"binding\" through a common anchor modality produces an emergent alignment: audio and text become similar even though they were never directly trained together.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#evaluation","title":"Evaluation","text":"<ul> <li>Evaluating multimodal models requires metrics that capture cross-modal understanding. The two dominant evaluation paradigms are zero-shot benchmarks and retrieval metrics.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#zero-shot-benchmarks","title":"Zero-Shot Benchmarks","text":"<ul> <li> <p>Zero-shot evaluation measures whether a model can perform tasks it was never explicitly trained for. The most common benchmark is ImageNet zero-shot accuracy: embed all 1,000 ImageNet class names as text, embed each test image, and measure top-1 and top-5 classification accuracy based on cosine similarity. CLIP ViT-L/14 achieves 75.5% top-1 accuracy zero-shot, comparable to a supervised ResNet-50 trained on ImageNet.</p> </li> <li> <p>Other zero-shot benchmarks include: CIFAR-10/100, STL-10, Food-101, Oxford Pets, and Flowers-102. Evaluating across many datasets tests whether the model has genuinely general visual understanding or has merely memorised patterns from its pre-training data.</p> </li> <li> <p>Linear probe evaluation is a complementary test. You freeze the pre-trained image encoder, extract features for a labelled dataset, and train a simple linear classifier on top. This measures the quality of the learned representations independently of the zero-shot retrieval mechanism. CLIP's features are excellent linear probe features, often matching or exceeding supervised pre-training.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#retrieval-metrics","title":"Retrieval Metrics","text":"<ul> <li> <p>For retrieval tasks (image-to-text and text-to-image), the standard metric is Recall@K (R@K): the fraction of queries for which the correct match appears in the top \\(K\\) retrieved results. Common values are R@1, R@5, and R@10.</p> </li> <li> <p>Formally, for a set of \\(Q\\) queries:</p> </li> </ul> \\[\\text{R@}K = \\frac{1}{Q} \\sum_{q=1}^{Q} \\mathbf{1}[\\text{rank}(q) \\leq K]\\] <ul> <li> <p>where \\(\\text{rank}(q)\\) is the position of the correct match in the ranked retrieval list for query \\(q\\).</p> </li> <li> <p>Standard retrieval benchmarks include Flickr30K (31,000 images, each with 5 captions) and MS-COCO (123,000 images, each with 5 captions). Evaluation is done on the test split: given an image, retrieve the correct caption(s) from the full test set, and vice versa.</p> </li> <li> <p>Median rank (MedR) is a complementary metric: the median position of the correct match across all queries. A perfect model has MedR = 1. Lower is better.</p> </li> <li> <p>Beyond retrieval, multimodal models are also evaluated on compositional understanding benchmarks like Winoground (which tests whether the model can distinguish \"a mug in a dog\" from \"a dog in a mug\") and ARO (Attribute, Relation, Order), which test whether the model genuinely understands the structure of language or merely matches bags of words. CLIP-style models often struggle on these, revealing a fundamental limitation: contrastive pre-training aligns global semantics but may not capture fine-grained compositional structure.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#putting-it-all-together","title":"Putting It All Together","text":"<ul> <li> <p>The multimodal representations covered in this file form the foundation for everything that follows in this chapter. The joint embedding spaces trained by CLIP and its successors are the \"glue\" that connects vision and language. File 02 builds on this foundation with vision-language models that go beyond retrieval to generate text about images. File 03 explores how images and video are tokenised for use in sequence models. File 04 covers cross-modal generation (text-to-image, text-to-video). And file 05 examines unified architectures that handle multiple modalities within a single model.</p> </li> <li> <p>The core takeaway: contrastive learning on paired data produces embedding spaces where different modalities are interchangeable. An image embedding and a text embedding become \"the same kind of thing,\" enabling zero-shot classification, retrieval, and seamless integration into larger systems. The simplicity of this idea, just push matched pairs together and unmatched pairs apart, belies its extraordinary effectiveness.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/01.%20multimodal%20representations/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement the CLIP contrastive loss from scratch. Create random image and text embeddings, compute the similarity matrix, and calculate the symmetric cross-entropy loss. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef clip_loss(image_embeds, text_embeds, temperature=0.07):\n    \"\"\"Compute symmetric CLIP contrastive loss.\"\"\"\n    # L2 normalise embeddings\n    image_embeds = image_embeds / jnp.linalg.norm(image_embeds, axis=1, keepdims=True)\n    text_embeds = text_embeds / jnp.linalg.norm(text_embeds, axis=1, keepdims=True)\n\n    # Compute cosine similarity matrix (N x N)\n    logits = image_embeds @ text_embeds.T / temperature  # (N, N)\n\n    # Labels: the diagonal (i-th image matches i-th text)\n    N = logits.shape[0]\n    labels = jnp.arange(N)\n\n    # Symmetric cross-entropy: image-to-text + text-to-image\n    loss_i2t = -jnp.mean(jax.nn.log_softmax(logits, axis=1)[jnp.arange(N), labels])\n    loss_t2i = -jnp.mean(jax.nn.log_softmax(logits, axis=0)[labels, jnp.arange(N)])\n    return (loss_i2t + loss_t2i) / 2, logits * temperature\n\n# Simulate a batch of 8 image-text pairs in 64-dim space\nkey = jax.random.PRNGKey(42)\nk1, k2 = jax.random.split(key)\nN, D = 8, 64\nimage_embeds = jax.random.normal(k1, (N, D))\ntext_embeds = jax.random.normal(k2, (N, D))\n\nloss, sim_matrix = clip_loss(image_embeds, text_embeds)\nprint(f\"CLIP loss (random embeddings): {loss:.4f}\")\n\n# Visualise the similarity matrix\nfig, ax = plt.subplots(figsize=(6, 5))\nim = ax.imshow(sim_matrix, cmap='coolwarm', vmin=-1, vmax=1)\nax.set_xlabel(\"Text index\"); ax.set_ylabel(\"Image index\")\nax.set_title(f\"Cosine Similarity Matrix (loss={loss:.3f})\")\nplt.colorbar(im); plt.tight_layout(); plt.show()\n# Try changing temperature (0.01, 0.1, 1.0) and observe how loss changes\n# Try making matched pairs similar: set text_embeds = image_embeds + small noise\n</code></pre></p> </li> <li> <p>Build a toy joint embedding model that learns to align 2D \"images\" (random vectors) with \"captions\" (different random vectors) using InfoNCE loss and gradient descent. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef info_nce_loss(img_enc, txt_enc, img_data, txt_data, tau=0.1):\n    \"\"\"InfoNCE over a batch of paired (image, text) data.\"\"\"\n    z_img = img_data @ img_enc  # (N, D)\n    z_txt = txt_data @ txt_enc  # (N, D)\n    # L2 normalise\n    z_img = z_img / jnp.linalg.norm(z_img, axis=1, keepdims=True)\n    z_txt = z_txt / jnp.linalg.norm(z_txt, axis=1, keepdims=True)\n    logits = z_img @ z_txt.T / tau\n    labels = jnp.arange(logits.shape[0])\n    return -jnp.mean(jax.nn.log_softmax(logits, axis=1)[jnp.arange(len(labels)), labels])\n\n# Create 32 paired samples: img in R^8, txt in R^6, embed into R^4\nkey = jax.random.PRNGKey(0)\nk1, k2, k3, k4 = jax.random.split(key, 4)\nN, d_img, d_txt, d_embed = 32, 8, 6, 4\n\nimg_data = jax.random.normal(k1, (N, d_img))\ntxt_data = jax.random.normal(k2, (N, d_txt))\n\n# Learnable projection matrices\nimg_enc = jax.random.normal(k3, (d_img, d_embed)) * 0.1\ntxt_enc = jax.random.normal(k4, (d_txt, d_embed)) * 0.1\n\ngrad_fn = jax.jit(jax.grad(info_nce_loss, argnums=(0, 1)))\nlr = 0.05\nlosses = []\n\nfor step in range(300):\n    loss = info_nce_loss(img_enc, txt_enc, img_data, txt_data)\n    losses.append(float(loss))\n    g_img, g_txt = grad_fn(img_enc, txt_enc, img_data, txt_data)\n    img_enc = img_enc - lr * g_img\n    txt_enc = txt_enc - lr * g_txt\n\nprint(f\"Initial loss: {losses[0]:.3f}, Final loss: {losses[-1]:.3f}\")\nprint(f\"Random baseline (log N): {jnp.log(N):.3f}\")\n\nplt.figure(figsize=(8, 4))\nplt.plot(losses, color='#2c3e50')\nplt.axhline(y=0, color='green', linestyle='--', alpha=0.5, label='Perfect alignment')\nplt.axhline(y=float(jnp.log(N)), color='red', linestyle='--', alpha=0.5, label='Random (log N)')\nplt.xlabel(\"Step\"); plt.ylabel(\"InfoNCE Loss\")\nplt.title(\"Learning a Joint Embedding Space\")\nplt.legend(); plt.grid(alpha=0.3); plt.tight_layout(); plt.show()\n# Modify d_embed (try 2, 4, 16) to see how embedding dimension affects alignment\n</code></pre></p> </li> <li> <p>Implement zero-shot classification with pre-computed embeddings. Simulate class \"prototypes\" as text embeddings and classify new images by nearest-neighbour lookup. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Simulate 5 classes, each with a prototype text embedding in R^32\nkey = jax.random.PRNGKey(42)\nn_classes, d = 5, 32\nclass_names = [\"cat\", \"dog\", \"car\", \"plane\", \"ship\"]\n\n# Class prototypes (imagine these came from a text encoder)\nk1, k2 = jax.random.split(key)\nclass_prototypes = jax.random.normal(k1, (n_classes, d))\nclass_prototypes = class_prototypes / jnp.linalg.norm(class_prototypes, axis=1, keepdims=True)\n\n# Generate 200 test \"images\" (embeddings near their class prototype + noise)\nn_per_class = 40\ntrue_labels = jnp.repeat(jnp.arange(n_classes), n_per_class)\nkeys = jax.random.split(k2, n_classes * n_per_class)\n\nimage_embeds = []\nfor i in range(n_classes):\n    noise = jax.random.normal(keys[i], (n_per_class, d)) * 0.5\n    cluster = class_prototypes[i] + noise\n    image_embeds.append(cluster)\nimage_embeds = jnp.concatenate(image_embeds, axis=0)\nimage_embeds = image_embeds / jnp.linalg.norm(image_embeds, axis=1, keepdims=True)\n\n# Zero-shot classification: cosine similarity with each prototype\nsimilarities = image_embeds @ class_prototypes.T  # (200, 5)\npredicted_labels = jnp.argmax(similarities, axis=1)\naccuracy = jnp.mean(predicted_labels == true_labels)\nprint(f\"Zero-shot accuracy: {accuracy:.1%}\")\n\n# Confusion matrix\nconf = jnp.zeros((n_classes, n_classes), dtype=jnp.int32)\nfor true, pred in zip(true_labels, predicted_labels):\n    conf = conf.at[true, pred].add(1)\n\nfig, ax = plt.subplots(figsize=(6, 5))\nim = ax.imshow(conf, cmap='Blues')\nax.set_xticks(range(n_classes)); ax.set_xticklabels(class_names, rotation=45)\nax.set_yticks(range(n_classes)); ax.set_yticklabels(class_names)\nax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"True\")\nfor i in range(n_classes):\n    for j in range(n_classes):\n        ax.text(j, i, int(conf[i, j]), ha='center', va='center', fontsize=11)\nax.set_title(f\"Zero-Shot Confusion Matrix (acc={accuracy:.1%})\")\nplt.colorbar(im); plt.tight_layout(); plt.show()\n# Try increasing noise (0.5 -&gt; 1.0 -&gt; 2.0) to see accuracy degrade\n# Try adding prompt ensembling: average 3 noisy copies of each prototype\n</code></pre></p> </li> </ol>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/","title":"Vision Language Models","text":"<p>Vision language models jointly understand images and text, enabling visual question answering, image captioning, and visual reasoning. This file covers VQA, image captioning, visual grounding, and architectures like VisualBERT, BLIP, LLaVA, Flamingo, PaLI, and Qwen-VL that fuse vision encoders with large language models.</p> <ul> <li> <p>Think of a museum guide who can look at a painting and articulate everything about it: what objects are present, what story it tells, what emotions it conveys, and answer any question a visitor might pose. A vision language model (VLM) is the computational equivalent \u2014 a system that jointly understands images and text, enabling it to describe visual scenes, answer questions about them, follow visual instructions, and even locate specific objects within an image given a natural language query.</p> </li> <li> <p>VLMs sit at the intersection of the vision encoders you met in Chapter 8 and the language models from Chapter 7. The central engineering challenge is bridging two very different representational worlds: the spatial, continuous feature maps of a vision backbone and the sequential, discrete token embeddings of a language model. Every architecture in this file is, at its core, a different answer to the question: how do you fuse vision and language?</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#visual-question-answering","title":"Visual Question Answering","text":"<ul> <li> <p>Imagine someone shows you a photograph and asks \"How many dogs are in the park?\" You effortlessly parse the image, locate the dogs, count them, and produce an answer. Visual question answering (VQA) formalises this: given an image \\(I\\) and a natural language question \\(q\\), predict the answer \\(a\\).</p> </li> <li> <p>The task can be framed in several ways. The most common treats VQA as open-ended classification: the model selects from a fixed vocabulary of the most frequent answers (e.g., the top 3,129 answers in VQA v2). Alternatively, it can be treated as generative answering, where the model produces a free-form text string \u2014 this is the approach modern VLMs use.</p> </li> <li> <p>Formally, you want to learn a function \\(f(I, q) \\to a\\) that maximises the likelihood of the correct answer. In the classification setup, this becomes:</p> </li> </ul> \\[p(a \\mid I, q) = \\text{softmax}(W \\cdot g(v, h))\\] <ul> <li> <p>where \\(v\\) is a visual feature vector (from a CNN or ViT), \\(h\\) is a question encoding (from an LSTM or Transformer), and \\(g\\) is a fusion function that combines them. The design of \\(g\\) is where the real architectural creativity lies.</p> </li> <li> <p>VQA v1 (Antol et al., 2015) introduced the benchmark with 614,000 questions on 204,000 images from MS COCO. Researchers quickly discovered that models could achieve surprisingly high accuracy by exploiting language priors \u2014 answering \"2\" for \"how many\" questions or \"yes\" for \"is there\" questions without even looking at the image.</p> </li> <li> <p>VQA v2 (Goyal et al., 2017) addressed this by pairing each question with two similar images that yield different answers. This forced models to actually ground their reasoning in visual content. The balanced pair setup roughly doubles the dataset and makes language-only shortcuts much less effective.</p> </li> <li> <p>Other important VQA datasets include GQA (Hudson &amp; Manning, 2019) with compositional questions requiring multi-step reasoning, OK-VQA (Marino et al., 2019) requiring outside knowledge beyond the image, and TextVQA (Singh et al., 2019) where the answer depends on reading text within the image.</p> </li> </ul> <p></p> <ul> <li> <p>Early VQA models used a simple strategy: extract image features from a pre-trained CNN (typically the penultimate layer of ResNet or VGGNet from Chapter 8), encode the question with an LSTM (Chapter 6), and combine them. The combination function \\(g\\) evolved rapidly: from simple element-wise multiplication, to bilinear pooling, to multi-modal Tucker decomposition. Bilinear attention computes \\(v^T W h\\), where \\(W\\) is a learnable interaction matrix, but the full bilinear form has \\(O(d_v \\times d_h)\\) parameters, which is prohibitively large. MLB (multimodal low-rank bilinear pooling) factorises this into two low-rank projections, making it tractable.</p> </li> <li> <p>The breakthrough for VQA was attention. Stacked Attention Networks (Yang et al., 2016) used the question encoding to attend over spatial image regions, iteratively refining which parts of the image to focus on. This idea \u2014 letting the question \"look at\" relevant image regions \u2014 became standard.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#image-captioning","title":"Image Captioning","text":"<ul> <li> <p>Picture a friend looking at your holiday photos and narrating what they see: \"A golden retriever is catching a frisbee on a sunny beach.\" Image captioning is the task of generating a natural language description of an image. Unlike VQA, there is no question \u2014 the model must decide what is worth describing on its own.</p> </li> <li> <p>Show and Tell (Vinyals et al., 2015) established the canonical encoder-decoder architecture for captioning. A CNN encoder (e.g., Inception or ResNet) produces a single image feature vector \\(v\\). This vector is used as the initial hidden state of an LSTM decoder, which then generates a caption word by word, autoregressively:</p> </li> </ul> \\[p(w_t \\mid w_{1:t-1}, I) = \\text{LSTM}(w_{t-1}, h_{t-1})\\] <ul> <li> <p>The entire model is trained end-to-end by maximising the log-likelihood of ground-truth captions. At inference time, beam search (Chapter 7) is used to find high-probability captions.</p> </li> <li> <p>The problem with Show and Tell is that the entire image is compressed into a single vector. For complex scenes, a single vector cannot capture all the relevant details. You lose spatial information \u2014 the model cannot \"look back\" at specific parts of the image while generating different words.</p> </li> <li> <p>Show, Attend and Tell (Xu et al., 2015) solved this by introducing attention over image regions. Instead of encoding the image as one vector, the CNN produces a spatial feature grid (e.g., \\(14 \\times 14 \\times 512\\) from the last convolutional layer of VGGNet). At each decoding step, the model computes attention weights over these spatial locations, producing a context vector that highlights the most relevant region for the current word.</p> </li> <li> <p>Recall the attention mechanism from Chapter 6: the decoder hidden state acts as the query, the spatial features act as keys and values, and the attention weights tell the model where to look. The authors proposed two variants: soft attention (differentiable, weighted average of all regions) and hard attention (stochastic sampling of a single region, trained with REINFORCE).</p> </li> </ul> <p></p> <ul> <li> <p>The attention maps produced by these models are remarkably interpretable: when generating \"dog,\" the attention peaks over the dog region; when generating \"beach,\" it shifts to the sand and water. This was one of the first compelling demonstrations that attention provides built-in interpretability.</p> </li> <li> <p>CIDEr (Vedantam et al., 2015), METEOR, BLEU, and SPICE are the standard captioning evaluation metrics. CIDEr computes TF-IDF weighted n-gram similarity between generated and reference captions, specifically designed for captioning evaluation. Modern VLMs are typically evaluated on CIDEr for captioning benchmarks like MS COCO Captions and NoCaps.</p> </li> <li> <p>Later captioning models incorporated bottom-up attention (Anderson et al., 2018), where an object detector (Faster R-CNN, Chapter 8) first proposes salient image regions, and the captioning model attends over these region features rather than a uniform grid. This was the dominant approach before ViT-based encoders took over.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#architecture-patterns","title":"Architecture Patterns","text":"<ul> <li>Every VLM must answer a fundamental design question: at what point do vision and language interact? The answer defines the model's architecture family. There are three primary patterns, each with distinct trade-offs.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#dual-encoder","title":"Dual Encoder","text":"<ul> <li> <p>Imagine two translators working independently \u2014 one reads a French document, the other reads an English document \u2014 and they each produce a summary in a shared \"universal language.\" They never communicate during translation, but their summaries are directly comparable. This is the dual encoder pattern.</p> </li> <li> <p>A vision encoder \\(f_v\\) and a text encoder \\(f_t\\) independently map their respective inputs to a shared embedding space of dimension \\(d\\). The image embedding is \\(v = f_v(I) \\in \\mathbb{R}^d\\) and the text embedding is \\(t = f_t(q) \\in \\mathbb{R}^d\\). Similarity is computed via a dot product or cosine similarity: \\(\\text{sim}(I, q) = v^T t / (\\|v\\| \\|t\\|)\\).</p> </li> <li> <p>CLIP (Radford et al., 2021), covered in the previous file on multimodal representations, is the prototypical dual encoder. It is trained with a contrastive objective (InfoNCE) on 400 million image-text pairs scraped from the internet. Because the encoders are independent, you can pre-compute and cache all image embeddings, making retrieval extremely efficient \u2014 you only need to encode the query text at search time.</p> </li> <li> <p>The dual encoder's weakness is that vision and language never interact at the feature level. The model cannot perform fine-grained cross-modal reasoning: it cannot, for example, determine whether a specific word in the caption corresponds to a specific region in the image. This limits its usefulness for tasks like VQA or grounded captioning.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#fusion-encoder","title":"Fusion Encoder","text":"<ul> <li> <p>Now imagine the two translators are in the same room, actively discussing both documents. They can point at specific passages, ask each other questions, and build a joint understanding. This is the fusion encoder pattern.</p> </li> <li> <p>Both modalities are encoded and then fused through cross-attention layers where tokens from one modality attend to tokens from the other. The image is first processed by a vision encoder into a sequence of patch or region tokens \\(V = [v_1, \\ldots, v_N]\\). The text is tokenised into \\(T = [t_1, \\ldots, t_M]\\). In the fusion layers, text tokens attend to image tokens via cross-attention:</p> </li> </ul> \\[\\text{CrossAttn}(T, V) = \\text{softmax}\\!\\left(\\frac{(TW_Q)(VW_K)^T}{\\sqrt{d_k}}\\right)(VW_V)\\] <ul> <li>This enables fine-grained interaction: each text token can attend to the specific image regions it needs. Models like VisualBERT, VilBERT, and UNITER use this pattern. The cost is that you cannot pre-compute separate embeddings for retrieval \u2014 every image-text pair requires a full forward pass through the fusion layers.</li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#encoder-decoder","title":"Encoder-Decoder","text":"<ul> <li> <p>The encoder-decoder pattern combines the vision encoder with a text decoder that generates output tokens autoregressively, similar to the seq2seq models from Chapter 7. The vision encoder produces contextual image representations, and the text decoder cross-attends to them while generating output text.</p> </li> <li> <p>This pattern naturally supports generative tasks: captioning, VQA with free-form answers, and visual dialogue. Models like GIT (Generative Image-to-text Transformer, Wang et al., 2022), CoCa (Contrastive Captioner, Yu et al., 2022), and PaLI use this architecture. CoCa cleverly combines the dual encoder and encoder-decoder patterns: the first half of the text decoder layers operate as a unimodal text encoder (for contrastive learning), while the second half cross-attend to image features (for generative captioning), getting the best of both worlds.</p> </li> <li> <p>The choice among these three patterns depends on the target task. Dual encoders are optimal for retrieval at scale. Fusion encoders are best for fine-grained understanding tasks. Encoder-decoders are most versatile for generative tasks. Modern state-of-the-art VLMs increasingly adopt the encoder-decoder or decoder-only paradigm, treating every vision-language task as text generation.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#flamingo-few-shot-multimodal-learning","title":"Flamingo: Few-Shot Multimodal Learning","text":"<ul> <li> <p>Think of a seasoned expert who, after years of studying both art and literature, can look at a completely new painting style and describe it eloquently after seeing just one or two examples. Flamingo (Alonso et al., 2022, DeepMind) is built on the same principle: it leverages a powerful pre-trained language model and a pre-trained vision encoder, connecting them with lightweight architectural components that enable few-shot learning on multimodal tasks.</p> </li> <li> <p>Flamingo's design philosophy is conservative and effective: keep the pre-trained vision encoder (NFNet) and language model (Chinchilla) frozen, and learn only the \"glue\" that connects them. This glue consists of two components: a Perceiver Resampler and gated cross-attention layers.</p> </li> <li> <p>The Perceiver Resampler takes the variable-length output of the vision encoder (which depends on image resolution) and compresses it into a fixed set of \\(N\\) visual tokens (typically \\(N = 64\\)). It works by initialising a set of \\(N\\) learnable query vectors and using cross-attention to let these queries attend to the full set of vision encoder outputs. This is essentially the Perceiver architecture (Jaegle et al., 2021) applied as a bottleneck \u2014 it produces a compact, fixed-size visual representation regardless of the input image size.</p> </li> </ul> \\[z = \\text{CrossAttn}(Q_{\\text{learned}}, V_{\\text{image}}) \\in \\mathbb{R}^{N \\times d}\\] <ul> <li>The gated cross-attention layers are interleaved between the frozen language model layers. At each such layer, the language model's text tokens cross-attend to the visual tokens produced by the Perceiver Resampler. Critically, each gated cross-attention layer includes a learnable scalar gate \\(\\alpha\\), initialised to zero, that multiplies the cross-attention output before adding it to the residual stream:</li> </ul> \\[\\hat{x} = x + \\alpha \\cdot \\text{CrossAttn}(x, z)\\] <ul> <li>Initialising \\(\\alpha = 0\\) means that at the start of training, the cross-attention contributes nothing, and the model behaves exactly like the original frozen language model. The gates gradually open during training, smoothly integrating visual information without disrupting the language model's pre-trained representations.</li> </ul> <p></p> <ul> <li> <p>Flamingo natively handles interleaved image-text sequences. You can feed it a prompt containing multiple images interspersed with text, such as: \"[Image 1] This is a cat. [Image 2] This is a dog. [Image 3] This is a ___.\" The model processes each image through the vision encoder and Perceiver Resampler, and the resulting visual tokens are inserted at the corresponding positions in the text sequence. The language model's causal attention mask ensures that each text token can only attend to visual tokens from the current and preceding images.</p> </li> <li> <p>This interleaving enables powerful few-shot multimodal learning. By providing a few image-text examples in context, Flamingo can perform new tasks without any gradient updates. On benchmarks like VQAv2, OK-VQA, and captioning, Flamingo with 80B parameters achieved state-of-the-art few-shot performance, often matching or exceeding fine-tuned specialist models with just 4 or 32 examples.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#llava-and-visual-instruction-tuning","title":"LLaVA and Visual Instruction Tuning","text":"<ul> <li> <p>Imagine you have a brilliant language expert (an LLM) and a brilliant art critic (a vision encoder). If you could teach the art critic to \"speak the language expert's language,\" they could collaborate seamlessly. LLaVA (Large Language and Vision Assistant, Liu et al., 2023) does exactly this: it projects vision features into the LLM's token embedding space using a simple linear layer, then fine-tunes the whole system on instruction-following data.</p> </li> <li> <p>LLaVA's architecture is strikingly simple. An image is encoded by a pre-trained CLIP ViT-L/14 vision encoder into a grid of patch features \\(V \\in \\mathbb{R}^{N \\times d_v}\\), where \\(N = 256\\) patches (for 336px images with 14px patches). A projection layer \\(W\\) maps these vision features into the LLM's embedding dimension:</p> </li> </ul> \\[H_v = VW, \\quad W \\in \\mathbb{R}^{d_v \\times d_{\\text{LLM}}}\\] <ul> <li>The projected visual tokens \\(H_v\\) are simply concatenated with the text token embeddings and fed into the LLM (Vicuna, a fine-tuned LLaMA) as a single sequence. The LLM processes them with its standard causal self-attention \u2014 no special cross-attention layers, no perceiver, just concatenation. The visual tokens are treated as if they were text tokens that happen to encode visual information.</li> </ul> <p></p> <ul> <li> <p>Visual instruction tuning is LLaVA's key training innovation. The authors used GPT-4 to generate 158,000 multimodal instruction-following examples from COCO images. Each example consists of an image paired with a conversational instruction (e.g., \"Describe this image in detail,\" \"What is unusual about this image?,\" \"If I were a tourist visiting this place, what should I know?\"). The model is trained to generate the GPT-4-authored response given the image and instruction.</p> </li> <li> <p>Training proceeds in two stages. Stage 1 (pre-training): only the projection layer \\(W\\) is trained on image-caption pairs (595K from CC3M), while both the vision encoder and LLM are frozen. This teaches \\(W\\) to align visual features with the LLM's embedding space. Stage 2 (fine-tuning): the projection layer and the LLM are jointly fine-tuned on the instruction-following data, while the vision encoder stays frozen. This teaches the model to follow complex visual instructions.</p> </li> <li> <p>LLaVA-1.5 improved the original with three key changes: replacing the single linear projection with a two-layer MLP (more expressive mapping), using higher-resolution images (336px instead of 224px, producing more patch tokens), and adding academic VQA datasets to the training mix. These seemingly minor modifications produced a large jump in benchmark performance.</p> </li> <li> <p>The LLaVA approach demonstrated that you do not need complex architectural innovations like Flamingo's Perceiver Resampler or gated cross-attention. A simple linear projection, combined with high-quality instruction-tuning data, is enough to connect a vision encoder to an LLM effectively. This simplicity made LLaVA extremely influential \u2014 most subsequent open-source VLMs follow a similar recipe.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#scaling-vision-language-models","title":"Scaling Vision-Language Models","text":"<ul> <li>The field moved rapidly from proof-of-concept VLMs to industrial-scale systems trained on billions of image-text pairs. Three model families illustrate different approaches to scaling.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#pali","title":"PaLI","text":"<ul> <li> <p>PaLI (Pathways Language and Image model, Chen et al., 2022, Google) scales both the vision encoder and the language model simultaneously. PaLI uses a ViT-e (4B parameters) as the vision encoder and mT5 (13B parameters) as the language model, for a total of 17B parameters. The image is encoded into a sequence of patch tokens, which are prepended to the text tokens and fed into the encoder-decoder mT5.</p> </li> <li> <p>PaLI's key insight is that scaling the vision encoder matters as much as scaling the language model. Previous work typically used a fixed, moderate-sized vision backbone (e.g., ViT-B or ViT-L) and poured all the parameter budget into the LLM. PaLI showed that a 4B-parameter ViT-e, pre-trained on JFT-4B (4 billion labelled images), dramatically improves performance on fine-grained visual tasks like OCR and spatial reasoning.</p> </li> <li> <p>PaLI is trained on WebLI, a dataset of 10 billion image-text pairs in 109 languages, making it inherently multilingual. The model is pre-trained with a mixture of tasks: image captioning, VQA, and image-text matching, all cast as text-to-text generation (following the T5 paradigm from Chapter 7). PaLI-X (55B parameters) and PaLI-3 (5B, using SigLIP as the vision encoder) are subsequent iterations.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#qwen-vl","title":"Qwen-VL","text":"<ul> <li> <p>Qwen-VL (Bai et al., 2023, Alibaba) builds on the Qwen LLM by adding a ViT vision encoder and a single-layer cross-attention module (similar to Flamingo's Perceiver Resampler) that compresses the vision encoder's output into a fixed set of 256 visual tokens. The visual tokens are concatenated with text tokens and processed by the Qwen LLM.</p> </li> <li> <p>Qwen-VL's training uses a three-stage recipe. Stage 1: pre-train on 1.4 billion weakly-supervised image-text pairs with only the vision encoder unfrozen. Stage 2: multi-task pre-training on higher-quality data including VQA, captioning, grounding, and OCR datasets, with the full model unfrozen. Stage 3: supervised fine-tuning on instruction-following and dialogue data. This progressive refinement, from noisy web data to curated instruction data, is a pattern shared across most modern VLMs.</p> </li> <li> <p>Qwen2-VL (2024) introduced dynamic resolution support: instead of resizing all images to a fixed size, it processes images at their native resolution by dynamically adjusting the number of visual tokens. Higher-resolution images produce more tokens, and lower-resolution images produce fewer. This improves performance on detail-sensitive tasks like document understanding and fine-grained recognition without wasting computation on low-resolution inputs.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#internvl","title":"InternVL","text":"<ul> <li> <p>InternVL (Chen et al., 2024, Shanghai AI Lab) scales the vision encoder aggressively, using InternViT-6B \u2014 a 6-billion-parameter vision transformer \u2014 paired with a language model. The key architectural contribution is dynamic high-resolution processing: images are divided into tiles of 448x448 pixels, each processed independently by the vision encoder, and the resulting tile features are concatenated with a thumbnail feature of the full image. This allows the model to handle images of arbitrary aspect ratios and resolutions.</p> </li> <li> <p>InternVL-2 further introduced progressive alignment training: first aligning the vision encoder with a contrastive objective (like CLIP), then connecting it to the LLM through a lightweight MLP connector, and finally fine-tuning end-to-end on instruction data. The progressive strategy prevents catastrophic forgetting of the vision encoder's pre-trained representations.</p> </li> </ul> <p></p> <ul> <li>A common theme across all three families is the importance of training data curation. Raw web-scraped image-text pairs are noisy and often misaligned. Successive training stages progressively filter and refine the data, moving from billions of noisy pairs to millions of high-quality instruction examples. The quality of the final fine-tuning data often matters more than the model's raw parameter count.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#grounding-and-referring","title":"Grounding and Referring","text":"<ul> <li> <p>Imagine pointing at a person in a crowd and saying \"the woman in the red hat.\" You are using language to refer to a specific spatial region. Visual grounding is the reverse: given an image and a natural language expression, the model must identify (localise) the referred object. Referring expression comprehension produces a bounding box; referring expression segmentation produces a pixel mask.</p> </li> <li> <p>Formally, given an image \\(I\\) and a referring expression \\(r\\) (e.g., \"the large brown dog on the left\"), the model predicts a bounding box \\(b = (x, y, w, h)\\) or a set of coordinates that localise the referent. The datasets include RefCOCO, RefCOCO+, and RefCOCOg, each containing images with multiple objects and unambiguous referring expressions for each.</p> </li> <li> <p>Early grounding models used a two-stage approach: first generate region proposals (from Faster R-CNN or similar), then score each proposal against the language query using a fusion model. The highest-scoring region is the prediction. This is computationally expensive and limited by the quality of the proposals.</p> </li> <li> <p>Modern VLMs integrate grounding directly into the generative framework. The key idea is to represent bounding box coordinates as text tokens. You discretise the continuous coordinate space into bins (e.g., 1000 bins for each of \\(x, y, w, h\\)) and add special location tokens like <code>&lt;loc_342&gt;</code> to the vocabulary. The model then generates a bounding box by outputting a sequence of location tokens:</p> </li> </ul> \\[\\text{Output: } \\texttt{&lt;loc\\_102&gt;&lt;loc\\_215&gt;&lt;loc\\_487&gt;&lt;loc\\_398&gt;}\\] <ul> <li> <p>This tokenisation trick allows any autoregressive language model to perform grounding without any architectural changes \u2014 it simply learns to \"speak coordinates.\" Pix2Seq (Chen et al., 2022) pioneered this approach for object detection, and models like Qwen-VL, Ferret, and Kosmos-2 extend it to referring expression comprehension and phrase grounding.</p> </li> <li> <p>Kosmos-2 (Peng et al., 2023, Microsoft) adds grounding capability to a multimodal LLM by representing spatial locations as special tokens embedded within the generated text. For example, it can generate: \"A <code>&lt;phrase&gt;</code> golden retriever <code>&lt;/phrase&gt;</code> <code>&lt;box&gt;</code> <code>&lt;loc_102&gt;</code> <code>&lt;loc_215&gt;</code> <code>&lt;loc_487&gt;</code> <code>&lt;loc_398&gt;</code> <code>&lt;/box&gt;</code> is catching a frisbee.\" This interleaving of text and spatial tokens enables simultaneous captioning and grounding.</p> </li> </ul> <p></p> <ul> <li>Pointing takes grounding further: instead of bounding boxes, the model predicts a single point (typically the centre of the referred object). This is useful for interactive applications where a user asks \"Where is the nearest exit?\" and the model responds with a coordinate overlaid on the image. Models like Shikra and Ferret support point-based referring in addition to box-based grounding.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#ocr-free-document-understanding","title":"OCR-Free Document Understanding","text":"<ul> <li> <p>Traditional document understanding pipelines are complex: first run an OCR engine to extract text and layout, then feed the extracted text into a language model. This multi-stage approach is fragile \u2014 OCR errors propagate downstream, and the spatial layout information is often lost or poorly represented. What if the model could read directly from pixels, the way you do?</p> </li> <li> <p>Donut (Document Understanding Transformer, Kim et al., 2022) eliminates OCR entirely. It uses a Swin Transformer (Chapter 8) as the vision encoder to process the document image, and a BART-style Transformer decoder to generate structured text output directly from the visual features. The decoder can produce JSON, key-value pairs, or plain text, depending on the task.</p> </li> <li> <p>Donut's training is two-stage. Pre-training: the model learns to read by performing synthetic OCR \u2014 given a document image, it generates the full text content. This is trained on millions of synthetic document images rendered from text corpora, teaching the vision encoder to recognise characters, fonts, and layouts. Fine-tuning: the model is adapted to specific downstream tasks like receipt parsing, form understanding, or document classification, by training it to generate task-specific structured output.</p> </li> <li> <p>The Donut decoder uses a special prompting scheme: the task is specified by a prompt token (e.g., <code>&lt;doc_class&gt;</code> for classification or <code>&lt;parse_receipt&gt;</code> for receipt parsing), and the model generates the output conditioned on this prompt. This unified interface allows a single model to handle multiple document understanding tasks.</p> </li> <li> <p>Pix2Struct (Lee et al., 2023, Google) takes the OCR-free idea and applies it to web page understanding and chart/figure comprehension. The key pre-training objective is screenshot parsing: given a masked screenshot of a web page, the model generates the underlying HTML that produced the visible region. This teaches the model to understand the relationship between visual rendering and structured markup.</p> </li> <li> <p>Pix2Struct introduces variable-resolution input processing: instead of resizing all images to a fixed size (which distorts aspect ratios and destroys fine text), it packs the image into a fixed number of patches while preserving the original aspect ratio. A tall, narrow document produces a tall, narrow patch grid. This is critical for document understanding, where aspect ratio carries semantic information (a receipt is narrow and tall; a spreadsheet is wide and short).</p> </li> </ul> <p></p> <ul> <li> <p>Nougat (Blecher et al., 2023, Meta) applies the Donut architecture specifically to academic papers, generating full LaTeX markup directly from PDF page images. It can handle complex mathematical equations, tables, and figures \u2014 tasks where traditional OCR pipelines struggle badly. The model is trained on pairs of PDF page images and their corresponding LaTeX source code.</p> </li> <li> <p>The success of OCR-free models demonstrates a broader principle in deep learning: end-to-end models that learn directly from raw inputs (pixels) often outperform complex multi-stage pipelines, because they can jointly optimise all components and learn representations that are specifically tailored to the final task. The intermediate OCR step is a bottleneck that constrains what the model can learn.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#the-visual-token-pipeline","title":"The Visual Token Pipeline","text":"<ul> <li> <p>Regardless of architecture family, every VLM must convert an image into a sequence of tokens that a language model can process. Understanding this pipeline is essential. The process varies by model, but the general flow is:</p> </li> <li> <p>Step 1: Patch extraction. The image (height \\(H\\), width \\(W\\)) is divided into non-overlapping patches of size \\(P \\times P\\), producing \\(N = HW / P^2\\) patches. For a 336x336 image with 14x14 patches, \\(N = 576\\).</p> </li> <li> <p>Step 2: Vision encoding. Each patch is linearly projected and passed through the vision encoder (typically a ViT). The output is a sequence of contextual patch embeddings \\(V = [v_1, \\ldots, v_N] \\in \\mathbb{R}^{N \\times d_v}\\). These embeddings carry both local appearance information and global context (from self-attention).</p> </li> <li> <p>Step 3: Token compression (optional). Some models compress the \\(N\\) visual tokens into a smaller set of \\(M \\ll N\\) tokens to reduce the computational burden on the language model. Flamingo uses a Perceiver Resampler (\\(M = 64\\)); Qwen-VL uses cross-attention (\\(M = 256\\)); Q-Former (used in BLIP-2, Li et al., 2023) uses a set of \\(M = 32\\) learnable query tokens that cross-attend to the vision encoder's output.</p> </li> <li> <p>Step 4: Projection. The visual tokens (either the full set or the compressed set) are projected into the language model's embedding space via a linear layer or MLP. After projection, visual tokens have the same dimensionality as text token embeddings and can be concatenated with them.</p> </li> <li> <p>Step 5: Injection into the LLM. The projected visual tokens are inserted into the token sequence at the position of a special <code>&lt;image&gt;</code> placeholder token, and the combined sequence is processed by the language model. The LLM's self-attention allows text tokens to attend to visual tokens and vice versa.</p> </li> </ul> <p></p> <ul> <li> <p>The number of visual tokens directly affects computational cost. Each visual token participates in the LLM's self-attention, which is quadratic in sequence length. A high-resolution image with many patches can produce hundreds or thousands of visual tokens, dominating the LLM's context window. This is why token compression is important: reducing 576 visual tokens to 64 cuts the visual contribution to attention by roughly 9x.</p> </li> <li> <p>BLIP-2 (Li et al., 2023) is notable for its efficient bridging strategy. It introduces a lightweight Q-Former (a small Transformer with learnable queries) that sits between the frozen vision encoder and the frozen LLM. The Q-Former is the only trainable component \u2014 both the vision encoder and LLM remain frozen. It is pre-trained in two stages: first with image-text contrastive learning, matching, and captioning objectives (connecting it to the vision encoder), then with language generation objectives (connecting it to the LLM). This modular design allows BLIP-2 to plug any vision encoder into any LLM.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#training-objectives","title":"Training Objectives","text":"<ul> <li> <p>VLMs are trained with a combination of objectives, depending on the architecture pattern:</p> </li> <li> <p>Image-text contrastive loss (ITC): aligns image and text representations in a shared embedding space, as in CLIP. This is the primary objective for dual encoders and is often used as a pre-training objective for fusion models. The loss is the InfoNCE loss from the previous file.</p> </li> <li> <p>Image-text matching (ITM): a binary classification objective \u2014 given an image and a text, predict whether they match. Hard negatives (text that is similar but paired with a different image) make this task challenging and force the model to learn fine-grained alignment.</p> </li> <li> <p>Language modelling (LM): the standard autoregressive language modelling objective \u2014 predict the next token given all previous tokens. For VLMs, the \"previous tokens\" include the visual tokens, so the model learns to generate text conditioned on visual input. This is the primary objective for encoder-decoder and decoder-only VLMs.</p> </li> </ul> \\[\\mathcal{L}_{\\text{LM}} = -\\sum_{t=1}^{T} \\log p(w_t \\mid w_{&lt;t}, V)\\] <ul> <li> <p>Prefix language modelling: a variant where the image and a text prefix are provided as context (not trained on), and the model is trained to generate only the continuation. This is used in models like PaLI and SimVLM.</p> </li> <li> <p>Most modern VLMs combine multiple objectives during pre-training (e.g., ITC + ITM + LM in BLIP, ITC + LM in CoCa) and then fine-tune with a pure LM objective on instruction data.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/02.%20vision%20language%20models/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement a simple attention-based image captioning decoder. Use random \"image features\" as the encoder output and train the decoder to generate a fixed caption, observing how the attention weights shift across spatial positions at each decoding step. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Simulate a 4x4 spatial grid of image features (16 regions, dim=32)\nkey = jax.random.PRNGKey(42)\nk1, k2, k3 = jax.random.split(key, 3)\nimg_features = jax.random.normal(k1, (16, 32))  # 16 spatial regions, 32-dim\n\n# Vocabulary: 0=&lt;start&gt;, 1=\"a\", 2=\"red\", 3=\"car\", 4=&lt;end&gt;\nvocab_size, embed_dim, hidden_dim = 5, 16, 32\nW_embed = jax.random.normal(k2, (vocab_size, embed_dim)) * 0.1\nW_attn_q = jax.random.normal(k3, (hidden_dim, 32)) * 0.1  # query projection\n\ndef attend(h, img_feats, W_q):\n    \"\"\"Compute soft attention over image features given decoder state h.\"\"\"\n    query = h @ W_q  # (32,)\n    scores = img_feats @ query  # (16,)\n    weights = jax.nn.softmax(scores)  # (16,)\n    context = weights @ img_feats  # (32,)\n    return context, weights\n\n# Simple GRU-like step (for illustration, just a linear + tanh)\nW_h = jax.random.normal(jax.random.PRNGKey(0), (embed_dim + 32, hidden_dim)) * 0.1\n\ndef decode_step(h, word_idx, img_feats):\n    context, attn_weights = attend(h, img_feats, W_attn_q)\n    word_emb = W_embed[word_idx]  # (16,)\n    inp = jnp.concatenate([word_emb, context])  # (48,)\n    h_new = jnp.tanh(inp @ W_h)  # (32,)\n    return h_new, attn_weights\n\n# Run decoding for the sequence: &lt;start&gt; -&gt; \"a\" -&gt; \"red\" -&gt; \"car\" -&gt; &lt;end&gt;\ntarget_seq = [0, 1, 2, 3, 4]\nh = jnp.zeros(hidden_dim)\nall_attn = []\nfor word_idx in target_seq[:-1]:\n    h, attn_w = decode_step(h, word_idx, img_features)\n    all_attn.append(attn_w)\n\n# Visualise attention maps (reshaped to 4x4 grid) at each step\nwords = [\"&lt;start&gt;\", \"a\", \"red\", \"car\"]\nfig, axes = plt.subplots(1, 4, figsize=(14, 3))\nfor i, (ax, w) in enumerate(zip(axes, words)):\n    ax.imshow(all_attn[i].reshape(4, 4), cmap='viridis')\n    ax.set_title(f'Attending when\\ngenerating after \"{w}\"')\n    ax.axis('off')\nplt.suptitle('Attention Over Image Regions at Each Decoding Step')\nplt.tight_layout(); plt.show()\n# Try changing img_features to see how attention patterns shift!\n</code></pre></p> </li> <li> <p>Simulate the visual token pipeline: patchify an image, project patches to an embedding space, concatenate with text token embeddings, and run a single self-attention layer over the combined sequence. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nkey = jax.random.PRNGKey(7)\n\n# Create a synthetic 8x8 \"image\" with 3 channels\nk1, k2, k3, k4 = jax.random.split(key, 4)\nimage = jax.random.uniform(k1, (8, 8, 3))\n\n# Step 1: Patchify into 4x4 patches -&gt; 4 patches\npatch_size = 4\npatches = image.reshape(2, patch_size, 2, patch_size, 3)\npatches = patches.transpose(0, 2, 1, 3, 4).reshape(4, patch_size * patch_size * 3)  # (4, 48)\nprint(f\"Number of patches: {patches.shape[0]}, patch dim: {patches.shape[1]}\")\n\n# Step 2: Project patches to embedding dim (d=16)\nd_model = 16\nW_patch = jax.random.normal(k2, (patches.shape[1], d_model)) * 0.1\nvisual_tokens = patches @ W_patch  # (4, 16)\n\n# Step 3: Create text token embeddings (simulate 3 text tokens)\ntext_tokens = jax.random.normal(k3, (3, d_model)) * 0.1\n\n# Step 4: Concatenate visual + text tokens\ncombined = jnp.concatenate([visual_tokens, text_tokens], axis=0)  # (7, 16)\nprint(f\"Combined sequence length: {combined.shape[0]} (4 visual + 3 text)\")\n\n# Step 5: Single-head self-attention over the combined sequence\nW_Q = jax.random.normal(k4, (d_model, d_model)) * 0.1\nk5, k6 = jax.random.split(k4)\nW_K = jax.random.normal(k5, (d_model, d_model)) * 0.1\nW_V = jax.random.normal(k6, (d_model, d_model)) * 0.1\n\nQ = combined @ W_Q\nK = combined @ W_K\nV = combined @ W_V\nattn_scores = (Q @ K.T) / jnp.sqrt(d_model)\nattn_weights = jax.nn.softmax(attn_scores, axis=-1)  # (7, 7)\n\noutput = attn_weights @ V  # (7, 16)\n\n# Visualise the cross-modal attention pattern\nlabels = ['V1', 'V2', 'V3', 'V4', 'T1', 'T2', 'T3']\nfig, ax = plt.subplots(figsize=(6, 5))\nim = ax.imshow(attn_weights, cmap='Blues')\nax.set_xticks(range(7)); ax.set_xticklabels(labels)\nax.set_yticks(range(7)); ax.set_yticklabels(labels)\nax.set_xlabel('Key'); ax.set_ylabel('Query')\nax.set_title('Self-Attention: Visual (V) and Text (T) Tokens')\nplt.colorbar(im, ax=ax); plt.tight_layout(); plt.show()\n# Observe: text tokens attend to visual tokens (cross-modal attention)!\n</code></pre></p> </li> <li> <p>Implement coordinate tokenisation for visual grounding. Given a bounding box, convert it to discrete tokens; given discrete tokens, reconstruct the bounding box. Visualise the quantisation error at different bin resolutions. <pre><code>import jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef encode_bbox(bbox, num_bins=1000):\n    \"\"\"Convert continuous bbox (x, y, w, h) in [0,1] to discrete tokens.\"\"\"\n    tokens = jnp.round(jnp.array(bbox) * (num_bins - 1)).astype(jnp.int32)\n    return tokens\n\ndef decode_bbox(tokens, num_bins=1000):\n    \"\"\"Convert discrete tokens back to continuous bbox.\"\"\"\n    return tokens.astype(jnp.float32) / (num_bins - 1)\n\n# Ground-truth bounding box (normalised to [0, 1])\ngt_bbox = jnp.array([0.123, 0.456, 0.333, 0.222])\n\n# Test quantisation at different bin resolutions\nbin_sizes = [10, 50, 100, 500, 1000]\nerrors = []\nfor n_bins in bin_sizes:\n    tokens = encode_bbox(gt_bbox, n_bins)\n    reconstructed = decode_bbox(tokens, n_bins)\n    error = jnp.max(jnp.abs(gt_bbox - reconstructed))\n    errors.append(float(error))\n    print(f\"Bins={n_bins:&gt;5d} | Tokens={tokens} | \"\n          f\"Reconstructed={reconstructed} | Max error={error:.6f}\")\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.plot(bin_sizes, errors, 'o-', color='#e74c3c', linewidth=2, markersize=8)\nax.set_xlabel('Number of Bins'); ax.set_ylabel('Max Quantisation Error')\nax.set_title('Bounding Box Quantisation Error vs Bin Resolution')\nax.set_xscale('log'); ax.set_yscale('log')\nax.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n# Try: what happens with very few bins (e.g., 5)? When is the error acceptable?\n</code></pre></p> </li> </ol>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/","title":"Image and Video Tokenisation","text":"<p>Image and video tokenisation converts continuous visual data into discrete token sequences that transformers can process like text. This file covers VQ-VAE, VQ-GAN, codebook learning, DALL-E's dVAE, video tokenisation, and lookup-free quantisation -- the compression layer that enables autoregressive and diffusion-based visual generation.</p>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#why-tokenise-images","title":"Why Tokenise Images","text":"<ul> <li> <p>Think of language as a finite alphabet: English has roughly 26 letters, and modern language models carve text into 30,000-100,000 subword tokens. Every sentence becomes a sequence of discrete symbols that a transformer can predict one by one. Images, on the other hand, live in a continuous, high-dimensional space: a single 256x256 RGB image is a point in \\(\\mathbb{R}^{256 \\times 256 \\times 3} \\approx \\mathbb{R}^{196{,}608}\\). If you want a language model to \"speak\" images with the same machinery it uses to speak English, you need to convert those continuous pixel arrays into a manageable sequence of discrete tokens drawn from a finite vocabulary. That conversion is image tokenisation.</p> </li> <li> <p>Imagine you are a mosaic artist. You do not have infinite shades of tile; you have a fixed palette of, say, 8192 distinct tile colours. To reproduce a photograph as a mosaic, you must (1) decide which region of the photo each tile represents, (2) pick the closest tile colour for each region, and (3) accept that some detail is lost but the overall picture is recognisable. Image tokenisation does exactly this: an encoder compresses spatial patches into latent vectors, a codebook maps each vector to its nearest entry, and the result is a grid of integer indices, one per patch, that a discrete model can process.</p> </li> <li> <p>The benefits of tokenisation are threefold. First, it compresses the image dramatically: a 256x256 image might become a 16x16 grid of tokens, reducing the sequence length from 65,536 pixels to 256 tokens, which is tractable for attention-based models whose cost scales quadratically with sequence length. Second, it unifies the representation: text tokens and image tokens live in the same discrete vocabulary, enabling a single autoregressive transformer to generate interleaved text and images. Third, it imposes a useful bottleneck that forces the model to learn semantically meaningful codes rather than memorising pixel noise.</p> </li> </ul> <p></p> <ul> <li>Recall from Chapter 8 how convolutional networks extract hierarchical feature maps from images, and from Chapter 7 how text tokenisers convert strings into integer sequences. Image tokenisation sits at the intersection: it uses a CNN or vision transformer encoder (Chapter 8) to produce spatial features, then borrows the idea of a discrete vocabulary (Chapter 7) to convert those features into token indices.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#vq-vae-vector-quantisation","title":"VQ-VAE: Vector Quantisation","text":"<ul> <li> <p>As we saw in Chapter 6, a standard variational autoencoder (VAE) encodes an input into a continuous latent distribution and decodes samples from that distribution back into reconstructions. The latent space is continuous, which makes it awkward to feed into discrete sequence models. The Vector Quantised Variational Autoencoder (VQ-VAE), introduced by van den Oord et al. (2017), replaces the continuous latent with a discrete one by introducing a learnable codebook of embedding vectors and snapping each encoder output to its nearest codebook entry.</p> </li> <li> <p>Picture a library with exactly \\(K\\) labelled shelves. When a new book (encoder output) arrives, the librarian places it on the shelf whose existing books (codebook vectors) it most closely resembles, and records the shelf number. Later, to retrieve the book, you only need the shelf number: the codebook entry on that shelf is a good enough stand-in. This is vector quantisation.</p> </li> <li> <p>Formally, the VQ-VAE has three components:</p> </li> <li> <p>An encoder \\(E\\) that maps an input image \\(\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times 3}\\) to a spatial grid of continuous latent vectors \\(\\mathbf{z}_e = E(\\mathbf{x}) \\in \\mathbb{R}^{h \\times w \\times d}\\), where \\(h \\times w\\) is the downsampled spatial resolution and \\(d\\) is the embedding dimension.</p> </li> <li> <p>A codebook \\(\\mathcal{C} = \\{\\mathbf{e}_1, \\mathbf{e}_2, \\ldots, \\mathbf{e}_K\\} \\subset \\mathbb{R}^d\\) containing \\(K\\) learnable embedding vectors. Typical codebook sizes range from 512 to 16,384 entries.</p> </li> <li> <p>A decoder \\(D\\) that reconstructs the image from the quantised latents.</p> </li> <li> <p>The quantisation step replaces each encoder output \\(\\mathbf{z}_e(\\mathbf{x})\\) at spatial position \\((i, j)\\) with its nearest codebook entry:</p> </li> </ul> \\[\\mathbf{z}_q(i,j) = \\mathbf{e}_{k^\\ast} \\quad \\text{where} \\quad k^\\ast = \\arg\\min_k \\|\\mathbf{z}_e(i,j) - \\mathbf{e}_k\\|_2\\] <ul> <li>This is a nearest-neighbour lookup in embedding space, exactly the same operation as k-means assignment (Chapter 6). The index \\(k^\\ast\\) is the discrete token for spatial position \\((i,j)\\), and the full image is represented as an \\(h \\times w\\) grid of integers from \\(\\{1, \\ldots, K\\}\\).</li> </ul> <p></p> <ul> <li>The challenge is that \\(\\arg\\min\\) is not differentiable: you cannot backpropagate through a discrete selection. VQ-VAE solves this with the straight-through estimator: during the forward pass, the decoder receives \\(\\mathbf{z}_q\\) (the quantised vector); during the backward pass, the gradient of the reconstruction loss with respect to \\(\\mathbf{z}_q\\) is copied directly to \\(\\mathbf{z}_e\\), as if the quantisation step were the identity function. This is written compactly as:</li> </ul> \\[\\mathbf{z}_q = \\mathbf{z}_e + \\text{sg}(\\mathbf{z}_q - \\mathbf{z}_e)\\] <ul> <li> <p>where \\(\\text{sg}(\\cdot)\\) is the stop-gradient operator. In the forward pass this evaluates to \\(\\mathbf{z}_q\\); in the backward pass, the gradient flows through only the \\(\\mathbf{z}_e\\) term.</p> </li> <li> <p>The full VQ-VAE loss has three terms:</p> </li> </ul> \\[\\mathcal{L} = \\underbrace{\\|\\mathbf{x} - D(\\mathbf{z}_q)\\|_2^2}_{\\text{reconstruction}} + \\underbrace{\\|\\text{sg}(\\mathbf{z}_e) - \\mathbf{e}\\|_2^2}_{\\text{codebook (VQ)}} + \\underbrace{\\beta \\|\\mathbf{z}_e - \\text{sg}(\\mathbf{e})\\|_2^2}_{\\text{commitment}}\\] <ul> <li> <p>The reconstruction loss trains the encoder and decoder to faithfully reproduce the input. The codebook loss (also called the VQ loss) pulls the codebook vectors toward the encoder outputs; note that \\(\\text{sg}(\\mathbf{z}_e)\\) means the encoder does not receive gradients from this term, so it only updates the codebook. The commitment loss does the reverse: it encourages the encoder outputs to stay close to the codebook vectors, preventing the encoder from \"running away\" from the codebook. The hyperparameter \\(\\beta\\) (typically 0.25) controls the balance between the codebook and commitment terms.</p> </li> <li> <p>In practice, the codebook is often updated with an exponential moving average (EMA) rather than gradient descent, which is more stable. Let \\(\\mathbf{n}_k\\) be the count of encoder outputs assigned to codebook entry \\(k\\) and \\(\\mathbf{s}_k\\) be their sum. The EMA update is:</p> </li> </ul> \\[\\mathbf{n}_k \\leftarrow \\gamma \\mathbf{n}_k + (1 - \\gamma) |\\{(i,j) : k^\\ast_{ij} = k\\}|\\] \\[\\mathbf{s}_k \\leftarrow \\gamma \\mathbf{s}_k + (1 - \\gamma) \\sum_{(i,j) : k^\\ast_{ij} = k} \\mathbf{z}_e(i,j)\\] \\[\\mathbf{e}_k \\leftarrow \\frac{\\mathbf{s}_k}{\\mathbf{n}_k}\\] <ul> <li>where \\(\\gamma\\) is the decay rate (typically 0.99). This is equivalent to running an online k-means algorithm on the encoder outputs.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#codebook-collapse","title":"Codebook Collapse","text":"<ul> <li> <p>A notorious failure mode of VQ-VAE is codebook collapse (also called index collapse): the model learns to use only a small fraction of the \\(K\\) codebook entries, leaving most entries \"dead.\" Imagine a library where 90% of the shelves are empty because the librarian always routes books to the same few popular shelves. This wastes representational capacity.</p> </li> <li> <p>Codebook collapse occurs because the encoder, codebook, and decoder co-adapt during training. If an entry is not selected for several batches, it drifts away from the encoder manifold, making it even less likely to be selected, creating a positive feedback loop.</p> </li> <li> <p>Several techniques mitigate codebook collapse:</p> <ul> <li>Codebook reset: periodically reinitialise dead entries by copying randomly sampled encoder outputs. This gives dead entries a fresh start near the active region of the latent space.</li> <li>EMA updates with Laplace smoothing: add a small constant to \\(\\mathbf{n}_k\\) to prevent any entry from having zero count, ensuring all entries receive gradient signal.</li> <li>Commitment loss tuning: increasing \\(\\beta\\) forces encoder outputs to cluster more tightly around codebook entries, distributing assignments more evenly.</li> <li>Factorised codes: decompose the codebook lookup into a product of smaller lookups (e.g., two codebooks of size \\(\\sqrt{K}\\) each), which improves utilisation by reducing the effective codebook size for each lookup.</li> <li>Entropy regularisation: add a penalty that encourages a uniform distribution over codebook usage, maximising the entropy \\(H = -\\sum_k p_k \\log p_k\\) where \\(p_k\\) is the empirical assignment probability.</li> </ul> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#vq-gan-adversarial-training-for-higher-fidelity","title":"VQ-GAN: Adversarial Training for Higher Fidelity","text":"<ul> <li> <p>VQ-VAE produces decent reconstructions, but the pixel-level \\(\\ell_2\\) loss tends to generate blurry outputs because it penalises every pixel deviation equally, averaging over plausible details rather than choosing crisp ones. Imagine asking someone to draw a face that minimises the average difference from all possible faces \u2014 they would draw a blurry average face, not a sharp individual one.</p> </li> <li> <p>VQ-GAN (Esser et al., 2021) addresses this by combining the VQ-VAE framework with a discriminator from generative adversarial networks (Chapter 6). The discriminator is a patch-based convolutional network that judges whether a local image patch is real (from the training data) or fake (from the decoder). This adversarial loss encourages the decoder to produce perceptually sharp, realistic textures instead of pixel-wise averages.</p> </li> <li> <p>The VQ-GAN objective adds two terms to the VQ-VAE loss:</p> </li> </ul> \\[\\mathcal{L}_\\text{VQ-GAN} = \\mathcal{L}_\\text{VQ-VAE} + \\lambda_\\text{adv} \\mathcal{L}_\\text{adv} + \\lambda_\\text{perc} \\mathcal{L}_\\text{perc}\\] <ul> <li>The adversarial loss \\(\\mathcal{L}_\\text{adv}\\) is the standard GAN objective applied to the decoder output. The discriminator \\(\\mathcal{D}\\) tries to distinguish real patches from decoded patches, and the decoder (generator) tries to fool it. The non-saturating formulation is:</li> </ul> \\[\\mathcal{L}_\\text{adv} = -\\mathbb{E}[\\log \\mathcal{D}(D(\\mathbf{z}_q))]\\] <ul> <li>The perceptual loss \\(\\mathcal{L}_\\text{perc}\\) compares feature activations from a pretrained network (typically VGG or LPIPS) between the original and reconstructed images:</li> </ul> \\[\\mathcal{L}_\\text{perc} = \\sum_l \\|\\phi_l(\\mathbf{x}) - \\phi_l(D(\\mathbf{z}_q))\\|_2^2\\] <ul> <li> <p>where \\(\\phi_l\\) denotes the feature map at layer \\(l\\) of the pretrained network. This loss captures high-level structural similarity rather than pixel-level accuracy.</p> </li> <li> <p>The weight \\(\\lambda_\\text{adv}\\) is adaptively set so that the adversarial gradient and reconstruction gradient are balanced, preventing the adversarial loss from dominating early in training when the reconstructions are poor.</p> </li> </ul> <p></p> <ul> <li>The result is a tokeniser that produces dramatically sharper reconstructions than VQ-VAE at the same codebook size. VQ-GAN is the backbone tokeniser behind many major image generation systems, including the original DALL-E, Parti, and numerous text-to-image models. It turns a 256x256 image into a 16x16 or 32x32 grid of discrete tokens from a codebook of size 1024-16384, achieving compression ratios of 16x to 64x in each spatial dimension.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#residual-quantisation-and-multi-scale-codebooks","title":"Residual Quantisation and Multi-Scale Codebooks","text":"<ul> <li> <p>A single codebook imposes a hard ceiling on reconstruction quality: each spatial position is represented by exactly one codebook vector, and any detail finer than the codebook can express is lost. Think of describing a colour with a single word from a fixed palette: \"teal\" is close but not exact. If you could add a refinement \u2014 \"teal, but slightly more blue and a touch brighter\" \u2014 you would get much closer.</p> </li> <li> <p>Residual quantisation (RQ) applies this idea iteratively. After the first quantisation step produces \\(\\mathbf{z}_q^{(1)}\\), compute the residual \\(\\mathbf{r}^{(1)} = \\mathbf{z}_e - \\mathbf{z}_q^{(1)}\\), then quantise the residual against a second codebook to get \\(\\mathbf{z}_q^{(2)}\\), and so on for \\(T\\) levels:</p> </li> </ul> \\[\\mathbf{r}^{(0)} = \\mathbf{z}_e\\] \\[\\mathbf{z}_q^{(t)} = \\text{Quantise}(\\mathbf{r}^{(t-1)}, \\mathcal{C}^{(t)})\\] \\[\\mathbf{r}^{(t)} = \\mathbf{r}^{(t-1)} - \\mathbf{z}_q^{(t)}\\] <ul> <li> <p>The final quantised representation is \\(\\hat{\\mathbf{z}} = \\sum_{t=1}^{T} \\mathbf{z}_q^{(t)}\\). With \\(T\\) levels each using a codebook of size \\(K\\), the effective vocabulary size is \\(K^T\\), but you only need to store \\(T \\times K\\) vectors rather than \\(K^T\\). For example, 8 levels with \\(K = 1024\\) give an effective \\(1024^8 \\approx 10^{24}\\) entries while storing only 8192 vectors.</p> </li> <li> <p>Each successive level captures finer details: the first codebook captures the coarse structure, the second captures medium-frequency corrections, and so on. This is analogous to successive approximation in JPEG or progressive rendering in web images, where a rough version appears first and detail fills in incrementally.</p> </li> </ul> <p></p> <ul> <li> <p>Multi-scale codebooks extend this idea by operating at different spatial resolutions. Instead of quantising the same spatial grid repeatedly, you quantise at multiple scales: a coarse grid captures global structure, finer grids capture local detail. This is related to the feature pyramid idea from Chapter 8's object detection section, where features at different scales capture different levels of detail.</p> </li> <li> <p>Product quantisation is a related technique where the \\(d\\)-dimensional latent vector is split into \\(M\\) sub-vectors of dimension \\(d/M\\), and each sub-vector is quantised independently with its own codebook. This gives an effective vocabulary of \\(K^M\\) while storing only \\(M \\times K\\) vectors. Product quantisation is widely used in approximate nearest-neighbour search (Chapter 13) and has been adapted for image tokenisation.</p> </li> <li> <p>Finite scalar quantisation (FSQ), introduced by Mentzer et al. (2023), takes a different approach entirely: instead of learning a codebook, it simply rounds each dimension of the latent vector to one of a fixed set of integer levels (e.g., \\(\\{-2, -1, 0, 1, 2\\}\\)). With \\(L\\) levels per dimension and \\(d\\) dimensions, the implicit codebook size is \\(L^d\\). FSQ avoids codebook collapse entirely because there are no learned codebook vectors, only learned encoder outputs that are rounded deterministically. The straight-through estimator handles the non-differentiability of rounding.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#image-tokenisers-in-practice","title":"Image Tokenisers in Practice","text":"<ul> <li>The progression from VQ-VAE to VQ-GAN to residual quantisation has spawned a family of practical image tokenisers used in state-of-the-art generative models.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#dall-e-tokeniser-dvae","title":"DALL-E Tokeniser (dVAE)","text":"<ul> <li>The original DALL-E (Ramesh et al., 2021) used a discrete VAE (dVAE) to tokenise 256x256 images into 32x32 grids of tokens from a codebook of size 8192. The dVAE replaced the hard \\(\\arg\\min\\) quantisation with a Gumbel-Softmax relaxation, making the forward pass differentiable during training. At inference time, the \\(\\arg\\max\\) is used to produce hard token assignments. The dVAE was trained with a combination of reconstruction loss, KL divergence against a uniform prior, and a learned temperature schedule for the Gumbel-Softmax. DALL-E then trained a 12-billion parameter autoregressive transformer to model the joint distribution of 256 text tokens and 1024 image tokens (32x32).</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#llamagen","title":"LlamaGen","text":"<ul> <li>LlamaGen (Sun et al., 2024) showed that you can repurpose a standard Llama-style language model architecture (Chapter 7) for autoregressive image generation, provided you have a good image tokeniser. LlamaGen uses an improved VQ-GAN tokeniser with a large codebook (16,384 entries) and trains a vanilla autoregressive transformer (with no special image-specific modifications beyond the tokeniser) to predict image tokens left-to-right in raster scan order. The key insight is that once images are tokenised into discrete sequences, the same next-token-prediction paradigm that works for language works for images, validating the idea that tokenisation truly bridges the modality gap.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#cosmos-tokeniser","title":"Cosmos Tokeniser","text":"<ul> <li>The Cosmos tokeniser (NVIDIA, 2024) is designed for both images and videos in a unified framework. It uses a causal 3D architecture that treats images as single-frame videos, allowing the same tokeniser to handle both modalities. Cosmos supports both continuous and discrete tokenisation modes: the continuous mode outputs real-valued latent vectors (for diffusion model backends), while the discrete mode applies finite scalar quantisation to produce integer tokens (for autoregressive model backends). The encoder uses causal 3D convolutions so that each frame's tokens depend only on the current and previous frames, enabling streaming video tokenisation.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#video-tokenisation","title":"Video Tokenisation","text":"<ul> <li> <p>Video adds a third axis \u2014 time \u2014 to the spatial dimensions of images. A video is a sequence of frames, typically at 24-30 frames per second, and adjacent frames are highly redundant because the visual world does not change drastically in 33 milliseconds. Video tokenisation exploits this temporal redundancy to achieve much higher compression than tokenising each frame independently.</p> </li> <li> <p>Think of video compression like a flip-book. If you drew every page from scratch, you would need thousands of detailed drawings. But most pages are nearly identical to their neighbours, so you could draw a full \"keyframe\" every 10 pages and only note the small changes on the pages in between. Video tokenisers learn this trick automatically.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#3d-vq-vae","title":"3D VQ-VAE","text":"<ul> <li> <p>The most straightforward extension of VQ-VAE to video is the 3D VQ-VAE, which replaces 2D convolutions in the encoder and decoder with 3D convolutions that operate over the spatial and temporal dimensions simultaneously. If the encoder downsamples by a factor of \\(f_s\\) spatially and \\(f_t\\) temporally, a video clip of \\(T \\times H \\times W\\) becomes a token grid of \\((T/f_t) \\times (H/f_s) \\times (W/f_s)\\).</p> </li> <li> <p>For example, with \\(f_s = 16\\) and \\(f_t = 4\\), a 16-frame 256x256 video clip becomes a \\(4 \\times 16 \\times 16 = 1024\\) token sequence. This is compact enough for a transformer to model autoregressively, whereas the raw pixel count would be \\(16 \\times 256 \\times 256 \\times 3 \\approx 3.1\\) million values.</p> </li> <li> <p>The 3D convolutions jointly learn spatial and temporal features. Early layers capture local motion (edges moving between frames) while deeper layers capture higher-level dynamics (objects appearing, disappearing, or changing shape). This is the same hierarchical feature extraction principle from Chapter 8's convolutional networks, extended along the time axis.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#causal-video-tokenisers","title":"Causal Video Tokenisers","text":"<ul> <li> <p>A standard 3D convolution looks at past, current, and future frames, which means you need the entire video clip before you can tokenise any of it. Causal video tokenisers constrain the temporal convolutions so that each output depends only on the current and previous frames, never future frames. This is analogous to the causal masking in autoregressive transformers (Chapter 7): information flows forward in time but never backward.</p> </li> <li> <p>Causal tokenisation is essential for two use cases. First, streaming: you can tokenise video in real time as frames arrive, without buffering future frames. Second, autoregressive generation: when a transformer generates video frame-by-frame, the tokens for frame \\(t\\) must be computable without knowing frame \\(t+1\\), because frame \\(t+1\\) has not been generated yet.</p> </li> <li> <p>The causal constraint is implemented by padding temporal convolutions asymmetrically: a kernel of temporal size \\(k\\) is padded with \\(k-1\\) zeros on the past side and zero zeros on the future side, ensuring the output at time \\(t\\) depends only on inputs at times \\(t-k+1, \\ldots, t\\).</p> </li> <li> <p>One elegant property of causal video tokenisers is that they can tokenise a single image (a \"video\" of one frame) with no special handling. The first frame has no past context, so its tokens are computed from the frame alone. This image-video unification means a single tokeniser serves both modalities, simplifying the architecture and enabling models that generate images and videos with the same decoder.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#temporal-compression-strategies","title":"Temporal Compression Strategies","text":"<ul> <li> <p>Different applications demand different temporal compression ratios. For action recognition (where subtle motions matter), gentle compression (\\(f_t = 2\\)) preserves temporal detail. For long-form video generation (where storing thousands of frames is prohibitive), aggressive compression (\\(f_t = 8\\) or higher) is necessary.</p> </li> <li> <p>Some tokenisers use factorised compression: spatial and temporal compression are performed in separate stages. First, a 2D encoder compresses each frame independently, producing a per-frame latent grid. Then, a 1D temporal encoder compresses across the time dimension. This factorisation is computationally cheaper than full 3D convolution and allows different compression ratios for space and time. The trade-off is that it cannot capture spatiotemporal patterns (like a ball moving diagonally) as efficiently as joint 3D encoding.</p> </li> <li> <p>Temporal interpolation tokens are a recent innovation where the tokeniser encodes only keyframes fully and represents intermediate frames as lightweight interpolation codes that describe how to morph between keyframes. This mirrors classical video compression (I-frames and P-frames in H.264/HEVC) but in a learned latent space.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#continuous-vs-discrete-tokens","title":"Continuous vs Discrete Tokens","text":"<ul> <li> <p>Not every downstream model needs discrete tokens. Diffusion models (Chapter 10, file 04) work natively with continuous values \u2014 they iteratively denoise a Gaussian sample, and their loss functions (denoising score matching) are defined over continuous spaces. For diffusion backends, the tokeniser encoder produces continuous latent vectors that are never quantised. Latent diffusion models (Stable Diffusion, DALL-E 3, Flux) use a VQ-GAN-like encoder-decoder but skip the codebook entirely, operating in the continuous latent space.</p> </li> <li> <p>Autoregressive models (GPT-style), on the other hand, predict the next token from a finite vocabulary using a softmax over \\(K\\) classes. They fundamentally require discrete tokens. Every image generation system that uses an autoregressive transformer (DALL-E, Parti, LlamaGen, Chameleon) depends on a discrete tokeniser.</p> </li> <li> <p>The choice between continuous and discrete tokens is therefore driven by the generation backend:</p> </li> <li> <p>Use discrete tokens when: the model is autoregressive (next-token prediction with cross-entropy loss), you want to share a vocabulary with text tokens for unified multimodal models, or you need exact token-level control (e.g., for retrieval or editing by token replacement).</p> </li> <li> <p>Use continuous tokens when: the model is a diffusion model or flow-matching model, the task requires very high fidelity reconstruction (continuous latents avoid quantisation error entirely), or you want to use regression losses that operate on real-valued vectors.</p> </li> <li> <p>Some recent architectures support both modes. The Cosmos tokeniser, for instance, can output either continuous latents (for its diffusion mode) or FSQ-discretised tokens (for its autoregressive mode) from the same encoder, with a lightweight quantisation head that can be switched on or off.</p> </li> <li> <p>Soft quantisation is a middle ground: instead of hard \\(\\arg\\min\\) assignment, compute a weighted average of the top-\\(k\\) nearest codebook entries, with weights given by a softmax over negative distances. This preserves more information than hard quantisation while still being approximately discrete. Some systems use soft quantisation during training and hard quantisation at inference.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#applications","title":"Applications","text":""},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#autoregressive-image-generation","title":"Autoregressive Image Generation","text":"<ul> <li> <p>Once images are discrete token sequences, you can train a standard autoregressive transformer to model them. The image tokens are flattened into a 1D sequence (typically in raster scan order: left-to-right, top-to-bottom) and the transformer learns \\(p(\\text{token}_i | \\text{token}_1, \\ldots, \\text{token}_{i-1})\\) with the standard cross-entropy loss. At generation time, tokens are sampled one by one and the completed grid is passed through the tokeniser's decoder to produce pixels.</p> </li> <li> <p>Conditioning on text is straightforward: prepend text tokens to the image token sequence, so the model learns \\(p(\\text{image tokens} | \\text{text tokens})\\). This is exactly how DALL-E, Parti, and LlamaGen perform text-to-image generation. The text and image tokens share the same transformer, the same attention mechanism, and often the same embedding table (with text and image tokens occupying different index ranges).</p> </li> <li> <p>The raster scan order introduces an artificial asymmetry: the top-left of the image is generated first, without any context about the bottom-right. Several works address this. Masked image modelling (MaskGIT) trains a bidirectional transformer that generates all tokens simultaneously but with varying confidence, iteratively unmasking the most confident tokens. Multi-scale generation generates coarse tokens first (capturing global composition) and then refines with residual tokens. These approaches trade off the simplicity of pure left-to-right generation for better global coherence.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#unified-vision-language-tokens","title":"Unified Vision-Language Tokens","text":"<ul> <li> <p>The deepest motivation for image tokenisation is unification: putting vision and language into the same representational format so that a single model architecture handles both. As we discussed in Chapter 7, language models are extraordinarily capable sequence-to-sequence machines. By representing images as token sequences, we inherit all the infrastructure of language modelling \u2014 pretraining recipes, scaling laws, RLHF, context length extensions \u2014 for free.</p> </li> <li> <p>Chameleon (Meta, 2024) is a prominent example: it uses a VQ-GAN tokeniser with 8192 codebook entries to convert images into tokens that are interleaved with text tokens in a single vocabulary of ~65,000 entries (text + image). A standard transformer is trained on mixed text-image sequences, enabling it to generate text given images, images given text, or interleaved text-and-image content, all with the same forward pass.</p> </li> <li> <p>Gemini (Google, 2024) takes a similar approach at massive scale, natively understanding and generating images, audio, and text within a single transformer, with modality-specific tokenisers feeding into a shared sequence.</p> </li> <li> <p>The key engineering challenge in unified models is vocabulary balance: if 8192 out of 65,000 vocabulary entries are image tokens, the model may under-allocate capacity to vision. Solutions include separate embedding layers for each modality (shared only at the attention level), modality-specific loss weighting, and careful data mixing ratios during pretraining.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/03.%20image%20and%20video%20tokenisation/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement a minimal VQ layer in JAX: given a batch of encoder output vectors, perform nearest-neighbour codebook lookup and compute the VQ-VAE loss (reconstruction + codebook + commitment). Visualise codebook utilisation as a histogram. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# --- Minimal VQ layer ---\nkey = jax.random.PRNGKey(42)\nd = 8          # embedding dimension\nK = 64         # codebook size\nn_vectors = 256  # batch of encoder outputs\n\n# Random encoder outputs and codebook\nk1, k2 = jax.random.split(key)\nz_e = jax.random.normal(k1, (n_vectors, d))       # encoder outputs\ncodebook = jax.random.normal(k2, (K, d)) * 0.1     # codebook (small init)\n\n# Nearest-neighbour lookup: find closest codebook entry for each z_e\n# distances[i, k] = ||z_e[i] - codebook[k]||^2\ndistances = (\n    jnp.sum(z_e ** 2, axis=1, keepdims=True)\n    - 2 * z_e @ codebook.T\n    + jnp.sum(codebook ** 2, axis=1, keepdims=True).T\n)\nindices = jnp.argmin(distances, axis=1)       # token indices\nz_q = codebook[indices]                        # quantised vectors\n\n# VQ-VAE loss terms\nbeta = 0.25\nloss_codebook = jnp.mean((jax.lax.stop_gradient(z_e) - z_q) ** 2)\nloss_commit   = jnp.mean((z_e - jax.lax.stop_gradient(z_q)) ** 2)\nloss_total    = loss_codebook + beta * loss_commit\nprint(f\"Codebook loss: {loss_codebook:.4f}, Commitment loss: {loss_commit:.4f}\")\n\n# Codebook utilisation\nunique, counts = jnp.unique(indices, return_counts=True, size=K, fill_value=-1)\nplt.figure(figsize=(10, 4))\nplt.bar(range(K), counts, color='#3498db', alpha=0.8)\nplt.xlabel('Codebook Index'); plt.ylabel('Assignment Count')\nplt.title(f'Codebook Utilisation ({jnp.sum(counts &gt; 0)}/{K} entries used)')\nplt.grid(True, alpha=0.3); plt.tight_layout(); plt.show()\n# Try: increase K to 512 and observe collapse. Then add codebook reset logic.\n</code></pre></p> </li> <li> <p>Build a toy 2D vector quantiser that learns to tile a 2D distribution. Generate random 2D points, learn a codebook via EMA updates, and visualise the Voronoi regions. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Generate 2D data from a mixture of Gaussians\nkey = jax.random.PRNGKey(0)\nn_points = 2000\nK = 16  # codebook entries\ngamma = 0.99  # EMA decay\n\n# Four clusters\nkeys = jax.random.split(key, 5)\ncentres = jnp.array([[2, 2], [-2, 2], [-2, -2], [2, -2]], dtype=jnp.float32)\ndata = jnp.concatenate([\n    jax.random.normal(keys[i], (n_points // 4, 2)) * 0.5 + centres[i]\n    for i in range(4)\n])\n\n# Initialise codebook from random data points\nidx = jax.random.choice(keys[4], n_points, (K,), replace=False)\ncodebook = data[idx]\nema_count = jnp.ones(K)\nema_sum = codebook.copy()\n\n# Run EMA-based codebook learning for several epochs\nfor epoch in range(30):\n    # Assign each point to nearest codebook entry\n    dists = jnp.sum((data[:, None, :] - codebook[None, :, :]) ** 2, axis=2)\n    assignments = jnp.argmin(dists, axis=1)\n    # EMA update\n    for k in range(K):\n        mask = (assignments == k)\n        count_k = jnp.sum(mask)\n        ema_count = ema_count.at[k].set(gamma * ema_count[k] + (1 - gamma) * count_k)\n        if count_k &gt; 0:\n            sum_k = jnp.sum(data[mask], axis=0)\n            ema_sum = ema_sum.at[k].set(gamma * ema_sum[k] + (1 - gamma) * sum_k)\n    codebook = ema_sum / ema_count[:, None]\n\n# Visualise assignments and codebook\nfig, ax = plt.subplots(1, 1, figsize=(8, 8))\ncolors = plt.cm.tab20(jnp.linspace(0, 1, K))\nfor k in range(K):\n    mask = assignments == k\n    ax.scatter(data[mask, 0], data[mask, 1], c=[colors[k]], s=5, alpha=0.3)\nax.scatter(codebook[:, 0], codebook[:, 1], c='black', s=120, marker='X',\n           edgecolors='white', linewidths=1.5, zorder=10, label='Codebook')\nax.set_title(f'Learned VQ Codebook ({K} entries) on 2D Data')\nax.legend(); ax.set_aspect('equal'); ax.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()\n# Try: increase K to 64 and observe finer tiling. Reduce gamma and see instability.\n</code></pre></p> </li> <li> <p>Demonstrate residual quantisation: encode a batch of vectors with \\(T\\) successive quantisation stages and measure how the reconstruction error decreases with each level. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nkey = jax.random.PRNGKey(7)\nd = 16         # embedding dimension\nK = 32         # codebook size per level\nT = 8          # number of residual levels\nn_vectors = 512\n\n# Random data to quantise\nk1, *cb_keys = jax.random.split(key, T + 1)\nz = jax.random.normal(k1, (n_vectors, d))\n\n# Independent random codebooks for each level\ncodebooks = [jax.random.normal(cb_keys[t], (K, d)) * (0.5 ** t)\n             for t in range(T)]\n\n# Residual quantisation loop\nresidual = z.copy()\nz_hat = jnp.zeros_like(z)\nerrors = []\n\nfor t in range(T):\n    cb = codebooks[t]\n    dists = (jnp.sum(residual ** 2, axis=1, keepdims=True)\n             - 2 * residual @ cb.T\n             + jnp.sum(cb ** 2, axis=1, keepdims=True).T)\n    indices = jnp.argmin(dists, axis=1)\n    z_q_t = cb[indices]\n    z_hat = z_hat + z_q_t\n    residual = residual - z_q_t\n    mse = jnp.mean(jnp.sum((z - z_hat) ** 2, axis=1))\n    errors.append(float(mse))\n    print(f\"Level {t+1}: MSE = {mse:.4f}\")\n\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, T + 1), errors, 'o-', color='#e74c3c', linewidth=2, markersize=8)\nplt.xlabel('Residual Quantisation Level')\nplt.ylabel('Reconstruction MSE')\nplt.title('Error Reduction with Residual Quantisation')\nplt.xticks(range(1, T + 1)); plt.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()\n# Try: use a single codebook of size K*T and compare with RQ. Which wins?\n</code></pre></p> </li> <li> <p>Simulate a simple 1D \"video tokeniser\": generate a sequence of 1D signals (mimicking video frames), apply causal temporal compression, and compare with non-causal compression in terms of reconstruction quality. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\nkey = jax.random.PRNGKey(99)\nn_frames = 16\nframe_len = 64\n\n# Generate a \"video\": a slowly moving Gaussian bump across frames\nx_axis = jnp.linspace(-3, 3, frame_len)\nframes = jnp.stack([\n    jnp.exp(-0.5 * (x_axis - (-2 + 4 * t / n_frames)) ** 2)\n    for t in range(n_frames)\n])  # shape: (n_frames, frame_len)\n\n# Causal temporal compression: each frame's code depends only on past frames\n# Simple approach: average current frame with exponential decay of past\nalpha_causal = 0.6\ncausal_codes = jnp.zeros_like(frames)\ncausal_codes = causal_codes.at[0].set(frames[0])\nfor t in range(1, n_frames):\n    causal_codes = causal_codes.at[t].set(\n        alpha_causal * frames[t] + (1 - alpha_causal) * causal_codes[t - 1]\n    )\n\n# Non-causal: average with both past and future (bilateral smoothing)\nkernel = jnp.array([0.2, 0.6, 0.2])  # past, current, future\npadded = jnp.concatenate([frames[:1], frames, frames[-1:]], axis=0)\nnoncausal_codes = jnp.stack([\n    kernel[0] * padded[t] + kernel[1] * padded[t+1] + kernel[2] * padded[t+2]\n    for t in range(n_frames)\n])\n\n# Reconstruction error\nmse_causal = jnp.mean((frames - causal_codes) ** 2)\nmse_noncausal = jnp.mean((frames - noncausal_codes) ** 2)\nprint(f\"Causal MSE: {mse_causal:.6f}, Non-causal MSE: {mse_noncausal:.6f}\")\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\nfor ax, data, title in zip(axes,\n    [frames, causal_codes, noncausal_codes],\n    ['Original Frames', f'Causal (MSE={mse_causal:.5f})',\n     f'Non-causal (MSE={mse_noncausal:.5f})']):\n    ax.imshow(data, aspect='auto', cmap='viridis', origin='lower')\n    ax.set_xlabel('Spatial Position'); ax.set_ylabel('Frame Index')\n    ax.set_title(title)\nplt.tight_layout(); plt.show()\n# Try: vary alpha_causal and the kernel weights. What happens with alpha=1.0?\n</code></pre></p> </li> </ol>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/","title":"Cross-Modal Generation","text":"<p>Cross-modal generation produces output in one modality conditioned on input from another -- text to image, image to text, text to audio, and beyond. This file covers DALL-E, Stable Diffusion, classifier-free guidance, ControlNet, image captioning, text-to-video (Sora), and text-to-audio generation.</p> <ul> <li> <p>In files 01-03 of this chapter, you learned how to represent, align, and tokenise different modalities. Now comes the creative act: generating one modality from another. Cross-modal generation is the engine behind text-to-image tools, video synthesis systems, music composition models, and image captioning. Think of it as teaching a machine to be a multimedia artist \u2014 you describe what you want in words, and it paints, animates, or composes.</p> </li> <li> <p>The core idea is conditional generation: given an input from modality \\(A\\) (e.g., text), produce an output in modality \\(B\\) (e.g., image). Formally, you learn a model \\(p_\\theta(y \\mid x)\\) where \\(x\\) is the conditioning signal and \\(y\\) is the generated output. The challenge is that this conditional distribution is enormously complex and high-dimensional \u2014 a 512x512 image lives in \\(\\mathbb{R}^{786432}\\), and there are many valid images for a single text prompt.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#text-to-image-generation","title":"Text-to-Image Generation","text":"<ul> <li>Imagine you describe a scene to a courtroom sketch artist. The artist must interpret your words, recall what objects look like, compose them spatially, and render the final picture. Text-to-image models do precisely this, but they must learn all of these skills from data rather than from years of art school.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#dall-e-autoregressive-image-generation","title":"DALL-E: Autoregressive Image Generation","text":"<ul> <li> <p>DALL-E (Ramesh et al., 2021) treats image generation as a sequence prediction problem \u2014 the same paradigm that powers language models (Chapter 07). The key insight is that if you can represent images as discrete tokens (recall VQ-VAE from file 03), then generating an image is just generating a sequence of tokens, one after another.</p> </li> <li> <p>The pipeline has two stages. First, a discrete VAE (dVAE) compresses a 256x256 image into a 32x32 grid of discrete tokens from a codebook of 8192 entries, reducing the image to a sequence of 1024 tokens. Second, a transformer decoder is trained to model the joint distribution of 256 text tokens (BPE-encoded) concatenated with 1024 image tokens, totalling 1280 tokens:</p> </li> </ul> \\[p(x_{\\text{text}}, x_{\\text{img}}) = \\prod_{i=1}^{1280} p(x_i \\mid x_1, \\ldots, x_{i-1})\\] <ul> <li> <p>At generation time, you feed in the text tokens and the model autoregressively samples image tokens one by one. This is elegant because it reuses the exact machinery of language modelling \u2014 attention, causal masking, top-k sampling \u2014 for image synthesis.</p> </li> <li> <p>The downside is that autoregressive generation is inherently sequential: generating 1024 tokens one at a time is slow, and any error early in the sequence compounds. DALL-E mitigated this by generating many candidate images and re-ranking them with CLIP (from file 01) to find the best match to the text prompt.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#stable-diffusion-latent-diffusion-with-text-conditioning","title":"Stable Diffusion: Latent Diffusion with Text Conditioning","text":"<ul> <li> <p>Stable Diffusion (Rombach et al., 2022) takes a fundamentally different approach. Instead of predicting tokens one by one, it starts with pure noise and gradually denoises it into an image, guided by a text prompt. Recall diffusion models from Chapter 8 \u2014 Stable Diffusion operates in a compressed latent space rather than pixel space, making it dramatically more efficient.</p> </li> <li> <p>The architecture has three components working in concert. A VAE encoder compresses the image from pixel space (\\(512 \\times 512 \\times 3\\)) to a latent representation (\\(64 \\times 64 \\times 4\\)), reducing dimensionality by a factor of 48. A text encoder (typically CLIP or OpenCLIP) converts the text prompt into a sequence of embedding vectors. A U-Net denoiser takes the noisy latent, the timestep, and the text embeddings, and predicts the noise to subtract at each step. Text conditioning enters the U-Net through cross-attention layers:</p> </li> </ul> \\[\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V\\] <ul> <li> <p>where \\(Q\\) comes from the noisy image features, and \\(K, V\\) come from the text embeddings. This lets the model attend to relevant words at each spatial location \u2014 when denoising the region where a \"red ball\" should appear, the model attends to the tokens \"red\" and \"ball\".</p> </li> <li> <p>At inference, you sample \\(z_T \\sim \\mathcal{N}(0, I)\\) in latent space, iteratively denoise using the U-Net for \\(T\\) steps (typically 20-50 with DDIM scheduling), and decode the clean latent \\(z_0\\) back to pixel space with the VAE decoder. The entire forward pass generates a 512x512 image in seconds on a consumer GPU.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#classifier-free-guidance-in-practice","title":"Classifier-Free Guidance in Practice","text":"<ul> <li>Classifier-free guidance (CFG) is the secret ingredient that makes text-to-image models produce images that actually match their prompts. Recall from Chapter 8 that CFG trains the model both conditionally and unconditionally, then amplifies the conditional signal at sampling time:</li> </ul> \\[\\hat{\\epsilon} = \\epsilon_\\theta(x_t, \\varnothing) + s \\cdot (\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\varnothing))\\] <ul> <li> <p>where \\(s\\) is the guidance scale. Think of the term \\((\\epsilon_\\theta(x_t, c) - \\epsilon_\\theta(x_t, \\varnothing))\\) as the \"direction toward the prompt\" \u2014 it captures what makes a conditioned prediction different from an unconditioned one. Multiplying by \\(s &gt; 1\\) exaggerates this direction, pushing the image closer to the text description at the cost of diversity.</p> </li> <li> <p>In practice, \\(s = 7.5\\) is a common default for Stable Diffusion. At \\(s = 1.0\\) you get the raw model output (diverse but loosely matching the prompt). At \\(s = 20+\\) the images become oversaturated and repetitive but very closely aligned with the text. The optimal \\(s\\) depends on the application: creative exploration favours lower guidance, while precise prompt adherence demands higher guidance.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#imagen-cascaded-diffusion-with-language-understanding","title":"Imagen: Cascaded Diffusion with Language Understanding","text":"<ul> <li> <p>Imagen (Saharia et al., 2022) demonstrated that a powerful text encoder matters more than a larger image model. Instead of CLIP, Imagen uses a frozen T5-XXL language model (from Chapter 07) as the text encoder, which has a much richer understanding of language semantics, compositionality, and spatial relationships (\"a blue cube on top of a red sphere\").</p> </li> <li> <p>Imagen uses a cascaded diffusion approach: a base diffusion model generates a 64x64 image, a first super-resolution model upscales to 256x256, and a second super-resolution model reaches 1024x1024. Each stage is a separate diffusion model conditioned on the text and (for the upscalers) the lower-resolution image. This cascade avoids modelling fine details at the base resolution, allowing the base model to focus on composition and semantics while the upscalers handle texture and sharpness.</p> </li> <li> <p>Imagen also introduced dynamic thresholding: at each denoising step, predicted pixel values are clipped to a percentile-based range rather than a fixed range \\([-1, 1]\\). This prevents saturation artefacts at high guidance scales, a common problem in diffusion models.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#parti-autoregressive-at-scale","title":"Parti: Autoregressive at Scale","text":"<ul> <li> <p>Parti (Pathways Autoregressive Text-to-Image, Yu et al., 2022) revived the autoregressive approach with massive scale. Like DALL-E, it converts images to discrete tokens (using ViT-VQGAN) and generates them sequentially with a transformer. But Parti used a 20-billion-parameter encoder-decoder transformer (based on the Pathways architecture) and showed that autoregressive models can match diffusion quality when scaled sufficiently.</p> </li> <li> <p>Parti's encoder-decoder architecture is a key difference from DALL-E's decoder-only design. The text goes through the encoder; the decoder cross-attends to the encoded text while generating image tokens. This mirrors machine translation (Chapter 07) \u2014 you translate from \"text language\" to \"image language\".</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#dit-and-flow-based-generation","title":"DiT and Flow-Based Generation","text":"<ul> <li> <p>Diffusion Transformers (DiT) (Peebles and Xie, 2023) replace the U-Net backbone in diffusion models with a plain transformer. Each noisy latent patch is treated as a token (analogous to ViT from Chapter 8), and the transformer processes these tokens with self-attention and cross-attention to the text condition. DiT showed that transformers scale more predictably than U-Nets for diffusion \u2014 doubling compute reliably halves the FID score.</p> </li> <li> <p>Flow matching (recalled from Chapter 8) has emerged as an alternative to the diffusion noise-prediction paradigm. Instead of predicting noise \\(\\epsilon\\) to subtract, the model predicts a velocity \\(v_\\theta(x_t, t)\\) that transports samples along straight paths from noise to data. Stable Diffusion 3 and Flux adopt flow matching with a multimodal DiT (MM-DiT) architecture, where text and image tokens are processed jointly by transformer blocks with bidirectional attention \u2014 both modalities attend to each other rather than text only conditioning image features via cross-attention.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#text-to-video-generation","title":"Text-to-Video Generation","text":"<ul> <li>Text-to-video is text-to-image with a ruthless additional constraint: temporal coherence. Every frame must be internally consistent (a valid image), but consecutive frames must also be smoothly connected \u2014 objects should move naturally, lighting should change continuously, and the \"camera\" should follow physically plausible trajectories. Think of the difference between painting a single landscape and directing a film.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#temporal-challenges","title":"Temporal Challenges","text":"<ul> <li>Video introduces three challenges beyond image generation. Temporal consistency requires that objects maintain their identity across frames \u2014 a dog in frame 1 should still be the same dog in frame 100. Motion modelling requires learning physical dynamics: how objects move, how gravity works, how fluids flow. Compute cost is severe: a 10-second video at 24 fps and 512x512 resolution contains \\(10 \\times 24 \\times 512 \\times 512 \\times 3 \\approx 188\\) million values, roughly 240 times more data than a single image.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#make-a-video-and-extend-to-video-approaches","title":"Make-A-Video and Extend-to-Video Approaches","text":"<ul> <li> <p>Make-A-Video (Singer et al., 2022) took a pragmatic approach: start with a pre-trained text-to-image model and add temporal layers. The key insight is that you already have strong text-image models trained on billions of image-text pairs, and you only need to learn motion from (unlabelled) video data.</p> </li> <li> <p>Make-A-Video inserts temporal attention and temporal convolution layers into a pre-trained spatial U-Net. The spatial layers (pre-trained on images) handle appearance, while the new temporal layers (trained on video) handle motion. Spatial self-attention operates within each frame; temporal attention operates across frames at each spatial location. This factorisation is efficient because temporal and spatial patterns are largely separable.</p> </li> <li> <p>The generation pipeline mirrors Imagen's cascade: a base model generates 16 frames at 64x64, then spatial and temporal super-resolution models upscale to the final resolution and frame rate. A frame interpolation network increases temporal smoothness.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#videopoet-and-token-based-video-models","title":"VideoPoet and Token-Based Video Models","text":"<ul> <li> <p>VideoPoet (Kondratyuk et al., 2024) unifies video generation under the language modelling paradigm. All modalities \u2014 text, image, video, audio \u2014 are tokenised into discrete sequences, and a single large language model (LLM) is trained to predict tokens autoregressively across all modalities. This enables zero-shot capabilities: text-to-video, image-to-video, video-to-audio, video editing, and inpainting all emerge from the same model.</p> </li> <li> <p>VideoPoet tokenises video with a MAGVIT-v2 encoder (a 3D VQ-VAE, from file 03) that compresses spatial and temporal dimensions jointly. Audio is tokenised with SoundStream. The LLM backbone is pre-trained on text and fine-tuned on multimodal token sequences, learning the joint distribution across modalities.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#sora-style-temporal-diffusion","title":"Sora-Style Temporal Diffusion","text":"<ul> <li> <p>Sora (OpenAI, 2024) brought temporal diffusion to mainstream attention with its ability to generate long, coherent, physically plausible videos. While full architectural details are not published, the key ideas involve scaling DiT to spacetime: video frames are decomposed into spacetime patches (3D chunks across height, width, and time), which are treated as tokens for a large transformer.</p> </li> <li> <p>The spacetime patch approach means the model processes video as a native 3D signal rather than a sequence of 2D frames. This allows it to capture long-range temporal dependencies \u2014 the model can \"plan ahead\" across the entire video duration rather than generating frame by frame.</p> </li> <li> <p>Sora can handle variable durations, resolutions, and aspect ratios by adjusting the number of spacetime patches. Training on data at its native resolution (rather than cropping everything to squares) improves composition and framing quality.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#wan-open-source-video-generation","title":"Wan: Open-Source Video Generation","text":"<ul> <li> <p>Wan (Wan et al., 2025) is a family of open-source video generation models (1.3B and 14B parameters) built on a DiT backbone with 3D VAE temporal compression. Wan uses flow matching rather than traditional DDPM-style diffusion, learning straight transport paths from noise to video latents. The 3D VAE compresses video spatially and temporally (4x temporal compression), and the DiT processes the resulting spacetime latent tokens with full 3D attention.</p> </li> <li> <p>Wan supports text-to-video, image-to-video (animating a still image), and video editing. The 14B model generates coherent videos up to 5 seconds at 720p resolution, demonstrating that open-source models can approach the quality of proprietary systems when architectural and training recipe choices are carefully made.</p> </li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#text-to-audio-generation","title":"Text-to-Audio Generation","text":"<ul> <li>Picture a film composer reading a screenplay and scoring the soundtrack. Text-to-audio models do something analogous: given a text description (\"a thunderstorm with heavy rain and distant thunder\"), they generate the corresponding audio waveform. The challenge is bridging the gap between the discrete, symbolic nature of text and the continuous, temporal nature of sound.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#audiolm-language-modelling-for-audio","title":"AudioLM: Language Modelling for Audio","text":"<ul> <li> <p>AudioLM (Borsos et al., 2023) generates audio by predicting discrete audio tokens autoregressively, drawing on the same language modelling paradigm used by DALL-E for images. It uses a hierarchical token structure: semantic tokens (from a self-supervised model like w2v-BERT, recall Chapter 9) capture high-level content (what is being said or played), while acoustic tokens (from SoundStream, a neural audio codec) capture fine-grained acoustic details (how it sounds \u2014 timbre, recording quality).</p> </li> <li> <p>Generation proceeds in two stages. First, a transformer predicts semantic tokens given an optional audio prompt, establishing the high-level content plan. Second, another transformer predicts acoustic tokens conditioned on the semantic tokens, filling in acoustic details. This hierarchy mirrors the text-to-speech pipeline (Chapter 9) \u2014 semantic tokens play the role of phonemes, and acoustic tokens play the role of mel spectrogram frames.</p> </li> <li> <p>AudioLM can generate speech continuation (given 3 seconds of speech, generate the next 10), music continuation, and sound effects, all from a single model trained on audio-only data (no text labels needed for pre-training).</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#musiclm-text-conditioned-music","title":"MusicLM: Text-Conditioned Music","text":"<ul> <li> <p>MusicLM (Agostinelli et al., 2023) extends AudioLM to text-conditioned music generation. It adds a text-audio joint embedding (from MuLan, a CLIP-like model trained on music-text pairs) to condition the generation. The MuLan embedding captures semantic meaning of the text description (\"upbeat jazz with saxophone solo\") and guides the hierarchical token generation.</p> </li> <li> <p>MusicLM generates music at 24 kHz for arbitrary durations, maintaining melodic and rhythmic coherence over minutes-long pieces. It can also condition on a hummed melody (using melody tokens extracted by a pitch tracker) plus a text description, generating a full arrangement that follows the hummed tune in the style described by the text.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#musicgen-efficient-single-stage-generation","title":"MusicGen: Efficient Single-Stage Generation","text":"<ul> <li> <p>MusicGen (Copet et al., 2023) simplifies the multi-stage approach. Instead of separate semantic and acoustic models, MusicGen uses a single autoregressive transformer that directly generates multiple codebook levels from the audio codec. The key innovation is an interleaved codebook pattern: rather than generating all codebook levels for one timestep before moving to the next, MusicGen interleaves tokens across codebooks and timesteps in a pattern that allows parallel decoding of some codebook levels.</p> </li> <li> <p>Conditioning is straightforward: text is encoded by a T5 encoder, and the text embeddings are prepended to the audio token sequence (like a prefix prompt in a language model) or injected via cross-attention. MusicGen also supports melody conditioning: a chromagram (from the spectrogram features discussed in Chapter 9) of a reference melody is encoded and used alongside the text condition.</p> </li> </ul> \\[p(a_1, \\ldots, a_T) = \\prod_{t=1}^{T} \\prod_{k=1}^{K} p(a_{t,k} \\mid a_{&lt;t}, c_{\\text{text}})\\] <ul> <li>where \\(a_{t,k}\\) is the audio token at timestep \\(t\\) and codebook level \\(k\\), and \\(c_{\\text{text}}\\) is the text conditioning. The product over \\(k\\) factorises depending on the codebook pattern \u2014 some levels are predicted in parallel.</li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#image-to-text-generation","title":"Image-to-Text Generation","text":"<ul> <li>Now flip the direction: given an image, generate a natural language description. This is image captioning, and it is a form of conditional text generation where the image is the condition. Think of a museum guide describing a painting \u2014 they must perceive the visual content, understand the relationships between objects, and articulate their observations in fluent language.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#captioning-as-conditional-generation","title":"Captioning as Conditional Generation","text":"<ul> <li>The classic approach uses an encoder-decoder architecture (Chapter 07). A pre-trained CNN or ViT (Chapter 8) encodes the image into a set of feature vectors. A language model decoder generates the caption word by word, attending to the image features at each step:</li> </ul> \\[p(w_1, \\ldots, w_L \\mid I) = \\prod_{l=1}^{L} p(w_l \\mid w_1, \\ldots, w_{l-1}, I)\\] <ul> <li> <p>where \\(w_l\\) are the caption words and \\(I\\) is the image representation. Cross-attention connects the text decoder to the image features, allowing the model to \"look at\" different regions of the image as it generates different words \u2014 attending to the dog region when generating \"dog\" and the park region when generating \"park\".</p> </li> <li> <p>CoCa (Contrastive Captioners, Yu et al., 2022) unified contrastive learning (file 01's CLIP-style objective) with captioning in a single model. The image encoder produces features used both for contrastive alignment with text and for cross-attention in a captioning decoder. This multi-task training gives CoCa strong zero-shot recognition (from contrastive learning) and strong generation (from captioning).</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#modern-vision-language-captioning","title":"Modern Vision-Language Captioning","text":"<ul> <li> <p>Modern approaches often use large multimodal models (file 02) for captioning. Models like LLaVA, Qwen-VL, and GPT-4V treat captioning as a special case of visual question answering \u2014 the \"question\" is implicitly \"describe this image\". The visual encoder (CLIP ViT or SigLIP) produces patch tokens that are projected into the LLM's embedding space, and the LLM generates a free-form description.</p> </li> <li> <p>The advantage of LLM-based captioning over dedicated encoder-decoder models is instruction following: you can ask for different levels of detail (\"describe in one sentence\" vs. \"provide a detailed paragraph\"), focus on specific aspects (\"describe the colours\"), or generate structured output (\"list all objects with their positions\"). This flexibility comes from the LLM's instruction-tuning (Chapter 07).</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#video-audio-co-generation","title":"Video-Audio Co-Generation","text":"<ul> <li>Think of watching a film with the sound off \u2014 the experience is hollow. Visual content and audio are deeply coupled: a bouncing ball has a rhythmic thud, rain produces a patter, and a crowd generates cheers. Video-audio co-generation aims to produce both modalities together, maintaining temporal alignment between what you see and what you hear.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#joint-temporal-modelling","title":"Joint Temporal Modelling","text":"<ul> <li> <p>The core challenge is temporal synchronisation: the audio of a drum hit must coincide exactly with the visual frame showing the drumstick striking the drum. This requires a shared temporal representation that both modalities can reference.</p> </li> <li> <p>One approach is to generate video and audio from a shared latent timeline. Models like CoDi (Composable Diffusion, Tang et al., 2023) use separate diffusion models for each modality but align them through a shared latent space. During training, cross-modal attention layers learn to synchronise visual and audio features at each timestep. During generation, both diffusion processes run simultaneously, conditioning on each other through the shared alignment.</p> </li> <li> <p>VideoPoet (discussed above) takes a more unified approach: since all modalities are tokenised into a single sequence, the LLM naturally learns temporal correspondences between video and audio tokens. A video clip of a barking dog followed by the corresponding audio tokens teaches the model to associate visual barking motion with the sound of barking.</p> </li> <li> <p>Temporal alignment loss functions explicitly enforce synchronisation. One formulation uses contrastive learning at the frame level: the audio segment at time \\(t\\) should be more similar to the video frame at time \\(t\\) than to frames at other times:</p> </li> </ul> \\[\\mathcal{L}_{\\text{sync}} = -\\mathbb{E}_t \\left[\\log \\frac{\\exp(\\text{sim}(v_t, a_t) / \\tau)}{\\sum_{t'} \\exp(\\text{sim}(v_t, a_{t'}) / \\tau)}\\right]\\] <ul> <li>where \\(v_t\\) and \\(a_t\\) are the video and audio representations at time \\(t\\), and \\(\\tau\\) is a temperature parameter. This is structurally identical to the InfoNCE loss from file 01, but applied at the temporal frame level rather than at the clip level.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#instruction-following-generation","title":"Instruction-Following Generation","text":"<ul> <li>Imagine telling an artist \"make the sky more dramatic\" or \"replace the hat with a crown\". Instruction-following generation lets you edit images using natural language commands rather than precise spatial masks or brush strokes.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#instructpix2pix-editing-by-description","title":"InstructPix2Pix: Editing by Description","text":"<ul> <li> <p>InstructPix2Pix (Brooks et al., 2023) trains a conditional diffusion model that takes an input image and a text instruction, then produces the edited image. The clever part is how the training data is created: GPT-3 generates editing instructions (\"make it winter\", \"turn the cat into a dog\") paired with input-output text captions, and a text-to-image model (Stable Diffusion) generates the corresponding image pairs.</p> </li> <li> <p>The model is a modified Stable Diffusion U-Net that receives both the text instruction (via cross-attention) and the input image latent (concatenated channel-wise with the noisy latent). It uses dual classifier-free guidance with two guidance scales \u2014 one for the text instruction (\\(s_T\\)) and one for the input image (\\(s_I\\)):</p> </li> </ul> \\[\\hat{\\epsilon} = \\epsilon_\\theta(x_t, \\varnothing, \\varnothing) + s_I \\cdot (\\epsilon_\\theta(x_t, c_I, \\varnothing) - \\epsilon_\\theta(x_t, \\varnothing, \\varnothing)) + s_T \\cdot (\\epsilon_\\theta(x_t, c_I, c_T) - \\epsilon_\\theta(x_t, c_I, \\varnothing))\\] <ul> <li>where \\(c_I\\) is the input image condition and \\(c_T\\) is the text instruction. The first guidance term controls how much to preserve the input image; the second controls how strongly to follow the instruction. This gives the user a two-dimensional knob: high \\(s_I\\) preserves the original closely, while high \\(s_T\\) makes more dramatic edits.</li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#sdedit-and-noise-based-editing","title":"SDEdit and Noise-Based Editing","text":"<ul> <li> <p>SDEdit (Meng et al., 2022) offers a simpler editing approach that requires no special training. You take the input image, add noise to it (running the forward diffusion process to an intermediate timestep \\(t_0\\)), then denoise with a text prompt describing the desired output. The amount of noise controls the edit strength: low noise preserves the structure (colour changes, style transfer), while high noise allows major restructuring (object replacement, layout changes).</p> </li> <li> <p>The tradeoff is precise: at timestep \\(t_0\\), the noisy image retains \\(\\bar{\\alpha}_{t_0}\\) fraction of the original signal. The denoising process fills in the corrupted details according to the new text prompt. This is mathematically grounded: the diffusion model samples from the posterior \\(p(x_0 \\mid x_{t_0}, c)\\), where \\(x_{t_0}\\) constrains the generation to be \"close to\" the original.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#controlnet-spatial-conditioning","title":"ControlNet: Spatial Conditioning","text":"<ul> <li> <p>ControlNet (Zhang et al., 2023) adds fine-grained spatial control to text-to-image diffusion. A copy of the pre-trained U-Net encoder is trained to accept additional input conditions \u2014 edge maps (Canny edges), depth maps, pose skeletons, segmentation maps \u2014 while the original U-Net weights are frozen. The ControlNet encoder's outputs are added to the frozen U-Net's skip connections via zero convolutions (1x1 convolutions initialised to zero), ensuring training starts from the pre-trained model's behaviour and gradually learns the new condition.</p> </li> <li> <p>This architecture lets you provide a sketch, a depth map, or a human pose as the structural guide, and the text prompt fills in the appearance. The pre-trained weights handle photorealism and text understanding; the ControlNet layers handle spatial fidelity to the condition.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#consistency-and-alignment-metrics","title":"Consistency and Alignment Metrics","text":"<ul> <li>How do you measure whether a generated image is good? \"Good\" has at least two dimensions: quality (does it look like a real image?) and alignment (does it match the text prompt?). Several metrics have been developed to quantify these.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#frechet-inception-distance-fid","title":"Frechet Inception Distance (FID)","text":"<ul> <li> <p>Frechet Inception Distance (FID) (Heusel et al., 2017) measures the distance between the distribution of generated images and real images in the feature space of a pre-trained Inception network. Think of it as comparing the \"fingerprints\" of two image collections rather than comparing individual images.</p> </li> <li> <p>Both the real and generated image sets are passed through Inception-v3, and the activations from the penultimate layer are collected. These activations are modelled as multivariate Gaussians \\(\\mathcal{N}(\\mu_r, \\Sigma_r)\\) and \\(\\mathcal{N}(\\mu_g, \\Sigma_g)\\). The FID is the Frechet distance (Wasserstein-2 distance) between these Gaussians:</p> </li> </ul> \\[\\text{FID} = \\|\\mu_r - \\mu_g\\|^2 + \\text{Tr}\\left(\\Sigma_r + \\Sigma_g - 2(\\Sigma_r \\Sigma_g)^{1/2}\\right)\\] <ul> <li> <p>Lower FID is better. FID = 0 means the distributions are identical. FID captures both quality (if generated images are blurry, their features will differ from real images) and diversity (if the model suffers from mode collapse, \\(\\Sigma_g\\) will be smaller than \\(\\Sigma_r\\)). Typical state-of-the-art values on ImageNet 256x256 are FID &lt; 2.0.</p> </li> <li> <p>FID has known limitations: it assumes Gaussian feature distributions (which is approximate), it requires thousands of samples for stable estimates, and it uses Inception features (which may not capture all perceptually relevant differences).</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#inception-score-is","title":"Inception Score (IS)","text":"<ul> <li>The Inception Score (IS) (Salimans et al., 2016) measures two properties: each generated image should be confidently classifiable (the conditional class distribution \\(p(y \\mid x)\\) should be peaked), and the set of generated images should cover many classes (the marginal \\(p(y) = \\mathbb{E}_x[p(y \\mid x)]\\) should be uniform). IS combines these via the KL divergence:</li> </ul> \\[\\text{IS} = \\exp\\left(\\mathbb{E}_x \\left[D_{\\text{KL}}(p(y \\mid x) \\| p(y))\\right]\\right)\\] <ul> <li>Higher IS is better. The maximum IS equals the number of classes (1000 for ImageNet). IS rewards quality (sharp, recognisable images) and diversity (coverage of classes), but it has significant limitations: it ignores the real data distribution entirely, it cannot detect mode dropping within a class, and it is biased toward ImageNet-like images because it uses Inception's class predictions.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#clipscore-measuring-text-image-alignment","title":"CLIPScore: Measuring Text-Image Alignment","text":"<ul> <li>CLIPScore (Hessel et al., 2021) directly measures how well a generated image matches its text prompt using a pre-trained CLIP model (file 01). The score is simply the cosine similarity between the CLIP image embedding and the CLIP text embedding:</li> </ul> \\[\\text{CLIPScore}(I, T) = \\max(0, \\cos(E_I(I), E_T(T)))\\] <ul> <li> <p>where \\(E_I\\) and \\(E_T\\) are the CLIP image and text encoders. CLIPScore is reference-free \u2014 it does not require ground-truth images, only the text prompt. It correlates well with human judgements of text-image alignment and has become the standard metric for evaluating prompt fidelity in text-to-image models.</p> </li> <li> <p>For comparing against a reference caption, RefCLIPScore incorporates a reference image:</p> </li> </ul> \\[\\text{RefCLIPScore} = \\text{HarmonicMean}(\\text{CLIPScore}(I, T), \\max(0, \\cos(E_I(I), E_I(I_{\\text{ref}}))))\\] <ul> <li>This balances text alignment with visual similarity to a reference.</li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#human-evaluation","title":"Human Evaluation","text":"<ul> <li>Automated metrics are proxies; human judgement remains the gold standard. Common protocols include pairwise comparisons (which of two images better matches the prompt?), Likert scales (rate quality and alignment from 1-5), and Elo ratings (tournament-style ranking across models). The DrawBench and PartiPrompts benchmarks provide standardised prompt sets for systematic human evaluation.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#ethical-considerations","title":"Ethical Considerations","text":"<ul> <li>Cross-modal generation is one of the most ethically consequential areas in AI. The ability to create photorealistic images, videos, and audio from text descriptions raises profound concerns that practitioners must take seriously.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#deepfakes-and-misinformation","title":"Deepfakes and Misinformation","text":"<ul> <li> <p>Deepfakes are generated or manipulated media designed to depict events that never occurred. Text-to-image and text-to-video models can create convincing fake photographs of public figures, fabricated evidence, and misleading news imagery. The danger is not just that fakes exist, but that their existence undermines trust in all media \u2014 if any image could be fake, no image is fully trusted.</p> </li> <li> <p>Detection methods include training classifiers on real vs. generated images, analysing statistical artefacts (GAN-generated images have subtle spectral signatures), and embedding invisible watermarks (Stable Diffusion's invisible watermark, Google's SynthID). However, detection is an arms race: as generators improve, detectors must be constantly updated.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#bias-in-generation","title":"Bias in Generation","text":"<ul> <li> <p>Models trained on internet-scale data inherit and amplify societal biases. Text-to-image models disproportionately generate lighter-skinned faces, associate certain professions with specific genders, and default to Western cultural norms for underspecified prompts. These biases are rooted in the training data distribution and the CLIP/T5 text encoders, which encode biases from their own training corpora.</p> </li> <li> <p>Mitigation strategies include curating more representative training data, applying debiasing techniques to text encoders, using safety classifiers to filter problematic outputs, and enabling user control over demographic attributes. None of these are complete solutions, and ongoing auditing is essential.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#content-filtering-and-safety","title":"Content Filtering and Safety","text":"<ul> <li> <p>Responsible deployment requires multiple layers of protection. Input filtering blocks harmful prompts before generation. Output filtering classifies generated content and rejects harmful material. NSFW classifiers detect sexually explicit, violent, or otherwise harmful content. Stable Diffusion's safety checker, for instance, computes the cosine similarity between the generated image's CLIP embedding and a set of pre-defined harmful concept embeddings, flagging images that exceed a threshold.</p> </li> <li> <p>The open-source nature of many generation models (Stable Diffusion, Wan) creates tension between democratising access and preventing misuse. Once model weights are released, content filtering can be bypassed. This has led to debates about the appropriate level of openness and the responsibilities of model developers.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#intellectual-property-and-consent","title":"Intellectual Property and Consent","text":"<ul> <li>Generative models trained on internet data may reproduce copyrighted styles, trademarks, or likeness of real people without consent. The legal and ethical frameworks are still evolving, but responsible practice includes respecting opt-out mechanisms, acknowledging the creative contributions embedded in training data, and developing technical safeguards against memorisation and regurgitation of training examples.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/04.%20cross-modal%20generation/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<ol> <li> <p>Implement classifier-free guidance for a toy 2D diffusion model. Train a conditional diffusion model on a 2D dataset (e.g., labelled clusters), then sample with different guidance scales to observe the quality-diversity tradeoff. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\n# Toy 2D conditional diffusion with classifier-free guidance\ndef noise_schedule(T):\n    betas = jnp.linspace(1e-4, 0.02, T)\n    alphas = 1.0 - betas\n    return jnp.cumprod(alphas)\n\ndef forward_diffuse(x0, t, alpha_bars, key):\n    noise = jax.random.normal(key, x0.shape)\n    return jnp.sqrt(alpha_bars[t]) * x0 + jnp.sqrt(1 - alpha_bars[t]) * noise, noise\n\n# Generate labelled 2D data: class 0 = ring, class 1 = cluster\nkey = jax.random.PRNGKey(42)\nk1, k2, k3 = jax.random.split(key, 3)\ntheta = jax.random.uniform(k1, (200,)) * 2 * jnp.pi\nring = jnp.stack([jnp.cos(theta), jnp.sin(theta)], axis=1) * 2\nring += jax.random.normal(k2, ring.shape) * 0.1\ncluster = jax.random.normal(k3, (200, 2)) * 0.3\n\ndata = jnp.concatenate([ring, cluster])\nlabels = jnp.concatenate([jnp.zeros(200), jnp.ones(200)])\n\n# Simulate CFG: show how guidance pushes samples toward class-conditional modes\n# Try varying guidance_scale from 0.0 to 5.0 and observe results\nguidance_scales = [0.0, 1.0, 3.0, 7.0]\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nfor ax, s in zip(axes, guidance_scales):\n    ax.scatter(ring[:, 0], ring[:, 1], s=8, alpha=0.4, label='Ring (c=0)')\n    ax.scatter(cluster[:, 0], cluster[:, 1], s=8, alpha=0.4, label='Cluster (c=1)')\n    ax.set_title(f'Guidance scale s={s}')\n    ax.set_xlim(-4, 4); ax.set_ylim(-4, 4)\n    ax.set_aspect('equal'); ax.legend(fontsize=7)\nplt.suptitle('Experiment: vary guidance scale and observe quality vs diversity')\nplt.tight_layout(); plt.show()\n# Exercise: train a small MLP denoiser with class conditioning,\n# then implement the CFG formula to sample with different s values.\n</code></pre></p> </li> <li> <p>Compute FID between two sets of 2D samples using the full Frechet distance formula. Vary the generated distribution and observe how FID changes. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef compute_fid(real, generated):\n    \"\"\"Compute Frechet distance between two 2D sample sets.\"\"\"\n    mu_r, mu_g = jnp.mean(real, axis=0), jnp.mean(generated, axis=0)\n    sigma_r = jnp.cov(real.T)\n    sigma_g = jnp.cov(generated.T)\n    diff = mu_r - mu_g\n    # Matrix square root via eigendecomposition\n    product = sigma_r @ sigma_g\n    eigvals, eigvecs = jnp.linalg.eigh(product)\n    sqrt_product = eigvecs @ jnp.diag(jnp.sqrt(jnp.maximum(eigvals, 0))) @ eigvecs.T\n    fid = jnp.sum(diff ** 2) + jnp.trace(sigma_r + sigma_g - 2 * sqrt_product)\n    return fid\n\nkey = jax.random.PRNGKey(0)\nk1, k2, k3, k4 = jax.random.split(key, 4)\n\n# Real distribution: standard 2D Gaussian\nreal = jax.random.normal(k1, (1000, 2))\n\n# Generated distributions with increasing divergence\nshifts = [0.0, 0.5, 1.0, 2.0, 4.0]\nfig, axes = plt.subplots(1, len(shifts), figsize=(18, 3.5))\nfor ax, shift in zip(axes, shifts):\n    gen = jax.random.normal(k2, (1000, 2)) * (1 + shift * 0.2) + shift\n    fid = compute_fid(real, gen)\n    ax.scatter(real[:, 0], real[:, 1], s=3, alpha=0.3, label='Real')\n    ax.scatter(gen[:, 0], gen[:, 1], s=3, alpha=0.3, label='Generated')\n    ax.set_title(f'Shift={shift}\\nFID={fid:.2f}')\n    ax.set_xlim(-5, 8); ax.set_ylim(-5, 8)\n    ax.set_aspect('equal'); ax.legend(fontsize=7)\nplt.suptitle('FID increases as generated distribution diverges from real')\nplt.tight_layout(); plt.show()\n# Try: change the variance of generated samples without shifting the mean.\n# How does FID respond to a diversity mismatch vs a location mismatch?\n</code></pre></p> </li> <li> <p>Implement CLIPScore computation between text and image embeddings using random projections as a stand-in for CLIP. Observe how cosine similarity behaves as you vary the \"alignment\" between modalities. <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef cosine_similarity(a, b):\n    return jnp.dot(a, b) / (jnp.linalg.norm(a) * jnp.linalg.norm(b))\n\ndef clip_score(img_emb, txt_emb):\n    \"\"\"CLIPScore: clamped cosine similarity.\"\"\"\n    return jnp.maximum(0.0, cosine_similarity(img_emb, txt_emb))\n\nkey = jax.random.PRNGKey(42)\ndim = 512  # CLIP embedding dimension\n\n# Simulate aligned and misaligned pairs\n# Aligned: image and text embeddings share a component\nk1, k2, k3 = jax.random.split(key, 3)\nshared = jax.random.normal(k1, (dim,))\nshared = shared / jnp.linalg.norm(shared)\n\nnoise_levels = jnp.linspace(0, 5, 20)\nscores = []\nfor noise in noise_levels:\n    noise_vec = jax.random.normal(k2, (dim,)) * noise\n    img_emb = shared + noise_vec * 0.3\n    txt_emb = shared + jax.random.normal(k3, (dim,)) * noise * 0.3\n    scores.append(float(clip_score(img_emb, txt_emb)))\n\nplt.figure(figsize=(8, 4))\nplt.plot(noise_levels, scores, 'o-', color='#2c3e50')\nplt.xlabel('Noise level (misalignment)')\nplt.ylabel('CLIPScore')\nplt.title('CLIPScore decreases as text-image alignment degrades')\nplt.grid(True, alpha=0.3)\nplt.tight_layout(); plt.show()\n# Experiment: what happens if you normalise embeddings before adding noise?\n# How does dimensionality affect the score distribution?\n</code></pre></p> </li> </ol>"},{"location":"chapter%2010%3A%20multimodal%20learning/05.%20unified%20multimodal%20architectures/","title":"Unified Multimodal Architectures","text":"<p>Unified multimodal architectures replace separate specialist models with a single system that reads, reasons, and generates across text, images, audio, and video. This file covers any-to-any models (CoDi, NExT-GPT), natively multimodal LLMs (Gemini, GPT-4o), multimodal tokenisation strategies, and the architectural trade-offs of unification.</p>"},{"location":"chapter%2010%3A%20multimodal%20learning/05.%20unified%20multimodal%20architectures/#the-case-for-unification","title":"The Case for Unification","text":"<ul> <li> <p>Imagine a translator who speaks five languages and can switch between them mid-sentence without pausing. Early multimodal systems were more like five separate translators sitting in different rooms, each handling one language and passing notes through a slot in the wall. A unified multimodal architecture is the single polyglot: one model with shared weights that reads, writes, and reasons across text, images, audio, video, and even actions, all within a single forward pass.</p> </li> <li> <p>The motivation is both practical and theoretical. On the practical side, maintaining separate specialist models for every modality pair (text-to-image, image-to-text, audio-to-text, etc.) leads to a combinatorial explosion: \\(k\\) modalities require up to \\(k(k-1)\\) directed pipelines. A unified model collapses all of these into a single system. On the theoretical side, human cognition does not process vision and language in isolated modules; cross-modal binding happens early and deeply, and unification attempts to mirror this.</p> </li> <li> <p>Shared weights encourage transfer across modalities. A transformer that has learned temporal patterns in text (subject before verb, cause before effect) can repurpose those same attention circuits for temporal patterns in video (object appears before it moves) or audio (onset before sustain). This is the multimodal analogue of the transfer learning you saw in Chapter 7 with language model fine-tuning and in Chapter 8 with ImageNet pretraining.</p> </li> <li> <p>Formally, let \\(\\mathcal{M} = \\{m_1, m_2, \\ldots, m_k\\}\\) be a set of modalities. A unified model defines a single parameterised function \\(f_\\theta\\) that maps any subset of input modalities to any subset of output modalities:</p> </li> </ul> \\[f_\\theta : \\mathcal{P}(\\mathcal{M}) \\rightarrow \\mathcal{P}(\\mathcal{M})\\] <ul> <li>where \\(\\mathcal{P}(\\mathcal{M})\\) is the power set (all subsets) of modalities. The key constraint is that \\(\\theta\\) is largely shared; only thin, modality-specific adapter layers differ.</li> </ul> <p></p> <ul> <li>The promise of unification comes with a fundamental tension: modalities are structurally different. Text is a 1D sequence of discrete tokens. Images are 2D grids of continuous pixel values. Audio is a 1D continuous waveform with a very different temporal scale from text. Video adds a time axis to images. Reconciling these disparate structures into a single sequence that a transformer can digest is the central engineering challenge of this field.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/05.%20unified%20multimodal%20architectures/#any-to-any-models","title":"Any-to-Any Models","text":"<ul> <li> <p>Think of a universal remote control that can operate your television, air conditioning, and music system, all through the same interface. Any-to-any models are the AI equivalent: they accept any combination of modalities as input and produce any combination as output.</p> </li> <li> <p>CoDi (Composable Diffusion) achieves any-to-any generation by training modality-specific diffusion models and then aligning their latent spaces through a shared conditioning mechanism. Each modality has its own diffusion process (recall diffusion models from file 04 in this chapter), but the noise prediction networks are conditioned on a joint cross-attention layer that sees embeddings from all input modalities simultaneously. This lets CoDi generate, say, an image and matching audio from a text prompt in a single pass.</p> </li> <li> <p>NExT-GPT takes a different architectural approach. It connects an LLM backbone (the \"brain\") to modality-specific encoders on the input side and modality-specific decoders on the output side via lightweight projection layers. The input encoders (e.g., an image encoder from CLIP, an audio encoder from CLAP) translate each modality into the LLM's embedding space. The LLM reasons over the combined token sequence and emits special \"modality signal tokens\" that route information to the appropriate decoder (e.g., Stable Diffusion for images, AudioLDM for audio). Only the projection layers are trained; the LLM and the specialist encoders/decoders are kept frozen.</p> </li> <li> <p>Gemini (Google DeepMind) is natively multimodal from pretraining. Unlike NExT-GPT's plug-and-play approach, Gemini's transformer is trained from scratch on interleaved sequences of text, image, audio, and video tokens. This means cross-modal attention patterns develop organically during pretraining rather than being bolted on afterwards. The model uses the SentencePiece tokeniser for text and learns a visual tokeniser similar to the VQ approaches discussed in file 03 of this chapter.</p> </li> <li> <p>GPT-4o (\"o\" for \"omni\") represents yet another pattern: an end-to-end model where all modalities share the same transformer and the same next-token prediction objective. Audio input is processed as spectral tokens, images as patch tokens, and text as subword tokens, all fed into a single sequence. The model generates output tokens that are decoded by modality-specific heads. The key innovation is the low latency enabled by removing the cascade of separate ASR, LLM, and TTS models that earlier systems like GPT-4V relied on.</p> </li> </ul> <p></p> <ul> <li> <p>These models sit on a spectrum of integration depth:</p> <ul> <li>Shallow integration (NExT-GPT): frozen specialists connected by trained adapters. Fast to build, limited cross-modal reasoning.</li> <li>Medium integration (CoDi): shared conditioning across modality-specific generators. Better alignment, still modular.</li> <li>Deep integration (Gemini, GPT-4o): single model trained end-to-end on all modalities. Richest cross-modal reasoning, most expensive to train.</li> </ul> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/05.%20unified%20multimodal%20architectures/#modality-specific-encoders-and-decoders-with-a-shared-backbone","title":"Modality-Specific Encoders and Decoders with a Shared Backbone","text":"<ul> <li> <p>Picture a factory with a single assembly line (the shared backbone) but different loading docks for raw materials (encoders) and different shipping departments for finished goods (decoders). Each dock is specialised for its cargo, but once inside the factory, everything moves along the same conveyor belt.</p> </li> <li> <p>The dominant architectural pattern for unified models uses this three-part structure:</p> <ul> <li>Modality encoders \\(E_m\\) that convert raw input from modality \\(m\\) into a sequence of embedding vectors \\(\\mathbf{h}_1^m, \\mathbf{h}_2^m, \\ldots, \\mathbf{h}_{n_m}^m\\), each of dimension \\(d\\).</li> <li>A shared transformer backbone \\(T_\\theta\\) that processes the concatenated or interleaved embeddings from all input modalities using self-attention.</li> <li>Modality decoders \\(D_m\\) that convert the backbone's output embeddings back into the native format of modality \\(m\\) (text tokens, image pixels, audio waveforms).</li> </ul> </li> <li> <p>For text, the encoder is typically an embedding lookup table \\(E_\\text{text}(w) = \\mathbf{W}_e[w]\\) where \\(w\\) is a token index, identical to what you saw in Chapter 7 with transformers. For images, the encoder is often a Vision Transformer (ViT) that splits the image into patches and projects each patch linearly, as covered in Chapter 8. For audio, the encoder computes a mel spectrogram and processes it with either a convolutional frontend or an Audio Spectrogram Transformer (AST), as discussed in Chapter 9.</p> </li> <li> <p>The shared backbone is a standard transformer with self-attention across all modality tokens. Given a concatenated input sequence \\(\\mathbf{H} = [\\mathbf{h}_1^{m_1}, \\ldots, \\mathbf{h}_{n_1}^{m_1}, \\mathbf{h}_1^{m_2}, \\ldots, \\mathbf{h}_{n_2}^{m_2}]\\), the self-attention allows every token to attend to every other token regardless of modality:</p> </li> </ul> \\[\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right)\\mathbf{V}\\] <ul> <li> <p>This is the same attention formula from Chapter 7, but now \\(\\mathbf{Q}\\), \\(\\mathbf{K}\\), and \\(\\mathbf{V}\\) contain tokens from multiple modalities. An image-patch token can attend to a text token, enabling cross-modal reasoning without any separate cross-attention module.</p> </li> <li> <p>Modality embeddings are added to each token so the backbone knows which modality a token comes from. This is analogous to positional embeddings but encodes modality identity instead of sequence position. A learnable vector \\(\\mathbf{e}_m \\in \\mathbb{R}^d\\) is added to every token from modality \\(m\\):</p> </li> </ul> \\[\\tilde{\\mathbf{h}}_i^m = \\mathbf{h}_i^m + \\mathbf{e}_m + \\mathbf{p}_i\\] <ul> <li>where \\(\\mathbf{p}_i\\) is the positional embedding for position \\(i\\).</li> </ul> <p></p>"},{"location":"chapter%2010%3A%20multimodal%20learning/05.%20unified%20multimodal%20architectures/#multimodal-tokenisation","title":"Multimodal Tokenisation","text":"<ul> <li> <p>Imagine you are writing a letter that includes both English text and hand-drawn sketches. You might write a sentence, sketch a diagram, write another sentence referring to the diagram, then paste in a musical score. The letter is a single linear stream that interleaves different \"modalities.\" Multimodal tokenisation does precisely this: it converts text, images, audio, and video into a single flat sequence of tokens that a transformer processes left-to-right.</p> </li> <li> <p>For text, tokenisation is well established: byte-pair encoding (BPE) or SentencePiece produce a vocabulary of subword tokens, as covered in Chapter 7. The challenge is extending this idea to continuous modalities.</p> </li> <li> <p>For images, there are two broad approaches. The discrete approach uses a VQ-VAE or VQ-GAN (detailed in file 03 of this chapter) to map each image to a sequence of codebook indices. If the codebook has \\(|\\mathcal{C}|\\) entries and an image is encoded as \\(n\\) codes, the image becomes \\(n\\) discrete tokens drawn from a vocabulary of size \\(|\\mathcal{C}|\\), directly compatible with a text vocabulary. The continuous approach uses a ViT or CNN encoder to produce \\(n\\) continuous embedding vectors, which are linearly projected into the transformer's embedding dimension. Gemini and GPT-4o use variants of the continuous approach; autoregressive image generators like Parti and LlamaGen prefer the discrete route.</p> </li> <li> <p>For audio, the signal is typically converted to a mel spectrogram and then either discretised with a neural audio codec (e.g., EnCodec, SoundStream, which produce hierarchical discrete tokens) or projected continuously via a learned encoder. AudioLM, for example, represents audio as a sequence of discrete tokens from multiple codebook levels, then models them autoregressively.</p> </li> <li> <p>For video, tokenisation builds on image tokenisation but must also compress the temporal dimension. A common strategy uses a 3D VQ-VAE (as in VideoGPT or Cosmos Tokeniser from file 03) that quantises spatiotemporal patches into discrete tokens. The temporal compression factor is crucial: raw video at 24 fps produces far too many tokens per second without aggressive temporal downsampling.</p> </li> <li> <p>Once all modalities are tokenised, they are interleaved into a single sequence with special delimiter tokens marking modality boundaries. A typical format looks like:</p> </li> </ul> <pre><code>[TEXT] The cat sits on a mat [/TEXT] [IMAGE] &lt;img_tok_1&gt; &lt;img_tok_2&gt; ... &lt;img_tok_n&gt; [/IMAGE] [AUDIO] &lt;aud_tok_1&gt; ... &lt;aud_tok_m&gt; [/AUDIO]\n</code></pre> <ul> <li>The transformer then processes this entire mixed sequence using its standard causal (or bidirectional) attention mechanism. The modality delimiter tokens serve double duty: they inform the model about modality boundaries and act as \"pooling points\" whose representations summarise each modality segment.</li> </ul> <p></p> <ul> <li>A critical design choice is the token budget. A single image tokenised at 256 tokens and a text caption of 50 tokens means the image consumes 5x more of the context window. Models must balance resolution (more tokens = more detail) against context length (more tokens = higher memory and compute cost). Techniques like token merging (progressively combining similar tokens) and adaptive tokenisation (using fewer tokens for simple regions and more for complex ones) help manage this trade-off.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/05.%20unified%20multimodal%20architectures/#training-recipes-staged-pretraining-and-joint-fine-tuning","title":"Training Recipes: Staged Pretraining and Joint Fine-Tuning","text":"<ul> <li> <p>You would not teach a child calculus before arithmetic. Similarly, you cannot train a unified multimodal model on all modalities simultaneously from random initialisation and expect it to converge well. The dominant approach is staged training, where the model learns progressively more complex cross-modal capabilities in carefully ordered phases.</p> </li> <li> <p>Stage 1: Unimodal pretraining. Each modality encoder is trained independently on large unimodal datasets. The text backbone is pretrained with a standard language modelling objective (next-token prediction) on trillions of text tokens, exactly as in Chapter 7. The vision encoder is pretrained on image classification or self-supervised objectives (MAE, DINO) as in Chapter 8. The audio encoder is pretrained on speech recognition or audio classification data as in Chapter 9. This stage produces strong unimodal feature extractors.</p> </li> <li> <p>Stage 2: Cross-modal alignment. The pretrained encoders are connected to the shared backbone, and the model is trained on paired multimodal data (image-caption pairs, audio-transcript pairs) with a contrastive or generative objective. During this stage, the encoder weights may be frozen (to preserve unimodal knowledge) while only the projection layers and backbone are updated. This is the stage where CLIP-style alignment (from file 01 in this chapter) gets folded into the unified model.</p> </li> <li> <p>Stage 3: Joint multimodal pretraining. All parameters (or most of them) are unfrozen, and the model is trained on a mixture of unimodal and multimodal data with a single next-token prediction objective across all modality tokens. The loss function is:</p> </li> </ul> \\[\\mathcal{L} = -\\sum_{t=1}^{T} \\log p_\\theta(x_t \\mid x_{&lt;t})\\] <ul> <li> <p>where \\(x_t\\) can be a text token, an image token, or an audio token. The model must learn to predict the next token regardless of modality, which forces it to develop genuine cross-modal understanding.</p> </li> <li> <p>Stage 4: Instruction tuning and alignment. The pretrained model is fine-tuned on curated instruction-following datasets that include multimodal instructions (e.g., \"Describe this image in detail\", \"What sound does this video make?\", \"Generate an image of X\"). This stage often uses reinforcement learning from human feedback (RLHF) or direct preference optimisation (DPO) to align the model's outputs with human preferences.</p> </li> <li> <p>Modality-specific warm-up is a technique used within stages to prevent modality collapse. If one modality (typically text, which has the most training data) dominates the gradient signal, the model may \"forget\" weaker modalities. Warm-up strategies include:</p> <ul> <li>Gradient balancing: scaling gradients from each modality so they contribute equally to the parameter update.</li> <li>Data ratio scheduling: gradually increasing the proportion of multimodal data relative to unimodal data.</li> <li>Loss weighting: assigning modality-specific weights \\(\\lambda_m\\) so the total loss is \\(\\mathcal{L} = \\sum_m \\lambda_m \\mathcal{L}_m\\), with \\(\\lambda_m\\) tuned to balance learning rates across modalities.</li> </ul> </li> </ul> <p></p> <ul> <li>Why not skip stages? Training everything jointly from scratch is tempting but fails in practice for several reasons. First, the model must simultaneously learn low-level features (edge detection, phoneme recognition) and high-level cross-modal reasoning, which have very different learning dynamics. Second, the data distributions across modalities are wildly imbalanced (trillions of text tokens versus billions of image tokens versus hundreds of millions of audio clips). Third, the optimisation landscape is highly non-convex, and staged training provides a curriculum that guides the model towards a better basin, similar to the curriculum learning idea from Chapter 6.</li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/05.%20unified%20multimodal%20architectures/#multimodal-chain-of-thought-reasoning","title":"Multimodal Chain-of-Thought Reasoning","text":"<ul> <li> <p>When you solve a geometry problem, you might sketch a diagram, label the angles, write out an equation, and then solve it step by step. You do not jump directly from the problem statement to the answer. Multimodal chain-of-thought (CoT) reasoning enables models to do the same: generating intermediate reasoning steps that may involve text, visual annotations, or even generated diagrams before arriving at a final answer.</p> </li> <li> <p>In text-only CoT (as explored in Chapter 7's discussion of prompting strategies), the model generates a sequence of reasoning steps in natural language. Multimodal CoT extends this by allowing the intermediate steps to reference or generate visual content. For example, given a chart image and the question \"Which year had the highest sales?\", a multimodal CoT model might first describe the chart (\"The chart shows sales from 2018 to 2023...\"), then identify the relevant visual features (\"The tallest bar appears at 2021...\"), and finally output the answer (\"2021\").</p> </li> <li> <p>Formally, let \\(\\mathbf{x}\\) be a multimodal input and \\(y\\) be the target answer. Standard prediction models \\(p(y \\mid \\mathbf{x})\\) directly. Chain-of-thought introduces intermediate reasoning \\(\\mathbf{r} = (r_1, r_2, \\ldots, r_L)\\) and factorises the prediction as:</p> </li> </ul> \\[p(y \\mid \\mathbf{x}) = \\sum_{\\mathbf{r}} p(y \\mid \\mathbf{r}, \\mathbf{x}) \\cdot p(\\mathbf{r} \\mid \\mathbf{x})\\] <ul> <li> <p>In practice, the sum is approximated by greedy or beam-search decoding over reasoning chains. The reasoning steps \\(r_i\\) can be text tokens, references to image regions, or even generated visual tokens (e.g., a bounding box annotation overlaid on the input image).</p> </li> <li> <p>Training multimodal CoT typically involves curating datasets where human annotators provide step-by-step multimodal reasoning traces, then fine-tuning the model on these traces. Some approaches distill CoT capabilities from larger teacher models: the teacher generates reasoning traces for a large dataset, and the smaller student model is trained on both the inputs and the teacher's traces.</p> </li> <li> <p>Multimodal CoT is especially powerful for tasks that require spatial reasoning (e.g., \"Is the red ball to the left of the blue cube?\"), mathematical reasoning over diagrams (e.g., geometry problems), and multi-step visual question answering where the answer depends on combining information from multiple regions of an image.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/05.%20unified%20multimodal%20architectures/#multimodal-agents","title":"Multimodal Agents","text":"<ul> <li> <p>Think of a robot chef in a kitchen. It looks at the ingredients on the counter (vision), reads the recipe on a tablet (text), listens for the timer beeping (audio), and then physically picks up a knife and chops an onion (action). A multimodal agent is the digital version of this: a model that perceives the world through multiple modalities, reasons about what to do, and takes actions grounded in its perception.</p> </li> <li> <p>The agent loop follows the classic observe-reason-act cycle:</p> <ol> <li>Observe: The agent receives multimodal input from its environment (a screenshot, a user's spoken instruction, a video feed).</li> <li>Reason: The unified model processes the multimodal input, possibly using chain-of-thought to plan a sequence of steps.</li> <li>Act: The model outputs an action (a text response, a tool call, a mouse click at coordinates \\((x, y)\\), a robotic motor command).</li> </ol> </li> <li> <p>Tool use is a key capability of multimodal agents. The model is trained to recognise when it cannot answer a question directly and must instead invoke an external tool: a calculator, a code interpreter, a web browser, or a search engine. The model generates a structured tool call (e.g., <code>search(\"current weather in London\")</code>) as part of its output token sequence, the system executes the call, and the result is fed back as additional input tokens for the model to process.</p> </li> <li> <p>Visual grounding connects language to specific regions in an image or video. When an agent says \"click the blue button in the top-right corner,\" it must ground the phrase \"blue button in the top-right corner\" to pixel coordinates. Architecturally, this is achieved by training the model to output bounding box coordinates as special tokens or by having the model produce a heatmap over the image that indicates the referred region. This extends the grounding and referring work discussed in file 02 of this chapter (Vision Language Models) to the action domain.</p> </li> <li> <p>Web agents like WebVoyager and SeeAct demonstrate multimodal agents navigating websites. The agent receives a screenshot of a web page, identifies interactive elements (buttons, text fields, links), and outputs actions (click, type, scroll) to accomplish a user-specified goal. The key challenge is the enormous action space: a typical web page has hundreds of possible click targets.</p> </li> </ul> <p></p> <ul> <li> <p>Embodied agents extend this to physical environments. A robot with a camera and microphone receives visual and audio input, processes it through a unified model, and outputs motor commands. Projects like PaLM-E (Google) embed robotic sensor data directly into the token sequence of a language model, allowing the robot to follow instructions like \"pick up the green block near the bowl\" by grounding the instruction in its visual observation and generating a sequence of motor actions.</p> </li> <li> <p>The training recipe for agents adds a reinforcement learning (RL) stage on top of the standard staged pretraining. The agent interacts with an environment (a simulated desktop, a web browser, a robotic simulator), receives rewards for task completion, and updates its policy using algorithms like PPO or REINFORCE. The reward signal is typically sparse (1 for task success, 0 otherwise), making this optimisation challenging and heavily reliant on the strong priors from multimodal pretraining.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/05.%20unified%20multimodal%20architectures/#benchmarks-and-evaluation","title":"Benchmarks and Evaluation","text":"<ul> <li> <p>Evaluating a model that can see, hear, read, and act requires a diverse suite of benchmarks. No single metric captures multimodal competence, so the field relies on a collection of specialised evaluations.</p> </li> <li> <p>MMLU (Massive Multitask Language Understanding) tests knowledge across 57 academic subjects. While originally text-only, it serves as a baseline: a unified multimodal model should not lose text-only performance when it gains visual capabilities. A drop in MMLU after multimodal training signals catastrophic forgetting.</p> </li> <li> <p>MMBench evaluates vision-language understanding across 20 fine-grained ability dimensions, including attribute recognition, spatial relationship understanding, and OCR. Each question presents an image and a multiple-choice question. The benchmark systematically tests whether the model truly understands the image or is relying on text-only shortcuts.</p> </li> <li> <p>SEED-Bench provides 19,000 multiple-choice questions spanning 12 evaluation dimensions for both image and video understanding. It specifically tests temporal understanding (what happened before/after a given frame) and compositional reasoning (combining multiple visual attributes).</p> </li> <li> <p>MM-Vet evaluates integrated multimodal capabilities by requiring models to use multiple skills simultaneously: recognition, OCR, spatial awareness, language generation, and knowledge retrieval, all in a single question.</p> </li> <li> <p>MathVista tests mathematical reasoning over visual inputs: geometry diagrams, statistical charts, function plots, and scientific figures. This benchmark specifically targets multimodal chain-of-thought capabilities.</p> </li> <li> <p>Audio-visual benchmarks like AVQA (Audio-Visual Question Answering) test whether models can reason about the relationship between what they see and what they hear. For example: \"Is the person speaking the one on the left or the right?\"</p> </li> <li> <p>Agent benchmarks like WebArena, OSWorld, and SWE-bench evaluate task completion in interactive environments. The metric is typically the success rate: what fraction of tasks does the agent complete correctly? These benchmarks are particularly challenging because they require long-horizon planning and error recovery.</p> </li> <li> <p>Holistic evaluation frameworks like LMSYS Chatbot Arena use human preference judgements in a head-to-head format. Two models are shown the same multimodal input, and a human judge selects which response is better. Elo ratings are computed from thousands of such comparisons, providing a single scalar that correlates well with overall model quality.</p> </li> <li> <p>A persistent challenge in multimodal evaluation is data contamination: because these models are trained on internet-scale data, benchmark images and questions may appear in the training set. Careful deduplication and the creation of held-out test sets are essential but imperfect safeguards.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/05.%20unified%20multimodal%20architectures/#world-models","title":"World Models","text":"<ul> <li> <p>Imagine closing your eyes and visualising what will happen if you push a glass off the edge of a table. You \"see\" it fall, \"hear\" the shatter, and \"feel\" that it would be a bad idea. Your brain is running a world model: an internal simulation of the physical and causal structure of the environment that can predict future states across multiple modalities.</p> </li> <li> <p>In the AI context, a world model is a learned function that predicts the next state of the world given the current state and an action:</p> </li> </ul> \\[\\hat{s}_{t+1} = g_\\phi(s_t, a_t)\\] <ul> <li> <p>where \\(s_t\\) is the current state representation (which may include visual, auditory, and proprioceptive information), \\(a_t\\) is an action, and \\(\\hat{s}_{t+1}\\) is the predicted next state. The state \\(s_t\\) lives in a learned latent space rather than raw pixel space, making the prediction problem tractable.</p> </li> <li> <p>Video prediction models like Sora (OpenAI) and Genie (Google DeepMind) represent a major step towards world models. They learn to generate temporally coherent video frames conditioned on text prompts and/or action sequences. While they are often discussed as video generators, the underlying capability is closer to world simulation: the model has internalised enough physics (gravity, collision, occlusion, fluid dynamics) to render plausible futures.</p> </li> <li> <p>The connection to multimodal architectures is deep. A world model that predicts only pixels is limited; a truly useful world model predicts across modalities. If you push the glass, the world model should predict the visual trajectory (glass falls), the auditory event (glass shatters), and the semantic consequence (you now have broken glass on the floor). Unified multimodal architectures are natural candidates for world models because they already represent all modalities in a shared space.</p> </li> <li> <p>Formally, a multimodal world model optimises:</p> </li> </ul> \\[\\mathcal{L}_\\text{world} = \\mathbb{E}\\left[\\sum_{m \\in \\mathcal{M}} \\lambda_m \\| s_{t+1}^m - g_\\phi^m(s_t, a_t) \\|^2 \\right]\\] <ul> <li>where \\(s_{t+1}^m\\) is the ground-truth next-state representation in modality \\(m\\) and \\(g_\\phi^m\\) is the modality-specific prediction head of the world model. The shared latent dynamics \\(g_\\phi\\) operate in the joint multimodal space, while modality-specific heads decode predictions into each modality's native format.</li> </ul> <p></p> <ul> <li>JEPA (Joint Embedding Predictive Architecture), proposed by Yann LeCun, offers a framework for world models that avoids the pitfalls of pixel-level prediction. Instead of predicting raw pixels (which wastes capacity on irrelevant details like exact textures), JEPA predicts in embedding space. The model learns an encoder that maps observations to embeddings and a predictor that forecasts future embeddings:</li> </ul> \\[\\hat{\\mathbf{z}}_{t+1} = h_\\psi(\\mathbf{z}_t, a_t), \\quad \\mathbf{z}_t = \\text{Enc}(s_t)\\] <ul> <li> <p>The loss compares embeddings rather than raw observations, which is more robust to perceptual aliasing (many different pixel configurations may represent the same semantic state). This approach is especially promising for multimodal world models because it naturally operates in the shared embedding space that unified architectures already provide.</p> </li> <li> <p>World models have practical applications beyond academic interest. In model-based reinforcement learning, the agent uses its world model to \"imagine\" the consequences of actions before taking them, dramatically reducing the number of real-world interactions needed (recall the discussion of model-based RL from Chapter 11). In autonomous driving, a world model predicts how the scene will evolve over the next few seconds given different steering decisions. In robotics, a world model allows a robot to mentally rehearse a manipulation sequence before executing it.</p> </li> <li> <p>The frontier of world model research is moving towards interactive world models that run in real-time and respond to arbitrary user actions, essentially becoming general-purpose simulators learned entirely from data. Genie 2 (Google DeepMind) demonstrates this for 3D environments: given a single image, it generates an interactive, controllable 3D world that a user can explore. The convergence of world models and unified multimodal architectures suggests a future where a single model can perceive, predict, simulate, and act across all modalities.</p> </li> </ul>"},{"location":"chapter%2010%3A%20multimodal%20learning/05.%20unified%20multimodal%20architectures/#coding-tasks-use-colab-or-notebook","title":"Coding Tasks (use CoLab or notebook)","text":"<p>Task 1: Build a minimal multimodal token interleaver</p> <ul> <li>Write a function that takes a text string and a dummy \"image\" (a small 2D array) and interleaves their tokenised representations into a single flat sequence with modality embeddings.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\n\n# Simulate multimodal tokenisation: text tokens + \"image patch\" tokens\ndef interleave_modalities(text_tokens, image_patches, embed_dim=32, key=jax.random.PRNGKey(0)):\n    \"\"\"Interleave text and image tokens with learned modality embeddings.\"\"\"\n    k1, k2, k3 = jax.random.split(key, 3)\n    n_text = text_tokens.shape[0]\n    n_img = image_patches.shape[0]\n    # Random projection matrices (stand-ins for real encoders)\n    W_text = jax.random.normal(k1, (text_tokens.shape[-1], embed_dim)) * 0.02\n    W_img = jax.random.normal(k2, (image_patches.shape[-1], embed_dim)) * 0.02\n    # Modality embeddings: one for text, one for image\n    mod_emb = jax.random.normal(k3, (2, embed_dim)) * 0.02\n    text_embs = text_tokens @ W_text + mod_emb[0]  # (n_text, embed_dim)\n    img_embs = image_patches @ W_img + mod_emb[1]   # (n_img, embed_dim)\n    # Interleave: [IMG] tokens first, then [TEXT] tokens (like LLaVA)\n    combined = jnp.concatenate([img_embs, text_embs], axis=0)\n    print(f\"Combined sequence: {n_img} image + {n_text} text = {combined.shape[0]} tokens\")\n    return combined\n\n# Try it: 5 text tokens (dim 16) and 4 image patches (dim 64)\ntext = jax.random.normal(jax.random.PRNGKey(1), (5, 16))\nimage = jax.random.normal(jax.random.PRNGKey(2), (4, 64))\nseq = interleave_modalities(text, image)\n# Experiment: change embed_dim, swap the interleaving order, add a third modality\n</code></pre> <p>Task 2: Visualise cross-modal attention patterns</p> <ul> <li>Create a synthetic multimodal sequence and compute self-attention scores to see how image tokens attend to text tokens and vice versa.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef cross_modal_attention(n_text=6, n_img=4, d=32, key=jax.random.PRNGKey(42)):\n    \"\"\"Compute and visualise attention between text and image tokens.\"\"\"\n    k1, k2, k3 = jax.random.split(key, 3)\n    # Simulate token embeddings for two modalities\n    text_embs = jax.random.normal(k1, (n_text, d))\n    img_embs = jax.random.normal(k2, (n_img, d))\n    seq = jnp.concatenate([img_embs, text_embs], axis=0)  # (n_img+n_text, d)\n    # Learned Q, K projections\n    Wq = jax.random.normal(k3, (d, d)) * 0.1\n    Wk = jax.random.normal(jax.random.PRNGKey(99), (d, d)) * 0.1\n    Q, K = seq @ Wq, seq @ Wk\n    scores = Q @ K.T / jnp.sqrt(d)\n    attn = jax.nn.softmax(scores, axis=-1)\n    # Plot\n    labels = [f\"img_{i}\" for i in range(n_img)] + [f\"txt_{i}\" for i in range(n_text)]\n    fig, ax = plt.subplots(figsize=(7, 6))\n    ax.imshow(attn, cmap=\"viridis\")\n    ax.set_xticks(range(len(labels))); ax.set_xticklabels(labels, rotation=45, fontsize=8)\n    ax.set_yticks(range(len(labels))); ax.set_yticklabels(labels, fontsize=8)\n    ax.set_xlabel(\"Key (attended to)\"); ax.set_ylabel(\"Query (attending from)\")\n    ax.set_title(\"Cross-modal self-attention map\")\n    plt.colorbar(ax.images[0], ax=ax, shrink=0.8)\n    plt.tight_layout(); plt.show()\n\ncross_modal_attention()\n# Experiment: increase d, add a causal mask, observe how attention patterns change\n</code></pre> <p>Task 3: Simulate staged training with modality-specific loss weighting</p> <ul> <li>Demonstrate how modality-specific loss weights affect a toy multimodal training loop. Observe how balancing losses prevents one modality from dominating.</li> </ul> <pre><code>import jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\n\ndef staged_training_sim(steps=200, key=jax.random.PRNGKey(7)):\n    \"\"\"Simulate multimodal training with adjustable modality loss weights.\"\"\"\n    # Two 'modalities' with different loss scales (text loss ~10x larger than image loss)\n    losses_text, losses_img = [], []\n    param = jnp.array([0.0, 0.0])  # Shared param updated by both modality losses\n    lr = 0.05\n    # Try changing these weights to see the effect on convergence balance\n    lambda_text, lambda_img = 1.0, 5.0  # upweight the weaker modality\n\n    for step in range(steps):\n        k1, k2, key = jax.random.split(key, 3)\n        noise_t = jax.random.normal(k1, ()) * 0.3\n        noise_i = jax.random.normal(k2, ()) * 0.1\n        loss_t = (param[0] - 3.0) ** 2 + noise_t  # text target = 3.0\n        loss_i = 0.1 * (param[1] - 1.0) ** 2 + noise_i  # image target = 1.0 (smaller scale)\n        # Weighted combined gradient\n        grad_t = lambda_text * 2 * (param[0] - 3.0)\n        grad_i = lambda_img * 0.2 * (param[1] - 1.0)\n        param = param - lr * jnp.array([grad_t, grad_i])\n        losses_text.append(float(loss_t)); losses_img.append(float(loss_i))\n\n    fig, ax = plt.subplots(figsize=(8, 4))\n    ax.plot(losses_text, label=f\"Text loss (weight={lambda_text})\", alpha=0.7)\n    ax.plot(losses_img, label=f\"Image loss (weight={lambda_img})\", alpha=0.7)\n    ax.set_xlabel(\"Training step\"); ax.set_ylabel(\"Loss\"); ax.legend()\n    ax.set_title(\"Modality loss balancing during staged training\")\n    plt.tight_layout(); plt.show()\n\nstaged_training_sim()\n# Experiment: set lambda_img=1.0 and watch image loss converge much slower\n</code></pre>"},{"location":"chapter%2011%3A%20autonomous%20systems/01.%20perception/","title":"Perception","text":"<ul> <li>Sensor modalities: cameras (mono, stereo, fisheye), LiDAR (spinning, solid-state), radar, ultrasonic, IMU</li> <li>Sensor fusion: early fusion (raw data), late fusion (decision level), multi-sensor calibration</li> <li>3D object detection: PointPillars, CenterPoint, BEVFusion, bird's-eye-view representations</li> <li>Depth estimation: stereo matching, monocular depth networks, LiDAR-camera projection</li> <li>Occupancy networks: 3D occupancy prediction, voxel representations</li> <li>Lane detection and road topology: curve fitting, polynomial models, graph-based topology</li> <li>Semantic mapping: building environmental representations from sensor streams</li> </ul>"},{"location":"chapter%2011%3A%20autonomous%20systems/02.%20robot%20learning/","title":"Robot Learning","text":"<ul> <li>Robot kinematics: forward and inverse kinematics, DH parameters, joint spaces</li> <li>Dynamics and control: PID control, model predictive control (MPC), impedance control</li> <li>Imitation learning: behavioural cloning, DAgger, learning from demonstrations</li> <li>Sim-to-real transfer: domain randomisation, system identification, reality gap</li> <li>Reward shaping and curriculum learning for robotics</li> <li>Manipulation: grasping (analytical, data-driven), dexterous manipulation, contact-rich tasks</li> <li>Locomotion: legged robots, quadrupeds, humanoid balance, CPG-based control</li> <li>Safety: safe exploration, constrained RL, risk-aware planning</li> </ul>"},{"location":"chapter%2011%3A%20autonomous%20systems/03.%20vision-language-action%20models/","title":"Vision-Language-Action Models","text":"<ul> <li>From vision-language to action: grounding language instructions in physical actions</li> <li>VLAs: architecture (vision encoder + LLM + action head), RT-2, Octo, OpenVLA</li> <li>Action tokenisation: discretising continuous actions, action chunking</li> <li>Pretraining recipes: web-scale vision-language data \u2192 robot manipulation data</li> <li>Generalisation: unseen objects, environments, instructions</li> <li>Co-training with internet data and robot data</li> <li>Embodiment-agnostic models: one model for multiple robot form factors</li> <li>Benchmarks: SIMPLER, real-world evaluation protocols</li> </ul>"},{"location":"chapter%2011%3A%20autonomous%20systems/04.%20self-driving/","title":"Self-Driving Cars","text":"<ul> <li>Autonomous driving stack: perception \u2192 prediction \u2192 planning \u2192 control</li> <li>HD maps vs mapless driving: pre-built maps, online map construction</li> <li>Motion prediction: trajectory forecasting, social forces, graph neural networks for agent interaction</li> <li>Planning: rule-based planners, optimisation-based (trajectory optimisation), learning-based (neural planners)</li> <li>End-to-end driving: UniAD, from sensor inputs directly to control outputs</li> <li>Simulation: CARLA, nuPlan, closed-loop vs open-loop evaluation</li> <li>Safety: functional safety (ISO 26262), SOTIF, operational design domain (ODD)</li> <li>Levels of autonomy: SAE L1\u2013L5, current industry state</li> </ul>"},{"location":"chapter%2011%3A%20autonomous%20systems/05.%20space%20and%20extreme%20robotics/","title":"Space and Extreme Robotics","text":"<ul> <li>Space robotics: orbital servicing, planetary rovers (Mars rover autonomy), satellite inspection</li> <li>Communication constraints: high latency, limited bandwidth, onboard autonomy</li> <li>Radiation-hardened computing: constraints on hardware, model compression for space</li> <li>Autonomous navigation in unstructured terrain: visual-inertial odometry, hazard avoidance</li> <li>Underwater robotics: AUVs, ROVs, acoustic communication, SLAM in low-visibility</li> <li>Search and rescue robotics: disaster response, multi-robot coordination</li> <li>Swarm robotics: decentralised control, emergent behaviour, consensus algorithms</li> <li>Human-robot interaction: shared autonomy, teleoperation, trust calibration</li> </ul>"},{"location":"chapter%2012%3A%20computing%20and%20OS/01.%20discrete%20maths/","title":"Discrete Maths","text":"<ul> <li>Logic: propositional logic, truth tables, logical equivalences, predicate logic, quantifiers</li> <li>Proofs: direct proof, proof by contradiction, proof by induction, pigeonhole principle</li> <li>Sets: operations (union, intersection, complement, Cartesian product), power sets, cardinality</li> <li>Relations: equivalence relations, partial orders, total orders</li> <li>Functions: injective, surjective, bijective, composition, inverse</li> <li>Combinatorics: permutations, combinations, binomial theorem, inclusion-exclusion</li> <li>Graph theory: vertices, edges, paths, cycles, trees, planarity, colouring, Euler and Hamiltonian paths</li> <li>Recurrence relations and generating functions</li> </ul>"},{"location":"chapter%2012%3A%20computing%20and%20OS/02.%20computer%20architecture/","title":"Computer Architecture","text":"<ul> <li>Number systems: binary, hexadecimal, two's complement, IEEE 754 floating point</li> <li>Logic gates: AND, OR, NOT, NAND, XOR, multiplexers, adders</li> <li>CPU architecture: ALU, registers, program counter, instruction cycle (fetch-decode-execute)</li> <li>Instruction set architectures: CISC vs RISC, x86, ARM, RISC-V</li> <li>Pipelining: stages, hazards (data, control, structural), forwarding, branch prediction</li> <li>Memory hierarchy: registers \u2192 L1/L2/L3 cache \u2192 RAM \u2192 disk, cache associativity, cache coherence</li> <li>Virtual memory: page tables, TLB, page faults, address translation</li> <li>Bus architecture, I/O, interrupts, DMA</li> </ul>"},{"location":"chapter%2012%3A%20computing%20and%20OS/03.%20operating%20systems/","title":"Operating Systems","text":"<ul> <li>What an OS does: abstraction, resource management, isolation</li> <li>Processes: creation (fork/exec), process states, PCB, context switching</li> <li>Threads: kernel threads vs user threads, pthreads, thread pools</li> <li>Scheduling: FCFS, SJF, round robin, priority scheduling, multilevel feedback queues, CFS (Linux)</li> <li>Memory management: paging, segmentation, demand paging, page replacement (LRU, clock)</li> <li>File systems: inodes, FAT, ext4, journaling, B-tree based file systems</li> <li>I/O subsystem: buffering, spooling, device drivers</li> <li>System calls, user mode vs kernel mode, interrupts and traps</li> </ul>"},{"location":"chapter%2012%3A%20computing%20and%20OS/04.%20concurrency%20and%20parallelism/","title":"Concurrency and Parallelism","text":"<ul> <li>Concurrency vs parallelism: interleaving vs simultaneous execution</li> <li>Synchronisation primitives: mutexes, semaphores, condition variables, monitors</li> <li>Classic problems: producer-consumer, readers-writers, dining philosophers</li> <li>Deadlock: conditions (mutual exclusion, hold-and-wait, no preemption, circular wait), detection, prevention, avoidance (banker's algorithm)</li> <li>Lock-free and wait-free data structures: CAS operations, atomic variables</li> <li>Parallel programming models: shared memory (OpenMP), message passing (MPI)</li> <li>Async and event-driven: event loops, coroutines, async/await</li> <li>Amdahl's law, Gustafson's law, scalability limits</li> </ul>"},{"location":"chapter%2012%3A%20computing%20and%20OS/05.%20programming%20languages/","title":"Programming Languages","text":"<ul> <li>Language paradigms: imperative, object-oriented, functional, logic</li> <li>Type systems: static vs dynamic, strong vs weak, type inference, generics</li> <li>Memory management: stack vs heap, manual (C/C++), garbage collection (tracing, reference counting), ownership (Rust borrow checker)</li> <li>Compilation: lexing, parsing (ASTs), semantic analysis, code generation, LLVM</li> <li>Interpretation: bytecode VMs (JVM, CPython), JIT compilation</li> <li>Key language features: closures, pattern matching, algebraic data types, traits/interfaces</li> <li>Domain-specific languages: SQL, regex, shader languages</li> <li>Language design tradeoffs: performance vs safety vs expressiveness</li> </ul>"},{"location":"chapter%2013%3A%20data%20structures%20and%20algorithms/01.%20arrays%20and%20hashing/","title":"Arrays and Hashing","text":"<ul> <li>Arrays: contiguous memory, indexing, dynamic arrays (amortised doubling), cache locality</li> <li>Strings: encoding (ASCII, UTF-8), string matching (KMP, Rabin-Karp, Boyer-Moore)</li> <li>Hash tables: hash functions, collision resolution (chaining, open addressing, linear probing, robin hood hashing)</li> <li>Hash maps and hash sets: average vs worst-case complexity, load factor, rehashing</li> <li>Bloom filters: probabilistic membership, false positive rate, applications</li> <li>Two pointers technique, sliding window</li> <li>Prefix sums and difference arrays</li> </ul>"},{"location":"chapter%2013%3A%20data%20structures%20and%20algorithms/02.%20linked%20lists%2C%20stacks%2C%20and%20queues/","title":"Linked Lists, Stacks, and Queues","text":"<ul> <li>Singly linked lists: insertion, deletion, traversal, reversal</li> <li>Doubly linked lists: bidirectional traversal, sentinel nodes</li> <li>Circular linked lists</li> <li>Skip lists: probabilistic balancing, expected O(log n) search</li> <li>Stacks: LIFO, array-based and linked-list-based implementations</li> <li>Applications of stacks: function call stack, expression evaluation, parenthesis matching, monotonic stack</li> <li>Queues: FIFO, circular buffer, deque (double-ended queue)</li> <li>Priority queues and binary heaps: insert, extract-min, heapify, heap sort</li> </ul>"},{"location":"chapter%2013%3A%20data%20structures%20and%20algorithms/03.%20trees/","title":"Trees","text":"<ul> <li>Binary trees: traversals (inorder, preorder, postorder, level-order), height, depth</li> <li>Binary search trees (BST): search, insert, delete, successor/predecessor</li> <li>Balanced BSTs: AVL trees (rotations), red-black trees (colour invariants)</li> <li>B-trees and B+ trees: disk-friendly, database indexing, order and fill factor</li> <li>Tries: prefix trees, autocomplete, word search</li> <li>Segment trees: range queries, lazy propagation</li> <li>Fenwick trees (Binary Indexed Trees): prefix sums, point updates</li> <li>Union-Find (Disjoint Set Union): path compression, union by rank</li> </ul>"},{"location":"chapter%2013%3A%20data%20structures%20and%20algorithms/04.%20graphs/","title":"Graphs","text":"<ul> <li>Representations: adjacency matrix, adjacency list, edge list, incidence matrix</li> <li>Traversals: BFS (shortest path in unweighted graphs), DFS (cycle detection, topological sort)</li> <li>Shortest paths: Dijkstra (non-negative weights), Bellman-Ford (negative weights, cycle detection), Floyd-Warshall (all pairs)</li> <li>Minimum spanning trees: Kruskal (union-find), Prim (priority queue)</li> <li>Topological sort: Kahn's algorithm (BFS), DFS-based</li> <li>Strongly connected components: Tarjan's algorithm, Kosaraju's algorithm</li> <li>Network flow: Ford-Fulkerson, Edmonds-Karp, max-flow min-cut theorem</li> <li>Bipartite matching: Hungarian algorithm, Hopcroft-Karp</li> </ul>"},{"location":"chapter%2013%3A%20data%20structures%20and%20algorithms/05.%20sorting%20and%20search/","title":"Sorting and Search","text":"<ul> <li>Comparison sorts: bubble sort, insertion sort, merge sort, quicksort (pivot strategies, Hoare vs Lomuto partition), heapsort</li> <li>Lower bound for comparison sorting: O(n log n) via decision trees</li> <li>Non-comparison sorts: counting sort, radix sort, bucket sort</li> <li>Binary search: standard, lower/upper bound, search on answer (monotonic functions)</li> <li>Divide and conquer: master theorem, merge sort analysis, closest pair of points</li> <li>Greedy algorithms: activity selection, Huffman coding, interval scheduling</li> <li>Dynamic programming: overlapping subproblems, optimal substructure, memoisation vs tabulation, classic problems (knapsack, LCS, edit distance, coin change)</li> <li>Backtracking: N-queens, Sudoku, constraint satisfaction</li> </ul>"},{"location":"chapter%2014%3A%20SIMD%20and%20GPU%20programming/01.%20hardware%20fundamentals/","title":"Hardware Fundamentals","text":"<ul> <li>Moore's law and the end of frequency scaling: why parallelism matters</li> <li>CPU architecture recap: superscalar execution, out-of-order execution, branch prediction, speculative execution</li> <li>SIMD concept: single instruction, multiple data, data-level parallelism</li> <li>Vector registers and vector width: 128-bit, 256-bit, 512-bit</li> <li>Memory bandwidth vs compute: roofline model, arithmetic intensity</li> <li>Latency vs throughput: pipelining, instruction-level parallelism</li> <li>Chip families overview: x86 (Intel, AMD), ARM, RISC-V, Apple Silicon</li> <li>Thermal and power constraints: TDP, power efficiency, dark silicon</li> </ul>"},{"location":"chapter%2014%3A%20SIMD%20and%20GPU%20programming/02.%20ARM%20and%20NEON/","title":"ARM and NEON","text":"<ul> <li>ARM architecture: load-store ISA, register file, condition codes, Thumb mode</li> <li>ARM NEON: 128-bit SIMD, data types (int8, int16, float16, float32), register layout</li> <li>NEON intrinsics: load/store (vld1, vst1), arithmetic (vadd, vmul, vmla), shuffle and permute</li> <li>SVE and SVE2: scalable vector extensions, predicate registers, vector-length agnostic programming</li> <li>Apple Silicon specifics: AMX (Apple Matrix eXtensions), performance cores vs efficiency cores</li> <li>Practical examples: vectorised dot product, matrix multiply, image processing kernels</li> <li>Auto-vectorisation: compiler flags, pragmas, loop patterns that help/hinder vectorisation</li> </ul>"},{"location":"chapter%2014%3A%20SIMD%20and%20GPU%20programming/03.%20x86%20and%20AVX/","title":"x86 and AVX","text":"<ul> <li>x86 SIMD evolution: MMX \u2192 SSE \u2192 SSE2/3/4 \u2192 AVX \u2192 AVX2 \u2192 AVX-512 \u2192 AMX</li> <li>AVX/AVX2 programming: 256-bit YMM registers, intrinsics (mm256*), FMA instructions</li> <li>AVX-512: 512-bit ZMM registers, mask registers, gather/scatter, conflict detection</li> <li>Intel AMX: tile registers, TMUL (tile matrix multiply), BF16/INT8 acceleration</li> <li>Memory alignment: aligned vs unaligned loads, cache line considerations</li> <li>Performance pitfalls: AVX frequency throttling, register pressure, lane crossing penalties</li> <li>Benchmarking and profiling: RDTSC, perf, VTune, likwid</li> </ul>"},{"location":"chapter%2014%3A%20SIMD%20and%20GPU%20programming/04.%20GPU%20architecture%20and%20CUDA/","title":"GPU Architecture and CUDA","text":"<ul> <li>GPU vs CPU: throughput-oriented design, thousands of cores, SIMT execution model</li> <li>GPU memory hierarchy: global memory, shared memory, registers, L1/L2 cache, constant memory</li> <li>CUDA programming model: grids, blocks, threads, warps (32 threads), warp divergence</li> <li>Kernel launch: grid/block dimensions, occupancy, register usage</li> <li>Memory access patterns: coalesced access, bank conflicts in shared memory, memory fences</li> <li>Synchronisation: __syncthreads, atomic operations, cooperative groups</li> <li>Streams and concurrency: overlapping compute and data transfer, multi-stream execution</li> <li>Profiling: nsight compute, nsight systems, occupancy calculator</li> <li>NVIDIA GPU generations: Volta (tensor cores), Ampere (TF32, sparsity), Hopper (transformer engine, FP8), Blackwell</li> </ul>"},{"location":"chapter%2014%3A%20SIMD%20and%20GPU%20programming/05.%20triton%2C%20TPUs%2C%20and%20Vulkan/","title":"Triton, TPUs, and Vulkan","text":"<ul> <li>Triton: Python-based GPU kernel programming, block-level abstraction, auto-tuning</li> <li>Writing Triton kernels: tl.load, tl.store, tl.dot, masking, grid/block programs</li> <li>Triton vs CUDA: productivity vs control tradeoff, when to use each</li> <li>Flash Attention as a case study: memory-efficient attention via tiling, online softmax</li> <li>TPU architecture: systolic arrays, MXU (matrix multiply unit), HBM, ICI interconnect</li> <li>TPU programming: XLA compiler, JAX/pjit, GSPMD, sharding annotations</li> <li>Vulkan compute: compute shaders, SPIR-V, descriptor sets, command buffers</li> <li>Comparison: GPU (CUDA/Triton) vs TPU (JAX/XLA) vs Vulkan, choosing the right tool</li> </ul>"},{"location":"chapter%2015%3A%20systems%20design/01.%20systems%20design%20fundamentals/","title":"Systems Design Fundamentals","text":"<ul> <li>Client-server architecture, request-response model</li> <li>Networking basics: TCP/IP, UDP, HTTP/HTTPS, WebSockets, gRPC, protocol buffers</li> <li>DNS: resolution, caching, load balancing via DNS</li> <li>Proxies: forward proxy, reverse proxy (Nginx, HAProxy), API gateways</li> <li>Load balancing: round robin, least connections, consistent hashing, L4 vs L7</li> <li>Caching: cache-aside, write-through, write-back, eviction policies (LRU, LFU, TTL), CDNs</li> <li>Databases: SQL vs NoSQL, ACID, CAP theorem, sharding, replication (leader-follower, multi-leader)</li> <li>Message queues: Kafka, RabbitMQ, pub/sub, event-driven architecture, exactly-once delivery</li> <li>Consistency models: strong, eventual, causal, read-your-writes</li> <li>Rate limiting, circuit breakers, backpressure</li> </ul>"},{"location":"chapter%2015%3A%20systems%20design/02.%20cloud%20computing/","title":"Cloud Computing","text":"<ul> <li>Cloud service models: IaaS, PaaS, SaaS, FaaS (serverless)</li> <li>Major providers overview: AWS, GCP, Azure \u2014 compute, storage, networking primitives</li> <li>Virtualisation: hypervisors (Type 1, Type 2), VMs vs containers</li> <li>Containers: Docker (images, layers, Dockerfile), container registries</li> <li>Orchestration: Kubernetes (pods, services, deployments, StatefulSets, DaemonSets), Helm</li> <li>Storage: block (EBS), object (S3/GCS), file (EFS/NFS), data lakes</li> <li>Networking in cloud: VPCs, subnets, security groups, load balancers, service mesh (Istio, Envoy)</li> <li>Serverless: Lambda/Cloud Functions, cold starts, event triggers</li> <li>Cost management: spot/preemptible instances, reserved capacity, autoscaling policies</li> <li>Infrastructure as code: Terraform, CloudFormation, Pulumi</li> </ul>"},{"location":"chapter%2015%3A%20systems%20design/03.%20large%20scale%20infrastructure/","title":"Large Scale Infrastructure","text":"<ul> <li>Scalability: vertical vs horizontal scaling, stateless services</li> <li>Distributed systems: consensus (Paxos, Raft), leader election, distributed locks</li> <li>Microservices: service decomposition, API contracts, service discovery, saga pattern</li> <li>Data pipelines: batch processing (MapReduce, Spark), stream processing (Flink, Kafka Streams)</li> <li>Database scaling: read replicas, partitioning strategies (range, hash, directory), cross-shard queries</li> <li>Search systems: inverted indices, Elasticsearch, vector search (FAISS, Milvus, Pinecone)</li> <li>Observability: logging (ELK), metrics (Prometheus, Grafana), tracing (Jaeger, OpenTelemetry)</li> <li>Reliability: SLOs, SLIs, SLAs, error budgets, chaos engineering</li> <li>CI/CD: build pipelines, blue-green deployments, canary releases, feature flags</li> </ul>"},{"location":"chapter%2015%3A%20systems%20design/04.%20ML%20systems%20design/","title":"ML Systems Design","text":"<ul> <li>ML system lifecycle: problem framing \u2192 data \u2192 training \u2192 evaluation \u2192 deployment \u2192 monitoring</li> <li>Data management: feature stores, data versioning (DVC), labelling pipelines, data quality checks</li> <li>Training infrastructure: distributed training (data parallel, model parallel), experiment tracking (MLflow, W&amp;B)</li> <li>Model evaluation: offline metrics, A/B testing, shadow deployment, interleaving experiments</li> <li>Model serving: batch vs real-time inference, model registry, model versioning</li> <li>Feature engineering: online vs offline features, feature freshness, feature serving latency</li> <li>ML pipelines: orchestration (Airflow, Kubeflow, Metaflow), reproducibility</li> <li>Monitoring: data drift, concept drift, model degradation, alerting</li> </ul>"},{"location":"chapter%2015%3A%20systems%20design/05.%20ML%20design%20examples/","title":"ML Design Examples","text":"<ul> <li>Recommendation system: candidate generation \u2192 ranking \u2192 re-ranking, collaborative filtering, content-based, embeddings, two-tower model</li> <li>Search ranking: query understanding, retrieval (BM25, dense retrieval), learning to rank (pointwise, pairwise, listwise)</li> <li>Ads click prediction: feature engineering (user, ad, context), real-time bidding, calibration, explore-exploit</li> <li>Fraud detection system: real-time streaming, feature pipelines, imbalanced classification, human-in-the-loop</li> <li>Content moderation: multi-modal classification (text + image), policy-as-code, escalation workflows</li> <li>Conversational AI system: intent detection, dialogue management, retrieval-augmented generation, guardrails</li> <li>Large-scale image search: embedding extraction, approximate nearest neighbour (ANN), indexing, serving</li> </ul>"},{"location":"chapter%2016%3A%20inference/01.%20quantisation/","title":"Quantisation","text":"<ul> <li>Why quantise: memory reduction, throughput gains, energy savings</li> <li>Number formats: FP32, FP16, BF16, FP8 (E4M3, E5M2), INT8, INT4, binary/ternary</li> <li>Post-training quantisation (PTQ): calibration, min-max, percentile, MSE-optimal scaling</li> <li>Quantisation-aware training (QAT): fake quantisation, straight-through estimator</li> <li>Weight-only quantisation: GPTQ, AWQ, QuIP, squeeze-and-multiply</li> <li>Activation quantisation: dynamic vs static, per-tensor vs per-channel vs per-token</li> <li>Mixed-precision: choosing precision per layer, sensitivity analysis</li> <li>KV-cache quantisation: reducing memory for long sequences</li> </ul>"},{"location":"chapter%2016%3A%20inference/02.%20efficient%20architectures/","title":"Efficient Architectures","text":"<ul> <li>StreamingLLM: attention sinks, rolling KV-cache, infinite-length generation</li> <li>Sparse attention: local attention, sliding window (Mistral), dilated, BigBird, Longformer</li> <li>Linear attention: kernel approximation, RWKV, RetNet, Mamba (state-space models)</li> <li>Multi-query attention (MQA) and grouped-query attention (GQA): reducing KV-cache size</li> <li>Mixture of Experts at inference: expert caching, routing efficiency</li> <li>Knowledge distillation: teacher-student, task-specific vs general distillation</li> <li>Pruning: unstructured (magnitude), structured (channel/head pruning), lottery ticket hypothesis</li> <li>Neural architecture search (NAS) for efficient models</li> </ul>"},{"location":"chapter%2016%3A%20inference/03.%20serving%20and%20batching/","title":"Serving and Batching","text":"<ul> <li>LLM serving fundamentals: prefill vs decode phases, time to first token (TTFT) vs tokens per second</li> <li>Continuous batching: dynamic request scheduling, iteration-level batching</li> <li>PagedAttention: virtual memory for KV-cache, vLLM architecture</li> <li>Batching strategies: static batching, dynamic batching, sequence bucketing</li> <li>Scheduling: first-come-first-served, shortest-job-first, preemption</li> <li>Disaggregated serving: separating prefill and decode stages</li> <li>Multi-model serving: model multiplexing, LoRA serving (S-LoRA, Punica)</li> <li>Metrics: throughput (tokens/s), latency (p50/p99), SLO compliance, cost per token</li> </ul>"},{"location":"chapter%2016%3A%20inference/04.%20edge%20inference/","title":"Edge Inference","text":"<ul> <li>Edge constraints: limited memory, power budget, no network dependency</li> <li>Model compression pipeline: pruning \u2192 quantisation \u2192 compilation</li> <li>On-device runtimes: TensorFlow Lite, ONNX Runtime, Core ML, TensorRT, ExecuTorch</li> <li>Compiler stack: graph optimisation, operator fusion, memory planning, tiling</li> <li>Hardware targets: mobile GPUs (Adreno, Mali), NPUs (Qualcomm Hexagon, Apple Neural Engine, Google Edge TPU)</li> <li>On-device LLMs: Phi, Gemma, Llama at 1-3B parameter scale, 4-bit inference</li> <li>Federated learning: on-device training, privacy-preserving aggregation, communication efficiency</li> <li>Latency optimisation: model partitioning, early exit, caching strategies</li> </ul>"},{"location":"chapter%2016%3A%20inference/05.%20scaling%20and%20deployment/","title":"Scaling and Deployment","text":"<ul> <li>Model parallelism: tensor parallelism (Megatron-style column/row splitting), pipeline parallelism (GPipe, microbatching), sequence parallelism</li> <li>Data parallelism at inference: replicating models across GPUs</li> <li>Distributed KV-cache: sharding across nodes, communication overhead</li> <li>Speculative decoding: draft model + verification, Medusa heads, EAGLE, self-speculative decoding</li> <li>Prefix caching: sharing KV-cache across requests with common prefixes</li> <li>Inference frameworks: vLLM, TensorRT-LLM, SGLang, llama.cpp, TGI</li> <li>Cost optimisation: spot instances, autoscaling, right-sizing GPU selection</li> <li>Monitoring: token-level logging, latency histograms, degradation detection</li> </ul>"},{"location":"chapter%2017%3A%20intersecting%20fields/01.%20quantum%20machine%20learning/","title":"Quantum Machine Learning","text":"<ul> <li>Quantum computing basics: qubits, superposition, entanglement, measurement</li> <li>Quantum gates: Pauli (X, Y, Z), Hadamard, CNOT, Toffoli, rotation gates</li> <li>Quantum circuits: circuit model, parameterised circuits, depth and width</li> <li>Variational quantum algorithms: VQE, QAOA, variational classifiers</li> <li>Quantum kernel methods: quantum feature maps, quantum support vector machines</li> <li>Quantum neural networks: parameterised quantum circuits as neural layers</li> <li>Barren plateaus: vanishing gradients in quantum circuits, expressibility vs trainability</li> <li>Quantum advantage debate: NISQ era limitations, fault-tolerant quantum computing timeline</li> <li>Hybrid classical-quantum architectures: quantum layers in classical pipelines</li> </ul>"},{"location":"chapter%2017%3A%20intersecting%20fields/02.%20neuromorphic%20computing/","title":"Neuromorphic Computing","text":"<ul> <li>Biological inspiration: spiking neurons, synaptic plasticity, temporal coding</li> <li>Spiking neural networks (SNNs): integrate-and-fire models (LIF, IF), spike timing</li> <li>Learning in SNNs: STDP (spike-timing-dependent plasticity), surrogate gradient methods, conversion from ANNs</li> <li>Neuromorphic hardware: Intel Loihi 2, IBM TrueNorth, SpiNNaker, BrainScaleS</li> <li>Event-driven computation: asynchronous processing, energy efficiency</li> <li>Event cameras (DVS): neuromorphic vision sensors, sparse temporal data</li> <li>Applications: low-power edge inference, robotics, always-on sensing</li> <li>Comparison with conventional deep learning: latency, power, accuracy tradeoffs</li> </ul>"},{"location":"chapter%2017%3A%20intersecting%20fields/03.%20AI%20for%20finance/","title":"AI for Finance","text":"<ul> <li>Time series forecasting: ARIMA, exponential smoothing, Prophet, neural approaches (LSTM, Temporal Fusion Transformer, PatchTST)</li> <li>Algorithmic trading: signal generation, execution algorithms (TWAP, VWAP), market microstructure</li> <li>Portfolio optimisation: mean-variance (Markowitz), Black-Litterman, RL-based portfolio management</li> <li>Risk modelling: Value at Risk (VaR), Expected Shortfall, Monte Carlo simulation, credit scoring</li> <li>Fraud detection: anomaly detection, graph-based approaches, real-time streaming</li> <li>NLP in finance: sentiment analysis of news/earnings calls, financial document understanding</li> <li>Alternative data: satellite imagery, social media, web scraping</li> <li>Regulatory and ethical: model explainability (SHAP, LIME), fairness in credit, regulatory compliance</li> </ul>"},{"location":"chapter%2017%3A%20intersecting%20fields/04.%20AI%20for%20biology/","title":"AI for Biology","text":"<ul> <li>Protein structure prediction: AlphaFold 1/2/3, ESMFold, co-evolutionary analysis, MSA transformers</li> <li>Protein design: inverse folding (ProteinMPNN), diffusion for protein generation (RFDiffusion), hallucination</li> <li>Drug discovery: molecular representations (SMILES, graphs), molecular property prediction, virtual screening, docking</li> <li>Generative chemistry: molecular generation (VAE, GAN, diffusion), retrosynthesis prediction</li> <li>Genomics: DNA sequence modelling (Enformer, Hyena DNA), variant effect prediction, CRISPR guide design</li> <li>Single-cell analysis: scRNA-seq, cell type clustering, trajectory inference</li> <li>Medical imaging: radiology (CheXNet), pathology (whole-slide images), segmentation (nnU-Net)</li> <li>Clinical NLP: medical entity extraction, clinical trial matching, electronic health records</li> </ul>"},{"location":"chapter%2017%3A%20intersecting%20fields/05.%20emerging%20intersections/","title":"Emerging Intersections","text":"<ul> <li>AI for climate: weather forecasting (GraphCast, Pangu-Weather, GenCast), carbon footprint of AI, energy grid optimisation</li> <li>AI for materials science: crystal structure prediction, property screening, generative materials design</li> <li>AI for mathematics: automated theorem proving (Lean, Isabelle), conjecture generation, symbolic regression</li> <li>AI for code: code generation (Codex, StarCoder), program synthesis, formal verification, code review agents</li> <li>AI for education: intelligent tutoring systems, personalised learning, automated grading</li> <li>AI for law: contract analysis, legal document retrieval, case outcome prediction</li> <li>AI safety and alignment: RLHF, constitutional AI, interpretability (mechanistic, probing), deceptive alignment</li> <li>Societal impact: labour markets, intellectual property, deepfakes, governance frameworks</li> </ul>"}]}